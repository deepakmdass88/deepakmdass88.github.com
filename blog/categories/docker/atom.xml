<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: docker | Welcome to My Nerd World]]></title>
  <link href="http://beingasysadmin.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://beingasysadmin.com/"/>
  <updated>2014-12-07T09:21:50+00:00</updated>
  <id>http://beingasysadmin.com/</id>
  <author>
    <name><![CDATA[Deepak M Das]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Load Test on Docker Freeswitch]]></title>
    <link href="http://beingasysadmin.com/blog/2014/07/24/load-test-on-docker-freeswitch/"/>
    <updated>2014-07-24T11:45:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2014/07/24/load-test-on-docker-freeswitch</id>
    <content type="html"><![CDATA[<p><code>Docker</code> is a very powerfull tool for managing Linux containers. In my previous <a href="http://beingasysadmin.wordpress.com/2014/06/16/dockerizing-freeswitch-docker-enters-telephony-world/">blog</a> i've explaind on how to setup a Docker Freeswitch. Docker is very mature now, version 1.0 has already been released. Docker is now supported by all major cloud vendors. Docker was showing promising results when i was performing my initial testing. So this time i decided to perform a heavy load test on the Freeswitch container to ensure that Docker can really enter Telephony. Like any normal sys admin, i was googling for Freeswitch load test, and most of the results were pointing to Sipp, an Open Source test tool / traffic generator for the SIP protocol. For me Sipp didnt helped me as it started throwing errors beyond 320 simultaneous calls. The UDP connections were timing out. I tried increasing the timeout, which didn't helped much.</p>

<p>So next choice is to use a Freeswitch itself, to generate calls. Using the FreeSwitch's <em>originate</em> command to generate simultaneous calls and hit the Docker Freeswitch container. I also decided to collect all system metrics, so that i knows how the machine behaves under various load tests conditions. For this i deciced to use <em>CollectD</em> and <em>Graphite</em> combo. Collectd 5+ has an inbuild graphite plugin which can send the collectd metrics to a graphite server.</p>

<p>I've already setup an Ubuntu-Freeswitch Docker <a href="https://registry.hub.docker.com/u/deepakmdass88/fs-ubuntu/">image</a>. First we need to pull the images from the Docker hub.</p>

<pre><code>$ docker pull deepakmdass88/fs-ubuntu
</code></pre>

<p>Now i'm going to start the Docker FreeSwitch container in foreground.</p>

<pre><code>$ docker run --rm --privilieged -i -t -p 5060:5060/tcp -p 5060:5060/udp -p 16384:16384/udp -p 16385:16385/udp -p 16386:16386/udp -p 16387:16387/udp -p 16388:16388/udp -p 16389:16389/udp -p 16390:16390/udp -p 16391:16391/udp -p 16392:16392/udp -p 16393:16393/udp -p 5080:5080/tcp -p 5080:5080/udp deepakmdass88/fs-ubuntu /bin/bash
</code></pre>

<p>The <code>privilieged</code> option was enabled because, the FreeSwitch init script sets some custom <strong>ulimit</strong> values, so the container has to be given special privileges. Corresponding SIP and RTP ports are forwarded from the host to the container.</p>

<p>Now before starting the Freeswitch service, we can set up the CollectD agent. By default, the Ubuntu repostiry contains CollectD versio 4.10, but the Graphite plugin is available from version 5.0+ onwards. So we can use somne PPA which has the corresponding version available.</p>

<pre><code>$ apt-get install python-software-properties

$ add-apt-repository ppa:joey-imbasciano/collectd5

$ apt-get update &amp;&amp; apt-get install collectd
</code></pre>

<p>Now in the <code>/etc/collectd.conf</code>, uncomment <em>LoadPlugin write_graphite</em>. Also, in the same file and uncomment the plugin definition and fill in the server details.</p>

<pre><code>&lt;Plugin write_graphite&gt;
  &lt;Carbon&gt;
        Host "dockergraphite.example.com"
        Port "2003"
        Protocol "tcp"
        LogSendErrors true
        Prefix "collectd."
        StoreRates true
        AlwaysAppendDS false
        EscapeCharacter "_"
  &lt;/Carbon&gt;
&lt;/Plugin&gt;
</code></pre>

<p>I've enabled a custom <a href="https://github.com/ReadyTalk/freeswitch-collectd-plugin">freeswitch</a> plugin, which will extract the current ongoing calls count from freeswitch and sends it to the graphite server. Once the config changes are done we can restart the CollectD service. Now we can check our graphite UI to see if the default metrics like memory, load, cpu etc. are reaching the graphite server. Once CollectD-Graphite setup is ready, we can go ahead with our load test. So, once the call has reached the server, we need some <em>Dialplan</em> to continue the calls. So the simplest method is to create an infinite loop of playing some file, or some conference. Below are some dialplans that i've created in the <code>public.xml</code></p>

<pre><code># Infinite Play Loop

 &lt;extension name="111222333"&gt;
       &lt;condition field="destination_number" expression="^111222333$"&gt;
         &lt;action application="answer"/&gt;
         &lt;action application="playback" data="sounds/music/8000/got.wav"/&gt;
         &lt;action application="transfer" data="111222333 XML public"/&gt;
       &lt;/condition&gt;
    &lt;/extension&gt;

# Test conference

  &lt;extension name="docker-fs-test-conf"&gt;
    &lt;condition field="destination_number" expression="^112233"&gt;
      &lt;action application="answer"/&gt;
      &lt;action application="sleep" data="500"/&gt;
      &lt;action application="conference" data="docker-test@public"/&gt;
    &lt;/condition&gt;
  &lt;/extension&gt;


# Default IVR menu

    &lt;extension name="ivr_demo"&gt;
      &lt;condition field="destination_number" expression="^5000$"&gt;
        &lt;action application="answer"/&gt;
        &lt;action application="sleep" data="2000"/&gt;
        &lt;action application="ivr" data="demo_ivr"/&gt;
      &lt;/condition&gt;
    &lt;/extension&gt;
</code></pre>

<p>Now, we have the dialplans ready, next is authentication. By default there are two ways, Digest auth and IP Whitelist. Here i'm going to use IP whitelist, so we need to whitelist our IP in the acl.conf file.</p>

<pre><code> &lt;list name="domains" default="deny"&gt;
      &lt;!-- domain= is special it scans the domain from the directory to build the ACL --&gt;
      &lt;node type="allow" domain="$${domain}"/&gt;
      &lt;node type="allow" cidr="xxx.xxx.xxx.xxx/32"/&gt;                 # IP of FS from which we are going to send the calls
      &lt;!-- use cidr= if you wish to allow ip ranges to this domains acl. --&gt;
      &lt;!-- &lt;node type="allow" cidr="192.168.0.0/24"/&gt; --&gt;
 &lt;/list&gt;
</code></pre>

<p>Now we can start the Freeswitch service.</p>

<pre><code>$ /etc/init.d/freeswitch start
</code></pre>

<p>We can check the freeswich service using the fs_cli command.</p>

<pre><code>$ /usr/local/freeswitch/bin/fs_cli -x "show status"

UP 0 years, 0 days, 6 hours, 34 minutes, 59 seconds, 648 milliseconds, 56 microseconds
FreeSWITCH (Version 1.5.13b git 39200cd 2014-07-02 21:55:21Z 64bit) is ready
1068 session(s) since startup
0 session(s) - peak 299, last 5min 0
0 session(s) per Sec out of max 30, peak 29, last 5min 0
1000 session(s) max
min idle cpu 0.00/100.00
Current Stack Size/Max 240K/8192K
</code></pre>

<p>Now freeswitch is ready to accept the connection. We can start sending the calls from our Load test freeswitch. Below is the script that was used to originate the calls from the load test Freeswitch machine. This will create simultaneous calls towards the Docker FS.</p>

<pre><code>IP_URI="sip:111222333@&lt;docker-fs-ip&gt;:5060"
MAX_CALLS=$1

while [ 1 ]; do

set -i req
req=$(/usr/local/freeswitch/bin/fs_cli -q -b -x "show channels count" | awk '{print $1}')
if [ $req -lt $MAX_CALLS ]; then
    /usr/local/freeswitch/bin/fs_cli -q -b -x "bgapi originate sofia/external/$SIP_URI loadtest"
else
    echo "sleep a bit ..."
    sleep 10s

fi
</code></pre>

<p>While bulk calls are being made from the Load test freeswitch machines, to test the Quality in real time, it's better to dial to the extension directly from a Sip Phone/Client and ensure that voice quality is good. Below is my Graphite dashboard for the load test.</p>

<p>Default Graphite UI</p>

<p><img src="/images/Docker-FS-Loadtest.png"></p>

<p><a href="https://github.com/urbanairship/tessera">Tessera</a> UI</p>

<p><img src="/images/tessera-graphite.png"></p>

<p>The FS was stable till 500 simultaneous calls, after that there was a sudden drop in calls and also the voice quality started dropping and in a minute the Freeswitch crashed due to Segmentation fault. I'm  going to analyze the core dump file to understand more about the crash. The other smaller drops that we see in the graph was caused by the Load test Freeswitch machine, as the load was getting high when the number of calls was increased. But 500 simultaneous calls are pretty decent and the there was no issue in voice quality till the number of calls crossed 500. Though it's very difficult to make a final confirmation, i decided to go ahead with phase 2 load test.</p>

<p>In the phase 2 test, i'm planning to use multiple FS load test machines to generate large simultaneous calls + running 2 separate FS containers on the same host and split the incoming calls to both these containers. Once the phase 2 test is completed, ill share the test results in an another blog post. Docker is still under heavy development, and i'm sure Docker will be entering Telephony soon.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Managing Docker Clusters using Mesos and Marathon]]></title>
    <link href="http://beingasysadmin.com/blog/2014/06/27/managing-docker-clusters-using-mesos-and-marathon/"/>
    <updated>2014-06-27T07:31:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2014/06/27/managing-docker-clusters-using-mesos-and-marathon</id>
    <content type="html"><![CDATA[<p><code>Docker</code> has became one of my favourite tool. It's super cool and super easy tool to manage linux containers. LXC's are around in IT world for some time, but by the entry of Docker last year, the wave started rising. Thanks to <a href="https://github.com/dotcloud/docker">Docker</a> team and <a href="https://twitter.com/solomonstre">Solomon Hykes</a> for open sourcing such a wonderfull project. I've already mentioned a lot of stuffs about Docker in my previos blogs, so today im going explain how Docker can be used as a Cluster. There are some interesting tools like <a href="https://coreos.com/">CoreOS</a>, <a href="https://github.com/spotify/helios">Helios</a> etc for managing Docker as a cluster. But today i'm going to explain on how to set up a Docker cluster using Apache <a href="http://mesos.apache.org/">Mesos</a>. CoreOS is a custom linux os which comes with SystemD. But the restriction is, we have to use that custom images of coreos.. Indeed CoreOS team open sourced some exciting tools like etcd fleet which works with CoreOS for managing Docker clusters. But Mesos is quite simple, we can install it via package, or even using tar balls available in thier Github repo onto most of the Linux Distro's and it's quite easy to configure also. Mesos is heavily used by Twitter to manage their data center's. And now <a href="http://mesosphere.io/">Mesosphere</a> has opensourced a new tool called <a href="https://github.com/mesosphere/marathon">Mararthon</a> which now provides a UI and a Rest API for maaging and scheduling Mesos Frameworks aka jobs, in this case containers as a service.</p>

<p>A few weeks ago, Mesos 0.19 was released which comes with an official support for Docker coantiners by intergrating <a href="https://github.com/mesosphere/deimos">Deimos</a> into it. And a few days ago Marathon has released their new version 0.6.0 supports launching any task in a Docker container via Mesos 0.19+</p>

<h2>Setting up Mesos Cluster</h2>

<p>In this test setup, i'm going to setup both Mesos master/slave and Zookeeper on the same Ubuntu 14.04 vagrant node. First we can install the dependencies,</p>

<pre><code>$ apt-get install curl python-setuptools python-pip python-dev python-protobuf
</code></pre>

<p>Now we can install Zookeeper</p>

<pre><code>$ apt-get install zookeeperd
</code></pre>

<p>After the installation, ZooKeeper has 1 configuration. Each Zookeeper needs to know its position in the quorum.</p>

<pre><code>$ echo 1 | sudo dd of=/var/lib/zookeeper/myid
</code></pre>

<p>Now we can setup Docker</p>

<pre><code>$ echo "deb http://get.docker.io/ubuntu docker main" &gt; /etc/apt/sources.list.d/docker.list

$ apt-get update &amp;&amp; apt-get install lxc-docker

$ docker version

   Client version: 1.0.0
   Client API version: 1.12
   Go version (client): go1.2.1
   Git commit (client): 63fe64c
   Server version: 1.0.0
   Server API version: 1.12
   Go version (server): go1.2.1
   Git commit (server): 63fe64c
</code></pre>

<p>Let's pull some basic ubuntu images from Docker Hub so that we can use the same for testing.</p>

<pre><code>$ docker pull libmesos/ubuntu
</code></pre>

<p>Now we can configure Mesos</p>

<pre><code>$ curl -fL http://downloads.mesosphere.io/master/ubuntu/14.04/mesos_0.19.0~ubuntu14.04%2B1_amd64.deb -o /tmp/mesos.deb

$ dpkg -i /tmp/mesos.deb

$ mkdir -p /etc/mesos-master

$ echo in_memory | sudo dd of=/etc/mesos-master/registry

## Mesos Python egg for use in authoring frameworks

$ curl -fL http://downloads.mesosphere.io/master/ubuntu/14.04/mesos-0.19.0_rc2-py2.7-linux-x86_64.egg -o /tmp/mesos.egg

$ easy_install /tmp/mesos.egg
</code></pre>

<p>We can download the latest Marathon 0.6 from <a href="http://downloads.mesosphere.io/marathon/marathon-0.6.0/marathon-0.6.0.tgz">here</a></p>

<pre><code>$ tar xvzf marathon-0.6.0.tgz
</code></pre>

<p>Mesos uses Deimos for managing dockers, Deimos can installed via pip</p>

<pre><code>$ pip install deimos
</code></pre>

<p>Also, we need to configure mesos to use Deimos,</p>

<pre><code>$ mkdir -p /etc/mesos-slave

$ echo /usr/local/bin/deimos | sudo dd of=/etc/mesos-slave/containerizer_path

$ echo external | sudo dd of=/etc/mesos-slave/isolation
</code></pre>

<p>Now we can start all the services.</p>

<pre><code>$ initctl reload-configuration

$ service docker start

$ service zookeeper start

$ service mesos-master start

$ service mesos-slave start

##### Starting Marathon #####

$ cd marathon-0.6.0

$ ./bin/start --master zk://localhost:2181/mesos --zk_hosts localhost:2181
</code></pre>

<p>Marathon will now start listening to port <em>8080</em>, We can access the UI from the browser via this port, also via rest API using the same port.</p>

<pre><code>curl localhost:8080/help   # gives us some details about the API's
</code></pre>

<p>I just went through the Deimos code, so under the hood they are using <code>docker run</code> with some default parameters like <code>--sig-proxy</code>,  <code>--rm</code>,  <code>--cidfile</code>,  <code>-v</code>, <code>-w</code> and extra parameters that we are passing while creating the task via Marathon.</p>

<p>As of now, we still can't pass details like Container image, Docker options via Marathon GUI. So we can use the Rest API for the time being. Below is a sample curl request for launcing a single container,</p>

<pre><code>curl -X POST -H "Accept: application/json" -H "Content-Type: application/json" \
    localhost:8080/v2/apps -d '{
        "container": {"image": "docker:///libmesos/ubuntu", "options": ["--privileged"]},
        "cpus": 0.5,
        "cmd": "sleep 500",
        "id": "docker-tester",
        "instances": 1,
        "mem": 300
    }'
</code></pre>

<p>We can pass custom options to the docker run command via "options". After making the curl request, we can check the syslog, as mesos will be logging into syslog by default. We can even see the Docker run command on the same.</p>

<pre><code>Jun 27 07:24:58 vagrant-ubuntu-trusty-64 deimos[19227]: deimos.containerizer.docker.launch() exit 0 // docker run --sig-proxy --rm --cidfile /tmp/deimos/mesos/00d459fb-22ca-4af7-9a97-ef8a510905f2/cid -w /tmp/mesos-sandbox -v /tmp/deimos/mesos/00d459fb-22ca-4af7-9a97-ef8a510905f2/fs:/tmp/mesos-sandbox --privileged -p 31498:31498 -c 512 -m 300m -e PORT=31498 -e PORT0=31498 -e PORTS=31498 libmesos/ubuntu sh -c 'sleep 500'
</code></pre>

<p>We can also use the Marathon Rest API to check the status of the job which we started.</p>

<pre><code>curl -X GET -H "Content-Type: application/json" localhost:8080/v2/apps
</code></pre>

<p>Below is the screenshort for the same from the Marathon UI.</p>

<p><img src="/images/marathon1.png"></p>

<p>We can also check if the container is launched via <code>docker ps</code> command.</p>

<p><img src="/images/docker1.png"></p>

<p>A more detailed report about the Docker job which we have launched can be viewed via the default Mesos GUI listening on port <em>5050</em> on the Mesos master. Now we can test the scalability of the Job. Currently we have only one container running. So now we can try scaling say adding one more node. We can do it in two ways, like via PUT request using curl or using GUI</p>

<pre><code>curl -X PUT -H "Content-Type: application/json" localhost:8080/v2/apps/docker-tester \
    "container": {"image": "docker:///libmesos/ubuntu", "options": ["--privileged"]},
            "cpus": 0.5,
            "cmd": "sleep 500",
            "id": "docker-tester",
            "instances": 2,     # increasing the instance count to 2
            "mem": 300
            }'
</code></pre>

<p>Now we can use the <code>docker ps</code> command to see if the new container is launched or not. Also we can see that status in UI also.</p>

<p><img src="/images/docker2.png"></p>

<p><img src="/images/marathon2.png"></p>

<p>Similarly, we can scale down also. I've tested the same and all seems to be good. Marathon ensures that the docker process will be running. So incase if the process crashes Marathon will restart the same and ensures that the instances are up and running as per our configuration. There are a few other Open Sourced Mesos Scheduler's  like Apache Aurora, Airbnb's Chronos. But for my requirement marathon is pretty straight and simple and also provides a very good Rest API layer for managing containers. Mesos, Marathon and Docker are still young, but provides a killer combination for managing clusters built over Docker containers.</p>
]]></content>
  </entry>
  
</feed>
