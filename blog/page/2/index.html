

<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Welcome to My Nerd World</title>
  <meta name="author" content="Deepak M Das">
  <link rel="author" href="humans.txt">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  
    
  
  <meta name="description" content=" ">
  
  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://beingasysadmin.com/blog/page/2/">
  <link href="/favicon.png" rel="icon">
  <link href='http://fonts.googleapis.com/css?family=Cantarell' rel='stylesheet' type='text/css'>
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Welcome to My Nerd World" type="application/atom+xml">
  <meta name="og:type" content="website" />
  <meta name="og:site_name" content="Welcome to My Nerd World" />
  <meta name="og:title" content="Welcome to My Nerd World" />
  <meta name="og:description" content=" " />
  <meta name="og:url" content="http://beingasysadmin.com/blog/page/2/index.html"/>
  <meta name="url" content="http://beingasysadmin.com/blog/page/2/index.html">
  
  <meta name="distribution" content="global">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <div id="front-wrapper">
  <div id="hero">
    <div id="hero-inner" class="container">
      <div class="span10 offset1">
  <h1>
    I&#8217;m <em>Deepak</em>,<br/>
    a <em>Random Sys Admin</em><br/>
    by <em>Trade</em>
  </h1>
</div>

    </div>
  </div>
  <section id="sub-hero">
    <div class="container">
      <div class="row">
  <div class="span4">
    <h2>about me</h2>
    <p>A Random Sys Admin by Trade, Hacker by Choice, Loves Linux, Puppet, Ruby, Monitoring, and a lot.</p>
  </div>
  <div class="span6">
    <h2>open source projects</h2>
    <dl class="dl-horizontal">
	    <dt><a href="https://github.com/deepakmdass88/">TweetGrabber</a><a href="https://github.com/deepakmdass88/" rel="tooltip" title="open sourced at Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a></dt>
	    <dd>A Live Tweet Grabber based built on Ruby+REDIS+SINATRA</dd>
      <dt><a href="https://github.com/deepakmdass88/ruby-virtmgr.git">Ruby-Virtmgr   </a><a href="https://github.com/deepakmdass88/ruby-virtmgr.git" rel="tooltip" title="open sourced at Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a></dt>
      <dd>A simple CLI Ruby app for Managing KVM based VM&#8217;s</dd>
    </dl>
  </div>
  <div class="span2">
    <h2>found on</h2>
    <a href="https://github.com/deepakmdass88/" rel="tooltip" title="Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a>
    <a href="http://www.linkedin.com/pub/deepak-dass/44/54/602" rel="tooltip" title="Linkedin"><img class="social_icon" title="Linkedin" alt="Linkedin icon" src="/images/glyphicons_377_linked_in.png"></a>
    <a href="http://twitter.com/deepakmdass88" rel="tooltip" title="Twitter"><img class="social_icon" title="Twitter" alt="Twitter icon" src="/images/glyphicons_391_twitter_t.png"></a>
    <a href="https://plus.google.com/105770729176086017609/posts" rel="tooltip" title="Google Plus"><img class="social_icon" title="Google Plus" alt="Google Plus icon" src="/images/glyphicons_386_google_plus.png"></a>
    <a href="http://www.quora.com/Deepak-M-Dass" rel="tooltip" title="Quora"><img class="social_icon" title="Quora" alt="Quora icon" src="/images/glyphicons_385_quora.png"></a>
    <h2>contact at</h2>
    <a href="mailto:deepakmdass88@gmail.com">deepakmdass88@gmail.com</a>
  </div>
</div>

    </div>
  </section>
  <div class="container">
    <div class="row">
    
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/07/24/load-test-on-docker-freeswitch/">Load Test on Docker Freeswitch</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-07-24T11:45:00+00:00" pubdate data-updated="true">Jul 24<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/docker/'>docker</a>, <a class='category' href='/blog/categories/freeswitch/'>freeswitch</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p><code>Docker</code> is a very powerfull tool for managing Linux containers. In my previous <a href="http://beingasysadmin.wordpress.com/2014/06/16/dockerizing-freeswitch-docker-enters-telephony-world/">blog</a> i&#8217;ve explaind on how to setup a Docker Freeswitch. Docker is very mature now, version 1.0 has already been released. Docker is now supported by all major cloud vendors. Docker was showing promising results when i was performing my initial testing. So this time i decided to perform a heavy load test on the Freeswitch container to ensure that Docker can really enter Telephony. Like any normal sys admin, i was googling for Freeswitch load test, and most of the results were pointing to Sipp, an Open Source test tool / traffic generator for the SIP protocol. For me Sipp didnt helped me as it started throwing errors beyond 320 simultaneous calls. The UDP connections were timing out. I tried increasing the timeout, which didn&#8217;t helped much.</p>

<p>So next choice is to use a Freeswitch itself, to generate calls. Using the FreeSwitch&#8217;s <em>originate</em> command to generate simultaneous calls and hit the Docker Freeswitch container. I also decided to collect all system metrics, so that i knows how the machine behaves under various load tests conditions. For this i deciced to use <em>CollectD</em> and <em>Graphite</em> combo. Collectd 5+ has an inbuild graphite plugin which can send the collectd metrics to a graphite server.</p>

<p>I&#8217;ve already setup an Ubuntu-Freeswitch Docker <a href="https://registry.hub.docker.com/u/deepakmdass88/fs-ubuntu/">image</a>. First we need to pull the images from the Docker hub.</p>

<pre><code>$ docker pull deepakmdass88/fs-ubuntu
</code></pre>

<p>Now i&#8217;m going to start the Docker FreeSwitch container in foreground.</p>

<pre><code>$ docker run --rm --privilieged -i -t -p 5060:5060/tcp -p 5060:5060/udp -p 16384:16384/udp -p 16385:16385/udp -p 16386:16386/udp -p 16387:16387/udp -p 16388:16388/udp -p 16389:16389/udp -p 16390:16390/udp -p 16391:16391/udp -p 16392:16392/udp -p 16393:16393/udp -p 5080:5080/tcp -p 5080:5080/udp deepakmdass88/fs-ubuntu /bin/bash
</code></pre>

<p>The <code>privilieged</code> option was enabled because, the FreeSwitch init script sets some custom <strong>ulimit</strong> values, so the container has to be given special privileges. Corresponding SIP and RTP ports are forwarded from the host to the container.</p>

<p>Now before starting the Freeswitch service, we can set up the CollectD agent. By default, the Ubuntu repostiry contains CollectD versio 4.10, but the Graphite plugin is available from version 5.0+ onwards. So we can use somne PPA which has the corresponding version available.</p>

<pre><code>$ apt-get install python-software-properties

$ add-apt-repository ppa:joey-imbasciano/collectd5

$ apt-get update &amp;&amp; apt-get install collectd
</code></pre>

<p>Now in the <code>/etc/collectd.conf</code>, uncomment <em>LoadPlugin write_graphite</em>. Also, in the same file and uncomment the plugin definition and fill in the server details.</p>

<pre><code>&lt;Plugin write_graphite&gt;
  &lt;Carbon&gt;
        Host "dockergraphite.example.com"
        Port "2003"
        Protocol "tcp"
        LogSendErrors true
        Prefix "collectd."
        StoreRates true
        AlwaysAppendDS false
        EscapeCharacter "_"
  &lt;/Carbon&gt;
&lt;/Plugin&gt;
</code></pre>

<p>I&#8217;ve enabled a custom <a href="https://github.com/ReadyTalk/freeswitch-collectd-plugin">freeswitch</a> plugin, which will extract the current ongoing calls count from freeswitch and sends it to the graphite server. Once the config changes are done we can restart the CollectD service. Now we can check our graphite UI to see if the default metrics like memory, load, cpu etc. are reaching the graphite server. Once CollectD-Graphite setup is ready, we can go ahead with our load test. So, once the call has reached the server, we need some <em>Dialplan</em> to continue the calls. So the simplest method is to create an infinite loop of playing some file, or some conference. Below are some dialplans that i&#8217;ve created in the <code>public.xml</code></p>

<pre><code># Infinite Play Loop

 &lt;extension name="111222333"&gt;
       &lt;condition field="destination_number" expression="^111222333$"&gt;
         &lt;action application="answer"/&gt;
         &lt;action application="playback" data="sounds/music/8000/got.wav"/&gt;
         &lt;action application="transfer" data="111222333 XML public"/&gt;
       &lt;/condition&gt;
    &lt;/extension&gt;

# Test conference

  &lt;extension name="docker-fs-test-conf"&gt;
    &lt;condition field="destination_number" expression="^112233"&gt;
      &lt;action application="answer"/&gt;
      &lt;action application="sleep" data="500"/&gt;
      &lt;action application="conference" data="docker-test@public"/&gt;
    &lt;/condition&gt;
  &lt;/extension&gt;


# Default IVR menu

    &lt;extension name="ivr_demo"&gt;
      &lt;condition field="destination_number" expression="^5000$"&gt;
        &lt;action application="answer"/&gt;
        &lt;action application="sleep" data="2000"/&gt;
        &lt;action application="ivr" data="demo_ivr"/&gt;
      &lt;/condition&gt;
    &lt;/extension&gt;
</code></pre>

<p>Now, we have the dialplans ready, next is authentication. By default there are two ways, Digest auth and IP Whitelist. Here i&#8217;m going to use IP whitelist, so we need to whitelist our IP in the acl.conf file.</p>

<pre><code> &lt;list name="domains" default="deny"&gt;
      &lt;!-- domain= is special it scans the domain from the directory to build the ACL --&gt;
      &lt;node type="allow" domain="$${domain}"/&gt;
      &lt;node type="allow" cidr="xxx.xxx.xxx.xxx/32"/&gt;                 # IP of FS from which we are going to send the calls
      &lt;!-- use cidr= if you wish to allow ip ranges to this domains acl. --&gt;
      &lt;!-- &lt;node type="allow" cidr="192.168.0.0/24"/&gt; --&gt;
 &lt;/list&gt;
</code></pre>

<p>Now we can start the Freeswitch service.</p>

<pre><code>$ /etc/init.d/freeswitch start
</code></pre>

<p>We can check the freeswich service using the fs_cli command.</p>

<pre><code>$ /usr/local/freeswitch/bin/fs_cli -x "show status"

UP 0 years, 0 days, 6 hours, 34 minutes, 59 seconds, 648 milliseconds, 56 microseconds
FreeSWITCH (Version 1.5.13b git 39200cd 2014-07-02 21:55:21Z 64bit) is ready
1068 session(s) since startup
0 session(s) - peak 299, last 5min 0
0 session(s) per Sec out of max 30, peak 29, last 5min 0
1000 session(s) max
min idle cpu 0.00/100.00
Current Stack Size/Max 240K/8192K
</code></pre>

<p>Now freeswitch is ready to accept the connection. We can start sending the calls from our Load test freeswitch. Below is the script that was used to originate the calls from the load test Freeswitch machine. This will create simultaneous calls towards the Docker FS.</p>

<pre><code>IP_URI="sip:111222333@&lt;docker-fs-ip&gt;:5060"
MAX_CALLS=$1

while [ 1 ]; do

set -i req
req=$(/usr/local/freeswitch/bin/fs_cli -q -b -x "show channels count" | awk '{print $1}')
if [ $req -lt $MAX_CALLS ]; then
    /usr/local/freeswitch/bin/fs_cli -q -b -x "bgapi originate sofia/external/$SIP_URI loadtest"
else
    echo "sleep a bit ..."
    sleep 10s

fi
</code></pre>

<p>While bulk calls are being made from the Load test freeswitch machines, to test the Quality in real time, it&#8217;s better to dial to the extension directly from a Sip Phone/Client and ensure that voice quality is good. Below is my Graphite dashboard for the load test.</p>

<p>Default Graphite UI</p>

<p><img src="/images/Docker-FS-Loadtest.png"></p>

<p><a href="https://github.com/urbanairship/tessera">Tessera</a> UI</p>

<p><img src="/images/tessera-graphite.png"></p>

<p>The FS was stable till 500 simultaneous calls, after that there was a sudden drop in calls and also the voice quality started dropping and in a minute the Freeswitch crashed due to Segmentation fault. I&#8217;m  going to analyze the core dump file to understand more about the crash. The other smaller drops that we see in the graph was caused by the Load test Freeswitch machine, as the load was getting high when the number of calls was increased. But 500 simultaneous calls are pretty decent and the there was no issue in voice quality till the number of calls crossed 500. Though it&#8217;s very difficult to make a final confirmation, i decided to go ahead with phase 2 load test.</p>

<p>In the phase 2 test, i&#8217;m planning to use multiple FS load test machines to generate large simultaneous calls + running 2 separate FS containers on the same host and split the incoming calls to both these containers. Once the phase 2 test is completed, ill share the test results in an another blog post. Docker is still under heavy development, and i&#8217;m sure Docker will be entering Telephony soon.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/06/27/managing-docker-clusters-using-mesos-and-marathon/">Managing Docker Clusters Using Mesos and Marathon</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-06-27T07:31:00+00:00" pubdate data-updated="true">Jun 27<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/containerization/'>containerization</a>, <a class='category' href='/blog/categories/docker/'>docker</a>, <a class='category' href='/blog/categories/docker-cluster/'>docker-cluster</a>, <a class='category' href='/blog/categories/marathon/'>marathon</a>, <a class='category' href='/blog/categories/mesos/'>mesos</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p><code>Docker</code> has became one of my favourite tool. It&#8217;s super cool and super easy tool to manage linux containers. LXC&#8217;s are around in IT world for some time, but by the entry of Docker last year, the wave started rising. Thanks to <a href="https://github.com/dotcloud/docker">Docker</a> team and <a href="https://twitter.com/solomonstre">Solomon Hykes</a> for open sourcing such a wonderfull project. I&#8217;ve already mentioned a lot of stuffs about Docker in my previos blogs, so today im going explain how Docker can be used as a Cluster. There are some interesting tools like <a href="https://coreos.com/">CoreOS</a>, <a href="https://github.com/spotify/helios">Helios</a> etc for managing Docker as a cluster. But today i&#8217;m going to explain on how to set up a Docker cluster using Apache <a href="http://mesos.apache.org/">Mesos</a>. CoreOS is a custom linux os which comes with SystemD. But the restriction is, we have to use that custom images of coreos.. Indeed CoreOS team open sourced some exciting tools like etcd fleet which works with CoreOS for managing Docker clusters. But Mesos is quite simple, we can install it via package, or even using tar balls available in thier Github repo onto most of the Linux Distro&#8217;s and it&#8217;s quite easy to configure also. Mesos is heavily used by Twitter to manage their data center&#8217;s. And now <a href="http://mesosphere.io/">Mesosphere</a> has opensourced a new tool called <a href="https://github.com/mesosphere/marathon">Mararthon</a> which now provides a UI and a Rest API for maaging and scheduling Mesos Frameworks aka jobs, in this case containers as a service.</p>

<p>A few weeks ago, Mesos 0.19 was released which comes with an official support for Docker coantiners by intergrating <a href="https://github.com/mesosphere/deimos">Deimos</a> into it. And a few days ago Marathon has released their new version 0.6.0 supports launching any task in a Docker container via Mesos 0.19+</p>

<h2>Setting up Mesos Cluster</h2>

<p>In this test setup, i&#8217;m going to setup both Mesos master/slave and Zookeeper on the same Ubuntu 14.04 vagrant node. First we can install the dependencies,</p>

<pre><code>$ apt-get install curl python-setuptools python-pip python-dev python-protobuf
</code></pre>

<p>Now we can install Zookeeper</p>

<pre><code>$ apt-get install zookeeperd
</code></pre>

<p>After the installation, ZooKeeper has 1 configuration. Each Zookeeper needs to know its position in the quorum.</p>

<pre><code>$ echo 1 | sudo dd of=/var/lib/zookeeper/myid
</code></pre>

<p>Now we can setup Docker</p>

<pre><code>$ echo "deb http://get.docker.io/ubuntu docker main" &gt; /etc/apt/sources.list.d/docker.list

$ apt-get update &amp;&amp; apt-get install lxc-docker

$ docker version

   Client version: 1.0.0
   Client API version: 1.12
   Go version (client): go1.2.1
   Git commit (client): 63fe64c
   Server version: 1.0.0
   Server API version: 1.12
   Go version (server): go1.2.1
   Git commit (server): 63fe64c
</code></pre>

<p>Let&#8217;s pull some basic ubuntu images from Docker Hub so that we can use the same for testing.</p>

<pre><code>$ docker pull libmesos/ubuntu
</code></pre>

<p>Now we can configure Mesos</p>

<pre><code>$ curl -fL http://downloads.mesosphere.io/master/ubuntu/14.04/mesos_0.19.0~ubuntu14.04%2B1_amd64.deb -o /tmp/mesos.deb

$ dpkg -i /tmp/mesos.deb

$ mkdir -p /etc/mesos-master

$ echo in_memory | sudo dd of=/etc/mesos-master/registry

## Mesos Python egg for use in authoring frameworks

$ curl -fL http://downloads.mesosphere.io/master/ubuntu/14.04/mesos-0.19.0_rc2-py2.7-linux-x86_64.egg -o /tmp/mesos.egg

$ easy_install /tmp/mesos.egg
</code></pre>

<p>We can download the latest Marathon 0.6 from <a href="http://downloads.mesosphere.io/marathon/marathon-0.6.0/marathon-0.6.0.tgz">here</a></p>

<pre><code>$ tar xvzf marathon-0.6.0.tgz
</code></pre>

<p>Mesos uses Deimos for managing dockers, Deimos can installed via pip</p>

<pre><code>$ pip install deimos
</code></pre>

<p>Also, we need to configure mesos to use Deimos,</p>

<pre><code>$ mkdir -p /etc/mesos-slave

$ echo /usr/local/bin/deimos | sudo dd of=/etc/mesos-slave/containerizer_path

$ echo external | sudo dd of=/etc/mesos-slave/isolation
</code></pre>

<p>Now we can start all the services.</p>

<pre><code>$ initctl reload-configuration

$ service docker start

$ service zookeeper start

$ service mesos-master start

$ service mesos-slave start

##### Starting Marathon #####

$ cd marathon-0.6.0

$ ./bin/start --master zk://localhost:2181/mesos --zk_hosts localhost:2181
</code></pre>

<p>Marathon will now start listening to port <em>8080</em>, We can access the UI from the browser via this port, also via rest API using the same port.</p>

<pre><code>curl localhost:8080/help   # gives us some details about the API's
</code></pre>

<p>I just went through the Deimos code, so under the hood they are using <code>docker run</code> with some default parameters like <code>--sig-proxy</code>,  <code>--rm</code>,  <code>--cidfile</code>,  <code>-v</code>, <code>-w</code> and extra parameters that we are passing while creating the task via Marathon.</p>

<p>As of now, we still can&#8217;t pass details like Container image, Docker options via Marathon GUI. So we can use the Rest API for the time being. Below is a sample curl request for launcing a single container,</p>

<pre><code>curl -X POST -H "Accept: application/json" -H "Content-Type: application/json" \
    localhost:8080/v2/apps -d '{
        "container": {"image": "docker:///libmesos/ubuntu", "options": ["--privileged"]},
        "cpus": 0.5,
        "cmd": "sleep 500",
        "id": "docker-tester",
        "instances": 1,
        "mem": 300
    }'
</code></pre>

<p>We can pass custom options to the docker run command via &#8220;options&#8221;. After making the curl request, we can check the syslog, as mesos will be logging into syslog by default. We can even see the Docker run command on the same.</p>

<pre><code>Jun 27 07:24:58 vagrant-ubuntu-trusty-64 deimos[19227]: deimos.containerizer.docker.launch() exit 0 // docker run --sig-proxy --rm --cidfile /tmp/deimos/mesos/00d459fb-22ca-4af7-9a97-ef8a510905f2/cid -w /tmp/mesos-sandbox -v /tmp/deimos/mesos/00d459fb-22ca-4af7-9a97-ef8a510905f2/fs:/tmp/mesos-sandbox --privileged -p 31498:31498 -c 512 -m 300m -e PORT=31498 -e PORT0=31498 -e PORTS=31498 libmesos/ubuntu sh -c 'sleep 500'
</code></pre>

<p>We can also use the Marathon Rest API to check the status of the job which we started.</p>

<pre><code>curl -X GET -H "Content-Type: application/json" localhost:8080/v2/apps
</code></pre>

<p>Below is the screenshort for the same from the Marathon UI.</p>

<p><img src="/images/marathon1.png"></p>

<p>We can also check if the container is launched via <code>docker ps</code> command.</p>

<p><img src="/images/docker1.png"></p>

<p>A more detailed report about the Docker job which we have launched can be viewed via the default Mesos GUI listening on port <em>5050</em> on the Mesos master. Now we can test the scalability of the Job. Currently we have only one container running. So now we can try scaling say adding one more node. We can do it in two ways, like via PUT request using curl or using GUI</p>

<pre><code>curl -X PUT -H "Content-Type: application/json" localhost:8080/v2/apps/docker-tester \
    "container": {"image": "docker:///libmesos/ubuntu", "options": ["--privileged"]},
            "cpus": 0.5,
            "cmd": "sleep 500",
            "id": "docker-tester",
            "instances": 2,     # increasing the instance count to 2
            "mem": 300
            }'
</code></pre>

<p>Now we can use the <code>docker ps</code> command to see if the new container is launched or not. Also we can see that status in UI also.</p>

<p><img src="/images/docker2.png"></p>

<p><img src="/images/marathon2.png"></p>

<p>Similarly, we can scale down also. I&#8217;ve tested the same and all seems to be good. Marathon ensures that the docker process will be running. So incase if the process crashes Marathon will restart the same and ensures that the instances are up and running as per our configuration. There are a few other Open Sourced Mesos Scheduler&#8217;s  like Apache Aurora, Airbnb&#8217;s Chronos. But for my requirement marathon is pretty straight and simple and also provides a very good Rest API layer for managing containers. Mesos, Marathon and Docker are still young, but provides a killer combination for managing clusters built over Docker containers.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/06/10/dockerizing-freeswitch-docker-enters-telephony/">Dockerizing Freeswitch - Docker Enters Telephony</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-06-10T11:39:00+00:00" pubdate data-updated="true">Jun 10<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/docker/'>Docker</a>, <a class='category' href='/blog/categories/freeswitch/'>FreeSwitch</a>, <a class='category' href='/blog/categories/telephony/'>Telephony</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p><code>Docker</code> has became one of the hottest topics in IT now a days. <code>Docker</code> is an open-source project that automates the deployment of applications inside software containers. Docker extends a common container format called <em>Linux Containers</em> (LXC), with a high-level API providing lightweight virtualization that runs processes in isolation.Docker uses <em>LXC, cgroups,</em> and the <em>Linux kernel</em> itself. Though i coudn&#8217;t make out to the DockerCon 2014 in SF, a lot new developments were announced on the DockerCon. Especially three new Opensource Projects <a href="https://github.com/docker/libcontainer">libcontainer</a>, <a href="https://github.com/docker/libchan">libchan</a> and <a href="https://github.com/docker/libswarm">libswarn</a>. Docker is indeed creating a revolution in the container space, creating a next generation of scalable platform management. There are a lot PAAS services like <a href="http://deis.io/">Deis</a>, <a href="http://signup.resin.io/">resin.io</a>, <a href="https://github.com/progrium/dokku">Dokku</a> which are already using Docker in production. Another important and exciting project is <a href="https://coreos.com/">CoreOS</a>. CoreOS uses tools like <a href="http://www.freedesktop.org/wiki/Software/systemd/">SystemD</a>, <a href="https://github.com/coreos/fleet">Fleet</a>, <a href="https://github.com/coreos/etcd">EtcD</a> to build a fully scalabale docker based cluster management system. I definitely need a separate blog to write about CoreOS, it&#8217;s really a super exciting project to play with.</p>

<p>Last week Docker Team released Version 1.0 of Docker. So i&#8217;ll be using the same in this new set up. It&#8217;s been almost 6 Month&#8217;s since i&#8217;ve been working @ <a href="http://plivo.com">Plivo</a> as a DevOps Engineer. Telephony was really a very new platform for me. And my first companion was offcourse <a href="http://freeswitch.org/">FreeSwitch</a>,a scalable open source cross-platform telephony platform designed to route and interconnect popular communication protocols using audio, video, text or any other form of media. I was heavily using Vagrant for all my experiments in my mac. But after started using Docker, it really made me crazy. I&#8217;ve played for some time wiht LXC&#8217;s long back. So this was like a leap back to the container world.</p>

<p>There are a lot of concerns on using Virtual Machines in Telephony world. Especially for the server&#8217;s that handles the Real Time voice packets, as voice quality is pretty important in Telephony. Docker&#8217;s again more light weight isolated environment, and i decide to see how Docker can perform with such issues. If Docker handle Freeswitch smoothly, then i&#8217;m sure that we can use Docker for other telephony app&#8217;s like OpenSIPS/Kamailio etc, as they handle only sessions not the Media traffic. I know there are a lot of concerns like CPU load, Network etc, but this is like an initial move to test Docker into Telephony.</p>

<h2>Setting Up Docker </h2>

<p>Docker 1.0 is available from the Official Docker repo.</p>

<pre><code>$ echo "deb http://get.docker.io/ubuntu docker main" &gt; /etc/apt/sources.list.d/docker.list

$ apt-get update &amp;&amp; apt-get install lxc-docker
</code></pre>

<p>Now we can check the Docker version using the docker binary itself.</p>

<pre><code>$ docker version

Client version: 1.0.0
Client API version: 1.12
Go version (client): go1.2.1
Git commit (client): 63fe64c
Server version: 1.0.0
Server API version: 1.12
Go version (server): go1.2.1
Git commit (server): 63fe64c
</code></pre>

<p>Now Docker is installed, but we need some OS images to use with docker. We can build custom images using debootstrap etc. But there are official minimal images available in Docker <a href="https://registry.hub.docker.com/">HUB</a>. We can search for the repositories and can pull those images via docker binary itself.</p>

<p>For example to pull the entire Ubuntu images, we can just do,</p>

<pre><code>$ docker pull ubuntu
</code></pre>

<p>But this will download all the ubuntu images available in the repo. We can also do selective download by using the tag.</p>

<pre><code>$ docker pull ubuntu:14:04
</code></pre>

<p>Once the images are downloaded, we can use images option in docker binary to see all the downloaded images.</p>

<pre><code>$ docker images

REPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
ubuntu                          14.04               ad892dd21d60        10 days ago         275.5 MB
</code></pre>

<p>Here i&#8217;m not going to daemonize the container, i&#8217;ll be using the interactive option. But first, let&#8217;s start a new container.</p>

<pre><code>$ docker run -t -i ubuntu:14.04 /bin/bash
</code></pre>

<p>This command will start a conatiner and will open up a bash session for us and we will be inside the bash session. Now to use an application we need to open up corresponding ports to outside world. We can use the &#8220;-p&#8221; option while starting a docker container to enable port forwarding. Under the hood, docker is using IPtables for the same. In the case of Freeswitch, we need to open 5060,5080 for the default Sofia profiles (Internal and External). Also we need to open the RTP ports. In this test i&#8217;ll be opening a predefined set of ports ie from &#8220;16384&#8221; to &#8220;16394&#8221;. (As my Docker host resides on Azure, creating an Endpoint for each port forward is really a pain, so i decided to open only a few). And also i&#8217;ll be opening port 22, so that we can have an ssh server inside the container.</p>

<pre><code>$ docker run -t -i -p 2223:22 -p 5060:5060/tcp -p 5060:5060/udp -p 16384:16384/udp -p 16385:16385/udp -p 16386:16386/udp -p 16387:16387/udp -p 16388:16388/udp -p 16389:16389/udp -p 16390:16390/udp -p 16391:16391/udp -p 16392:16392/udp -p 16393:16393/udp -p 5080:5080/tcp -p 5080:5080/udp ubuntu:14.04 /bin/bash
</code></pre>

<p>This will start a new container and Docker by default will setup the IPtables for port forwarding. So now my IPtables looks like this.</p>

<pre><code>Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
   43 16850 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:5080
    0     0 ACCEPT     tcp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           tcp dpt:5080
  988  198K ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16392
    0     0 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16389
    0     0 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16385
    0     0 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16393
 2026  405K ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16388
 8817 1763K ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16384
12144 8684K ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:5060
 4359  257K ACCEPT     tcp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           tcp dpt:5060
 9917 1983K ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16390
    0     0 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16387
    0     0 ACCEPT     tcp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           tcp dpt:22
   38  4848 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16391
    1   152 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16386
    0     0 ACCEPT     all  --  *      lxcbr0  0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  --  lxcbr0 *       0.0.0.0/0            0.0.0.0/0
 431K  630M ACCEPT     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
 128K   19M ACCEPT     all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0
   16  2460 ACCEPT     all  --  docker0 docker0  0.0.0.0/0            0.0.0.0/0
</code></pre>

<p>Now we can go ahead with Freeswitch compilation. In my previous <a href="http://beingasysadmin.com/blog/2014/02/14/sip-trunking-using-plivo-and-freeswitch/">blog</a>, i&#8217;ve mentioned how to compile and set up freeswitch. Once freeswitch is ready, we need to make a few changes. By default, Freeswitch uses STUN to route through NAT, but this doesn&#8217;t work with Docker. So we have to set the external IP manually. In the Freeswitch installed folder, edit <code>conf/autoload_configs/switch.conf.xml</code>. In this file we can set the External IP manually. Add the below lines to switch_conf.xml.</p>

<pre><code>&lt;X-PRE-PROCESS cmd="set" data="external_sip_ip=&lt;YOUR_EXTERNAL_IP&gt;"/&gt;
&lt;X-PRE-PROCESS cmd="set" data="external_rtp_ip=&lt;YOUR_EXTERNAL_IP&gt;"/&gt;
</code></pre>

<p>Also we need to modify the Default Sofia Profiles and need to set the <em>ext-rtp-ip</em> and <em>ext-sip-ip</em> to use our external IP added in the <code>switch_conf.xml</code> file while establishing connections. Add the below lines to the <code>conf/sip_profiles/internal.xml</code> and <code>conf/sip_profiles/external.xml</code></p>

<pre><code>&lt;param name="ext-rtp-ip" value="$${external_rtp_ip}"/&gt;
&lt;param name="ext-sip-ip" value="$${external_sip_ip}"/&gt;
</code></pre>

<p>Now we need to set teh RTP ip range to the range which we have forwarded while creting the container. So we need to edit <code>conf/autoload_configs/switch.conf.xml</code></p>

<pre><code>&lt;param name="rtp-start-port" value="16384"/&gt;
&lt;param name="rtp-end-port" value="16394"/&gt;
</code></pre>

<p>Once the changes are made, we can start the FreeSwitch service. Now to make sure that the External IP is working properly, we can check the sofia profile status using fs_cli. below is a sample output of the sofia profile status.</p>

<pre><code>freeswitch@internal&gt; sofia status profile internal
=================================================================================================
Name                internal
Domain Name         N/A
Auto-NAT            false
DBName              sofia_reg_internal
Pres Hosts          172.17.0.6,172.17.0.6
Dialplan            XML
Context             public
Challenge Realm     auto_from
RTP-IP              172.17.0.6
Ext-RTP-IP          &lt;my_external_ip&gt;
SIP-IP              172.17.0.6
Ext-SIP-IP          &lt;my_external_ip&gt;
URL                 sip:mod_sofia@&lt;my_external_ip&gt;:5060
BIND-URL            sip:mod_sofia@&lt;my_external_ip&gt;:5060;maddr=172.17.0.6;transport=udp,tcp
HOLD-MUSIC          local_stream://moh
OUTBOUND-PROXY      N/A
CODECS IN           OPUS,G722,PCMU,PCMA,GSM
CODECS OUT          OPUS,G722,PCMU,PCMA,GSM
TEL-EVENT           101
DTMF-MODE           rfc2833
CNG                 13
SESSION-TO          0
MAX-DIALOG          0
NOMEDIA             false
LATE-NEG            true
PROXY-MEDIA         false
ZRTP-PASSTHRU       true
AGGRESSIVENAT       false
CALLS-IN            0
FAILED-CALLS-IN     0
CALLS-OUT           0
FAILED-CALLS-OUT    0
REGISTRATIONS       1
</code></pre>

<p>Now freeswitch ahs started successfully. We can test some basic calls using softphones like Xlite, Telephone etc. By default, there are some default extensions and user&#8217;s available, so we can use the same for testing the calls. But i really wanted to try trunkning also and wanted to see the quality of the voice. So i created <a href="http://beingasysadmin.com/blog/2014/02/14/sip-trunking-using-plivo-and-freeswitch/">SIP trunking in Freeswitch using Plivo</a>. And i tested a couple of calls to US and India DID&#8217;s and no issues were detected in the quality. But again i need to test the laod of the server&#8217;s when it startes handling concurrent calls and also the voice quality. But i decied to d oit as a Phase II. But as of now, Docker FreeSwitch is working perfectly like a physical machine with out issues.</p>

<p>So now we have a working FreeSwitch container, now here comes the main advantage of the Docker. We can create a new image with all these changes, so that nex time i dont need to work from scratch. I can use this saved image and a readymade Docker Freeswitch container can be launched in seconds. Since we are in interactive mode, we should not quit the session before it&#8217;s saved or else all the things will be lost,becoz dokcer will destroy the same. So open up a new shell on the docker host and use the commit option. But to use the commit command, we need to know the container id, so here docker ps command comes handy.</p>

<pre><code>$ docker ps

CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS           NAMES

e7f3c02346d4        a4196763d248        /bin/bash           32 hours ago        Up 32 hours         0.0.0.0:2223-&gt;22/tcp, 0.0.0.0:5060-&gt;5060/tcp, 0.0.0.0:5060-&gt;5060/udp, 0.0.0.0:5080-&gt;5080/tcp, 0.0.0.0:5080-&gt;5080/udp, 0.0.0.0:16384-&gt;16384/udp, 0.0.0.0:16385-&gt;16385/udp, 0.0.0.0:16386-&gt;16386/udp, 0.0.0.0:16387-&gt;16387/udp, 0.0.0.0:16388-&gt;16388/udp, 0.0.0.0:16389-&gt;16389/udp, 0.0.0.0:16390-&gt;16390/udp, 0.0.0.0:16391-&gt;16391/udp, 0.0.0.0:16392-&gt;16392/udp, 0.0.0.0:16393-&gt;16393/udp   silly_turing
</code></pre>

<p>In my case &#8220;e7f3c02346d4&#8221; is the container ID. So i can use the same for commit. I won&#8217;t be commiting to the base Ubuntu image, as i can use the same for other purposes, so here i&#8217;ll commiting to a new image say &#8220;ubntu-fs-docker&#8221;</p>

<pre><code>$ docker commit -m "&lt;commit message&gt;" e7f3c02346d4 ubntu-fs-docker
</code></pre>

<p>Now we can use this &#8220;ubntu-fs-docker&#8221; image to launch a ready made FreeSwitch server&#8217;s.</p>

<p>Docker is a very juvenile project about more than a year old. But the use cases are expanding heavily in the Modern IT world. Docker is fueling up a new generation of scalable servers. Wishing all the best for Docker and kudos to <a href="https://twitter.com/solomonstre">Solomon Hykes</a> and the DotCloud team for opensourcing such a powerfull project</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/05/19/uchiwa-an-awesome-dashboard-for-sensu/">UCHIWA - an Awesome Dashboard for Sensu</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-05-19T06:42:00+00:00" pubdate data-updated="true">May 19<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/devops/'>DevOps</a>, <a class='category' href='/blog/categories/monitoring/'>Monitoring</a>, <a class='category' href='/blog/categories/sensu/'>Sensu</a>, <a class='category' href='/blog/categories/sensu-dashboard/'>sensu-dashboard</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>It&#8217;s been more than a year since i&#8217;ve started playing with <code>Sensu</code>. It was one of coolest monitoring projects that i&#8217;ve ever worked with. Perfect for <em>Cloud</em> infrastructure and backed by a cool community. Though Sensu is still in <em>Juvenile</em> (v 0.12) state, its mature enough to tackle majority of the monitoring issues. Recently i started migrating all our monitoring from traditional Nagios to Sensu and the pretty cool thing with sensu is, we can directly use the Nagios plugins with Sensu. That&#8217;s an easy task for migration. We don&#8217;t have to rebuild all the check&#8217;s to make with Sensu. But most of the people outside was pointing to the sensu&#8217;s default dashboard. Though the dashboard doesn&#8217;t looks pretty fancy, it can do all the functions. But having a good dashboard which can display the current status is always a time saver.</p>

<p>So i was being searching for a good dashboard and the first choice on the google search was <a href="https://github.com/sensu/sensu-admin.git">Sensu-Admin</a>. A Rails project, which needs a backend DB. But still i was not satisfied with it, and i started looking out for something different. The second choice was <a href="https://github.com/cloudant/sabisu">sabisu</a>. Sabisu uses Cloudant&#8217;s hosted Couchdb with Lucene. We just need to store all the events in a Redis List and a custom script which reads the data from the Redis List and pushes it to the Cloudant&#8217;s CouchDB. So we basically need a Cloudant account and the Webapp makes Lucene queries to the Cloudant DB and displays the results on the Sabisu dashboard. Though i tried to rebuild the same setup locally, like running a CouchDB+Lucene locally and sending the same data to the local couchdb. With some codehack&#8217;s i was able to make the webapp talk to my local CouchDB and display the results on the dashboard.</p>

<p>But then, I found a super cool dashboard project called <a href="https://github.com/palourde/uchiwa">UCHIWA</a> which was started in Github a few days back by <a href="https://github.com/palourde">Simon Plourde</a>. <a href="https://github.com/palourde/uchiwa">Uchiwa</a> is simple dashboard built with <em>NodeJS</em> and uses <em>SocketIO</em> for real time updates. The screenshot&#8217;s looks super cool and i decided to give it a try. It has only one dependency, NodeJS, no backend DB&#8217;s required as it talks to the Sensu&#8217;s API in realtime.</p>

<h2>Setting Up NodeJS</h2>

<p>For Ubuntu, we can use the chris-lea&#8217;s PPA.</p>

<pre><code>apt-get install python-software-properties        # required for "apt-add-repository" binary

apt-add-repository ppa:chris-lea/node.js

apt-get update

apt-get install nodejs
</code></pre>

<p>now we have the latest NodeJS on our system, we can start setting up Uchiwa.</p>

<h2>Setting Up Uchiwa Dashboard</h2>

<p><code>Uchiwa</code>&#8217;s source is available in Github.</p>

<pre><code>git clone https://github.com/palourde/uchiwa.git
</code></pre>

<p>Once  cloned, the repository contains the &#8220;package.json&#8221; file which contains the list of necessary dependencies. We can use &#8220;npm&#8221; (node package manager) to install all these.</p>

<pre><code>cd uchiwa

npm install
</code></pre>

<p>Now we need to create a config file for the app. There is a sample config file available in the repo. So we need to mention the Sensu&#8217;s API IP and Port number and also the auth credentials if any. Plus auth credentials for accessing Uchiwa Dashboard page.</p>

<p>once all these are set, we are done. We just need to start the service.</p>

<pre><code>node app.js 
</code></pre>

<p>We can access the page via <em>http://localhost:3000/</em>,  or we can proxy pass from the webserver. Instructions for <em>Nginx</em> is available on the Readme of the project. Now we need to keep this app running all the time. So it&#8217;s better to create a init/upstart process for the same, so that the process will start automatically when the system reboots. There is a cool Node project called <a href="https://github.com/nodejitsu/forever">forever</a> which is a  simple CLI tool for ensuring that a given script runs continuously.</p>

<p>I&#8217;ve created an upstart script for Uchiwa, bu putting a conf file &#8220;uchiwa.conf&#8221; in the &#8220;/etc/init&#8221; directory. Below is the content for the conf file. Once the file is in place, we have to do a reload of the upstart configuration. <code>initctl reload-configuration</code> will do the trick.</p>

<pre><code>description "uchiwa - dashboard for sensu"
env APP_PATH = "/usr/local/uchiwa/"

start on startup
stop on shutdown

script
  cd $APP_PATH
  exec forever start app.js
end script
</code></pre>

<p>Uchiwa looks pretty cool and neat and it has the stash support also. There are couple of addons required like &#8220;Downtime&#8221;, but Uchiwa is a pretty new project and i&#8217;m sure that this project is gonna grow soon. It has already received 99 stars on the Github. Kudos to <a href="https://github.com/palourde">Simon Plourde</a> for Open Sourcing this awesome project.</p>

<p><img src="/images/uchiwa1.png"></p>

<p><img src="/images/uchiwa2.png"></p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/05/05/extending-elk-stack-to-voip-infrastructure/">Extending ELK Stack to VOIP Infrastructure</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-05-05T05:12:00+00:00" pubdate data-updated="true">May 5<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/collectd/'>Collectd</a>, <a class='category' href='/blog/categories/elasticsearch/'>Elasticsearch</a>, <a class='category' href='/blog/categories/freeswitch/'>Freeswitch</a>, <a class='category' href='/blog/categories/kibana/'>Kibana</a>, <a class='category' href='/blog/categories/logstash/'>Logstash</a>, <a class='category' href='/blog/categories/monitoring/'>Monitoring</a>, <a class='category' href='/blog/categories/voip/'>VOIP</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>Being a <code>DevOps</code> guy, i always love metrics. Visualized metrics gives a good picture of what&#8217;s happening in our live battle stations. There are now a quite lot of Open Source tools for monitoring and visualizing. It&#8217;s more than a year since i&#8217;ve started using <em>Logstash</em>. It never turned me down. <code>ElasticSearch-Logstash-Kibana</code> (ELK) is a killer combination. Though i started Elasticsearch + Logstash as a log analyzer, later <a href="https://github.com/etsy/statsd/">StatsD</a> and <a href="http://graphite.wikidot.com/">Graphite</a> took it to the next level. When we have a simple infrastructure it&#8217;s easy to monitor. But when the infra starts scaling, it becomes quite difficult to keep track of all the events happening inside each nodes. Though service checks can help, but there is still limitation for it. I faced a lot of scenarios where things breaks but service checks will be fine. Under such scenarios logs are the only hope. They have all these events captured.</p>

<p>At Plivo, we manage a variety of servers from SIP, Media, Proxy, WebServers, DB&#8217;s etc. Being a fully Cloud based system, i really wanted to have a system which can keep track of all the live events/status of what&#8217;s really happening inside our infra. So my plan was to collect two important stats, 1) Server&#8217;s events 2) Application events.</p>

<h4>Collectd and Logstash</h4>

<p><code>Collectd</code> is a daemon which collects system performance statistics periodically. Since we have a lot Server&#8217;s which handle Realtime Media, it&#8217;s a very critical component for us. We need to ensure that the server&#8217;s are not getting overloaded and there is no latency in network. I&#8217;ve been using Logstash heavily for stashing all my logs. And there is a stable input plugin for collectd to send the all the system metrics to logstash.</p>

<p>First we need to enable the Network Plugin, and then we need to mention our Logstash server IP and port so that collectd can start injecting metrics. Below is a sample colectd configuration.</p>

<pre><code>Hostname    "test.plivo.com"
Interval 10
Timeout 4
Include "/etc/collectd/filters.conf"
Include "/etc/collectd/thresholds.conf"
ReportStats true
    LogLevel info
LoadPlugin interface
LoadPlugin load
LoadPlugin memory
LoadPlugin network
&lt;Plugin interface&gt;
    Interface "eth0"
    IgnoreSelected false
&lt;/Plugin&gt;
&lt;Plugin network&gt;
    Server "{logstash_server_ip}" "logstash_server_port"    # if no port number is mentioned, it will take the default port number (25826)
&lt;/Plugin&gt;
</code></pre>

<p>Now on the Logstash server, we need to add the CollectD plugin on to the input filter in the logstash&#8217;s config file.</p>

<pre><code>input {
      collectd {
      port =&gt; "5555"    # default port is 25826
      }
}
</code></pre>

<p>Now we are set. Based the plugins enabled in the collectd config file, collctd will start sending the metrics to Logstash on the Interval mentioned in the config, default is 10s. So in my case, i wanted the Load, CPU usage, Memory usage, Bandiwdth (TX and RX) etc. There are default plugins for all these metrics, which we can just enable it in the config file. We also had some custom plugins to collect some custom metrics. BTW writing custom plugin is pretty easy in Collectd.</p>

<p>Now using the Logstash&#8217;s Elasticsearch output plugin, we can keep these metrics in Elasticsearch. Now this where Kibana comes in. We can start visualizing these metrics via Kibana. We need to create a custom Lucene Query. Once we have the query, we can create a custom histogram&#8217;s for each of these queries. Below aresome sample Lucene queries that we can use with Kibana.</p>

<pre><code>For Load -&gt; collectd_type:"load" AND host:"test.plivo.com"
For Network usage -&gt; collectd_type:"if_octets" AND host:"test.plivo.com"
</code></pre>

<p>Below is the screenshot of  histogram for Load and Network (TX and RX)</p>

<p><img src="/images/collectd.png"></p>

<h5>Log Events</h5>

<p>Now next is to collect the events from the application logs. We use SIP protocol for all our VOIP sessions. So all our SIP server&#8217;s are very critical for us. SIP is pretty similar to HTTP. The response codes are very similar to HTTP responses, ie 1xx, 2xx, 3xx, 4xx, 5xx, 6xx. So i wrote some custom grok patterns so keep track of all of these responses and stores the same on the Elasticsearch.</p>

<p>The second stats which i was interested was our SIP registrar server. We provide SIP endpoints to our customers so that they can use the same with SIP/Soft phones. So i was more interested on stats like Number of registrations/sec, Auth error rates. Plus using ElasticSearch&#8217;s MAP facet&#8217;s i can create BetterMap. In my previous blog post&#8217;s i&#8217;ve mentioned on how to create these bettermaps using Kibana and Elasticsearch. Below bettermap screenshot shows us the SIP endpoint registrations from various locations in the last 2 hours.</p>

<p><img src="/images/bettermap.png"></p>

<p>Now using the Kibana we can start visualizing all these data&#8217;s. Below is a sample of Dashboard that i&#8217;ve created using Kibana.</p>

<p><img src="/images/event_logs.png"></p>

<p>ELK stack proved to be an amazing combination. We are currently injecting 3 million events every day and ElasticSearch was blazingly fast in indexing all theses.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/04/07/near-realtime-dashboard-with-kibana-and-elasticsearch/">Near RealTime Dashboard With Kibana and Elasticsearch</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-04-07T09:39:00+00:00" pubdate data-updated="true">Apr 7<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/elasticsearch/'>Elasticsearch</a>, <a class='category' href='/blog/categories/eventmonitoring/'>EventMonitoring</a>, <a class='category' href='/blog/categories/kibana/'>Kibana</a>, <a class='category' href='/blog/categories/monitoring/'>Monitoring</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>Being in <em>DevOps</em> it&#8217;s always Multi tasking. From Regular customer queries it goes through Monitoring, Troubleshooting etc. And offcourse when things breaks, it really becomes core multi tasking. Especially when you have a really scaling infrastructure, we should really understand what&#8217;s really happening in our infrastructure. Yes we do have many new generation cloud monitoring tools like Sensu, but what if we have a near real time system that can tell us the each and every events happeing in our infrastructure. Logs are the best places where we can keep track of the events, even if the monitoring tools has missed it. We have a lot of log aggregator tools like tool Logstash, Splunk, Apache Kafka etc. And for log based event collection the common choice will be always <strong>Logstash -> StatsD -> Graphite</strong>. And ElasticSearch for indexing these.</p>

<p>My requirement was pretty straight. Record the events, aggregate them and keeps track of them in a timely manner. Kibana uses ElasticSearch facets for aggregating the search query results.Facets provide aggregated data based on a search query. So as a first task, i decided to visualize the location of user&#8217;s who are registering their SIP endpoints on our SIP registrar server. Kibana gives us a good interface for the 2D heat map as well as a new option called <em>BetterMap</em>. Bettermap uses geographic coordinates to create clusters of markers on map and shade them orange, yellow and green depending on the density of the cluster. So from the logs, i just extracted the register events, and used a custom regex patterns to extract the details like the Source IP, usernames etc using logstash. Using the logstash&#8217;s GeoIP filter, the Geo Locations of the IP can be identified. For the BetterMap, we need  coordinates, in geojson format. <em>GeoJSON</em> is <strong>[longitude,latitude]</strong> in an array. From the Geo Locations that we have identified in the GeoIP filter, we can create this GeoJSON for each event that we are receiving. Below is a sample code that i&#8217;ve used  in logstash.conf for creating the GeoJSON in Logstash.</p>

<pre><code>if [source_ip]  {
    geoip {
      type =&gt; "kamailio-registers"
      source =&gt; "source_ip"
      target =&gt; "geoip"
      add_field =&gt; ["[geoip][coordinates]","%{[geoip][longitude]}"]
      add_field =&gt; ["[geoip][coordinates]","%{[geoip][latitude]}"]
    }
    mutate {
      convert =&gt; [ "[geoip][coordinates]", "float" ]
    }
  }
</code></pre>

<p>The above filter will create a GeoJSON array &#8220;geoip.coordinates&#8221;. This array can be used for creating the BetterMap in Kibana. Below are the settings for creating a BetterMap panel in the Kibana dashboard. While adding a new panel, select &#8220;bettermap&#8221; as the panel type, and the co-ordinate filed should be the one which contains the GeoJSON data. Make sure that the data is of the format [longitude,latitude], ie Longitude first and then followed by latitude.</p>

<p><img src="/images/bettermap_config.png"></p>

<p><img src="/images/bettermap.png"></p>

<p>Moving ahead, i decided to collect the events happening on our various other server&#8217;s. We were one of the earliest companies who started using SIP (Session Initiation Protocol). SIP employs design elements similar to the HTTP request/response transaction model. So similar to web traffic, i&#8217;ve decided to collect events related to 4XX, 5XX and 6XX error responses, as it is very important to us. Once the logs are shipped to logstash, i wrote another custom grok pattern, which extracts the Error Code and Error responses, including the server which returned the same. These data&#8217;s can be used for future analysis also. So i decided to store these on ElasticSearch. So now we have the real time event data&#8217;s stored, but how to visualize. Since i dont have to perform much mathematical analytics with data, i decided to to remove graphite. Kibana has a wonder full GUI for visualizing the data. So decided to go ahead with Kibana. One option is &#8220;histogram&#8221; panel time. Using histogram we can visualize the data via a regular bar graph, as well as using the area graph. There is another panel type called &#8220;terms&#8221; which can be used to display the agrregated events via pie chart, bar chart, or a table. And below is what i achieved with Kibana.</p>

<p><img src="/images/SIP-kibana.png"></p>

<p><img src="/images/SIP-pie.png"></p>

<p>This is just an inital setup. I&#8217;m going to add more events to this. As of now Kibana + Elasticsearch proves to be a promising combination for displaying all near real time events happening in my Infrastructure.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/30/event-monitoring-using-logstash-plus-statsd-plus-riemann/">Event Monitoring Using Logstash + StatsD + Riemann</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-03-30T14:48:00+00:00" pubdate data-updated="true">Mar 30<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/logstash/'>logstash</a>, <a class='category' href='/blog/categories/riemann/'>riemann</a>, <a class='category' href='/blog/categories/statsd/'>statsd</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>Being an OPS guy, i love Logs a lot. Logs contains lots of sensitive events recorded in it. Though a lot of people rely on monitoring tools, there are a lot of scenario where we still can&#8217;t rely on monitoring. In such scenarios, logs are the best sources to identify those events in a near real time fashion. A common scenario is Web Operations, where we need to count the the various 4xx, 5xx, Auth errors experienced by the user&#8217;s. I had a simliar requirement where i need to identify the 4xx, 5xx, 6xx errors and other similar failures on various SIP server&#8217;s. But apart from just visualising these error&#8217;s i also wanted a notification system which can notify me when the value crosses the threshold.</p>

<p><code>Logstash</code> and <code>StasD</code> is a perfect combination for aggregating events from the logs. StatsD has a Graphite backend, where it sends the aggreagated metric values for visualizing. But when we have large number graphs, and offcourse when being a multi tasking Ops guy, it&#8217;s not possible to sit and watch all these graphs. So we need a notification system which alert&#8217;s us when things starts breaking. Here comes <a href="http://riemann.io/">RIEMANN</a>. Riemann aggregates events from your servers and applications with a powerful stream processing language. Riemann is pretty light weight, easy to configure monitoring framework. Logstash sents the filtered events from the logs to StatsD output plugin. Based on the flushInterval, statsD iterates through the received events sents the aggregated metric values to the Graphite. There is also a Riemann output plugin for Logstash, but we need to pass the state/metric to the plugin. In my case, logstash filters the event from the log, so i need to converts these events to time based metric values. Since statsD already has these events converted into time series metrics, i decided to write a small backend for statsD that can send these aggregated metrics to Riemann.</p>

<p>The StatsD backend basically requires to main functions, one is &#8221;<em>flush_stats</em>&#8221; which will get invoked once the flush interval is reached. This function then iterates over the received metrics and passes these aggregated metrics to another function called &#8221;<em>post_stats</em>&#8221;, which sends the metrics to the corresponding aplications. In our case, we need to send the metrics to Riemann. There is a Riemann-Node plugin, which we can utilize here for sending the metrics to Riemann server. Below is the content for the &#8221;<em>flush_stats</em>&#8221; and &#8221;<em>post_stats</em>&#8221; functions. Currently i&#8217;ve added support only for counters. Soo i&#8217;ll be adding support for Counters and Timers also.</p>

<pre><code>flush_stats function
--------------------

var flush_stats = function riemann_flush(ts, metrics) {
var statString = '';
var numStats = 0;
var key;

var counters = metrics.counters;
var gauges = metrics.gauges;
var timers = metrics.timers;
var pctThreshold = metrics.pctThreshold;

for (key in counters) {
    var value = counters[key];
    var valuePerSecond = value / (flushInterval / 1000); // calculate "per second" rate

    statsString = value;
    service = key;
    time_stamp = ts;
    post_stats(statString, service_name, time_stamp);
}
}; 


post_stats function
-------------------

var post_stats = function riemann_post_metrics(statString, service_name, time_stamp) {

riemannStats.last_exception = Math.round(new Date().getTime() / 1000);

client.send(client.Event({
  service: service_name,
  metric:  statsString,
  time: time_stamp
}));
};
</code></pre>

<p>So here i&#8217;m not gonna send the per second metrics. I&#8217;m using the default 10 sec flushInterval. So every seconds StatsD will send the incremented metrics to Riemann. The namespace, sender etc are defined in the logstash conf itself. The full plugin file is available in <a href="https://gist.github.com/deepakmdass88/9851437">here</a></p>

<p>To use this Riemann backend, first we need to copy this file into the backend folder of the StatsD repo folder. Then we need to enable this plugin in the StatsD config file. Below is a sample config file which uses both graphite and Riemann backends.</p>

<pre><code>{
  riemannPort: 5555
, riemannHost: "localhost"
, graphitePort: 2003
, graphiteHost: "localhost"
, port: 8125
, backends: [ "./backends/riemann", "./backends/graphite" ]
}
</code></pre>

<p>So now StatsD will send out the incremented and the per second metric to Graphite and the Riemann backend will send the incremented metric to the Rieman server. No we can define the metric threshold and the notification method on the reimann config file. Below is my reimann metric threshold and notification.</p>

<pre><code>(streams
      (where (&gt;= metric 10)
        (where (service #"SIP")
          (email "deepakmdass88@gmail.com")))))
</code></pre>

<p>So whenever the recived metric value is beyond 10, Riemann will notify the same to my Email. I&#8217;ve done some dry testing with this setup. So far this setup never turned me down. Though there are some tweaks to be done, but this setup really suited to my requirement. being an OPS guy, my primary focus is to detect the outages at a very early stages to minimize the impact. Hope this guy will be an added defence layer for the same.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/02/23/building-im-server-using-kamailio/">Building IM Server Using Kamailio</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-02-23T14:59:00+00:00" pubdate data-updated="true">Feb 23<span>rd</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/im/'>IM</a>, <a class='category' href='/blog/categories/kamailio/'>Kamailio</a>, <a class='category' href='/blog/categories/sip/'>SIP</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>Instant messaging (IM) is a type of online chat which offers real-time text transmission over the Internet. Like XMPP, we can also use SIP for real-time text transmission. The Kamailio PRESENCE module helps us built the same locally. We can convert a simple Kamailio server into a full fledged IM server.</p>

<p>By default, all the registered endpoints will send a &#8220;PUBLISH&#8221; requests to the Presence server with their status and Presence server stores the status. Also the endpoints will send a &#8220;SUBSCRIBE&#8221; to other user&#8217;s status. For each &#8220;SUBSCRIBE&#8221; request, the Presence server will sent a &#8220;NOTIFY&#8221; request back with status of the user. This is how our UserAgent keeps track of each user&#8217;s status. Below is simple flow diagram for the same.</p>

<p><img src="http://beingasysadmin.files.wordpress.com/2014/02/screen-shot-2014-02-23-at-4-34-59-pm.png" alt="Alt text" /></p>

<h3>Setting up Kamailio</h3>

<p>In my <a href="" title="http://beingasysadmin.wordpress.com/2014/02/23/integrating-kamailio-with-freeswitch/">previous blog</a>, i&#8217;ve explained how to install Kamailio from source. The only difference is we need to enable two more modules &#8221;<strong>presence</strong>&#8221; and &#8221;<strong>presence_xml</strong>&#8221;. If Kamailio is already installed, we need to add these two modules into <code>modules.lst</code> at the &#8221;<strong>include_modules</strong>&#8221; line. Once the module names are added, we need to run &#8221;<strong>make install</strong>&#8221; to install the two new modules. Once the module is added, we need to enable the module.</p>

<p>Add &#8220;#!define WITH_PRESENCE&#8221; into the &#8220;kamailio.cfg&#8221; file. Then check if there is route logic defined for the &#8220;PRESENCE&#8221; module. By default there is a PRESENCE route defined in the default &#8220;kamailio.cfg&#8221; file. If it&#8217;s not there below is the route logic. Also we need to add &#8220;*route(PRESENCE)<strong>&#8221; in the &#8221;</strong>request_route**&#8221; section.</p>

<pre><code># Presence server route
route[PRESENCE] {
        if(!is_method("PUBLISH|SUBSCRIBE"))
                return;

        if(is_method("SUBSCRIBE") &amp;&amp; $hdr(Event)=="message-summary") {
                route(TOVOICEMAIL);
                # returns here if no voicemail server is configured
                sl_send_reply("404", "No voicemail service");
                exit;
        }

#!ifdef WITH_PRESENCE
        if (!t_newtran())
        {
                sl_reply_error();
                exit;
        }

        if(is_method("PUBLISH"))
        {
                handle_publish();
                t_release();
        } else if(is_method("SUBSCRIBE")) {
                handle_subscribe();
                t_release();
        }
        exit;
#!endif

        # if presence enabled, this part will not be executed
        if (is_method("PUBLISH") || $rU==$null)
        {
                xlog("@ 404 here 3");
                sl_send_reply("404", "Not here");
                exit;
        }
        return;
}
</code></pre>

<p>Now start the Kamailio server. Now we need to add some users. For that we can use &#8221;<strong>kamctl</strong>&#8221; binary.</p>

<pre><code>$ kamctl add user1@192.168.56.100 user1

$ kamctl add user2@192.168.56.100 user2
</code></pre>

<p>Let&#8217;s go ahead and test the settings. For testing we need some IM clients. In my testing, i&#8217;ve used <strong>Jitsi</strong> and <strong>adium</strong> IM clients. Once the accounts are configured on the clients, it&#8217;s better to start a packet capture using wireshark, so that we can see these <strong>PUBLISH</strong>, <strong>SUBSCRIBE</strong> and <strong>NOTIFY</strong> requests between the clients and the Kamailio server. If you are using <strong>Adium</strong>, go to account options and check the &#8220;Publish Status to Everyone&#8221; so that it will start sending <strong>PUBLISH</strong> request to the Kamailio server. Now add the accounts to the Contact lists on the IM clients and then we will be able to see the users status (ONline/OFFline). Now we can start the chat conversation between the user&#8217;s.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/02/23/integrating-kamailio-with-freeswitch/">Integrating Kamailio With FreeSwitch</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-02-23T11:05:00+00:00" pubdate data-updated="true">Feb 23<span>rd</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/kamailio/'>Kamailio</a>, <a class='category' href='/blog/categories/openser/'>OPENSER</a>, <a class='category' href='/blog/categories/freeswitch/'>freeswitch</a>, <a class='category' href='/blog/categories/voip/'>voip</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p><code>Kamailio</code> aka <code>OpenSER</code> is one of the most powerfull and popular Open Source SIP server. It can be used as <strong>SIP Proxy/Registrar/LB/Router</strong> etc. It also provides a lot of features <em>like WebSocket support for WebRTC, ; SIMPLE instant messaging and presence with embedded XCAP server and MSRP relay,IMS extensions,ENUM and offcourse AAA (accounting, authentication and authorization)</em> also. Kamailio is a modular system, ie, it has lot of modules which corresponds to particular functions. These modules can be easily installed and can be used easily in Kamailio. In this blog i&#8217;m going to use Kamailio as a proxy server. All the user&#8217;s are created in the Kamailio and FreeSwitch will be acting as a relay server for outbound calls. So Kamailio performs authentication and all the outbound calls wil be relayed to FreeSwitch.</p>

<h3>Installing Kamailio</h3>

<p>Lets download the latest version of Kamailio, now it&#8217;s 4.1</p>

<pre><code>$ wget http://www.Kamailio.org/pub/Kamailio/latest/src/Kamailio-4.1.1_src.tar.gz

$ tar xvzf Kamailio-4.1.1_src.tar.gz &amp;&amp; cd Kamailio-4.1.1
</code></pre>

<p>Before we start the build we need to install the basic dependencies.</p>

<pre><code>$ apt-get install gcc flex bison libmysqlclient-dev make libxml2-dev libssl-dev
</code></pre>

<p>Now we have the dependencies installed, we can go ahead with the build.</p>

<pre><code>$ make cfg      # generates config files for build system
</code></pre>

<p>Now open <code>modules.lst</code> and add the modules to be installed in &#8221;<strong>include_modules</strong>&#8221; section. In my case i&#8217;m going to use MySQL backend so it will be &#8221;<strong>include_modules= db_mysql</strong>&#8221; and then we can run the &#8221;<em>make all</em>&#8221;. The other way is we can mention the modules directly while running the &#8220;make&#8221; rather editing the modules.lst file.</p>

<pre><code>$ make include_modules="db_mysql" cfg
</code></pre>

<p>Now we can install,</p>

<pre><code>$ make install
</code></pre>

<p>The above command will install Kamailio to our system. There are four main binaries for Kamailio,</p>

<pre><code>Kamailio - Kamailio SIP server
kamdbctl - script to create and manage the Databases
kamctl - script to manage and control Kamailio SIP server
kamcmd - CLI - command line tool to interface with Kamailio SIP server
</code></pre>

<p>There is also one configuration file called &#8220;Kamailio.cfg&#8221; which is available by default at <code>/usr/local/etc/Kamailio/Kamailio.cfg</code></p>

<p>Let&#8217;s go ahead with setting up MySQL server for Kamailio.</p>

<pre><code>$ apt-get install mysql-server
</code></pre>

<p>Now edit the <code>/usr/local/etc/Kamailio/kamctlrc</code> Locate DBENGINE variable and set it to MYSQL by making &#8221;<strong>DBENGINE=MYSQL</strong>&#8221;. Now we can use the &#8220;kamdbctl&#8221; binary to craete the default tables and users.</p>

<pre><code>$ /usr/local/sbin/kamdbctl create
</code></pre>

<p>The script will add two users in MySQL:</p>

<ul>
<li><p>Kamailio - (with default password &#8216;Kamailiorw&#8217;) - user which has full access rights to &#8216;Kamailio&#8217; database</p></li>
<li><p>Kamailioro - (with default password &#8216;Kamailioro&#8217;) - user which has read-only access rights to &#8216;Kamailio&#8217; database</p></li>
</ul>


<p>There is a sample init.d script available along with Kamailio, which we can use it. We need to copy the sample init.d file to our system&#8217;s init.d folder. And the same for the system default file.</p>

<pre><code>$ cp /usr/local/src/Kamailio-4.1.1/pkg/Kamailio/deb/precise/Kamailio.init /etc/init.d/Kamailio

$ cp /usr/local/src/Kamailio-4.1.1/pkg/Kamailio/deb/precise/Kamailio.default /etc/default/Kamailio

$ chmod 755 /etc/init.d/Kamailio 
</code></pre>

<p>Edit the new init file and modify the $DAEMON and $CFGFILE values.</p>

<pre><code>DAEMON=/usr/local/sbin/Kamailio
CFGFILE=/usr/local/etc/Kamailio/Kamailio.cfg

$ mkdir -p /var/run/Kamailio    # Directory for the pid file
</code></pre>

<p>Default setting is to run Kamailio as user &#8221;<em>Kamailio</em>&#8221; and group &#8221;<em>Kamailio</em>&#8221;. For that we need to create the user:</p>

<pre><code>$ adduser --quiet --system --group --disabled-password \
      --shell /bin/false --gecos "Kamailio" \
      --home /var/run/Kamailio Kamailio

$ chown Kamailio:Kamailio /var/run/Kamailio
</code></pre>

<h3>Setting up Kamailio</h3>

<p>All the Kamailio configurations are mentioned in only one single file <code>/usr/local/etc/Kamailio/Kamailio.cfg</code>. All the logics are defined in this file, and Kamailio blindly executes this logics and perform the actions. It&#8217;s very important that the logics defined in the config should suit to our VOIP platform requirement.</p>

<p>First we need to enable the modules and the necessary features, so add the below lines in the Kamailio.cfg</p>

<pre><code>#!define WITH_MYSQL
#!define WITH_AUTH
#!define WITH_USRLOCDB
#!define WITH_FREESWITCH
</code></pre>

<p>We need to define the FreeSwitch server IP and port, for that we can add the below parameters in the &#8220;Custom Parameters&#8221; section.</p>

<pre><code>#!ifdef WITH_FREESWITCH
freeswitch.bindip = "192.168.56.100" desc "FreeSWITCH IP Address"
freeswitch.bindport = "5090" desc "FreeSWITCH Port"
#!endif
</code></pre>

<p>Now we can go ahead to the &#8221;<strong>request_route</strong>&#8221; section which performs the routing logic. Here i&#8217;m going to add two more routing logic for the FreeSwitch relay. After the &#8221;<strong>request_route</strong>&#8221; section, we can see the definition for each routing options. Below that we need to add our new route definitions.</p>

<pre><code>route[FSDISPATCH] {
        # dial number selection
        route(FSRELAY);
        exit;
}

route[FSRELAY] {
        $du = "sip:" + $sel(cfg_get.freeswitch.bindip) + ":" + $sel(cfg_get.freeswitch.bindport);
        if($var(newbranch)==1)
        {
                append_branch();
                $var(newbranch) = 0;
        }
        route(RELAY);
        exit;
}
</code></pre>

<p>By default, all the routes mentioned in the &#8220;request_route&#8221; will be executed line by line. There is a default route called &#8221;<strong>Location</strong>&#8221;, which splits the user part from the request URI and verifies if the user exists in the location table. But when we dial an outside number/user, this location check will fail, so i&#8217;m going to add a condition which checks if the user in the request URI contains a number with a length 9-15 will be relayed to the FreeSwitch. Again this is just a simple condition, we can create a more complex condition, like check the domain part, if the domain part contains a domain which doesnot belong to our Domain list, we can either decline the request, or we can relay to FreeSwitch or we can make DNS query and we can make Kamailio to process the request to that domain&#8217;s Proxy server. Like this we can define our own conditions in the config file, and Kamailio will execute it line by line.</p>

<p>I&#8217;m going to add my check condition on the &#8221;<strong>LOCATION</strong>&#8221; route definition.</p>

<pre><code>route[LOCATION] {

    #!ifdef WITH_SPEEDDIAL
        # search for short dialing - 2-digit extension
        if($rU=~"^[0-9][0-9]$")
                if(sd_lookup("speed_dial"))
                        route(SIPOUT);
    #!endif
        if($rU=~"^[0-9]{9,15}$")        # checking for numbers in the Request URI
                route(FSDISPATCH);
    #!ifdef WITH_ALIASDB
        # search in DB-based aliases
        if(alias_db_lookup("dbaliases"))
                route(SIPOUT);
    #!endif
        $avp(oexten) = $rU;
        if (!lookup("location")) {
                xlog("L_INFO", "CALL $rm $ci lookup here\n");
                xlog("L_INFO", "$fU@$fd - Lookup contact location for $rm\n");
                xlog("L_INFO", "rc is $var(rc)");
                switch ($rc) {
                        case -1:
                        case -3:
                                xlog("L_ERR", "$fU@$fd - No contact found\n");
                                send_reply("404", "Not Found here");
                                exit;
                        case -2:
                                send_reply("405", "Method Not Allowed");
                                exit;
                }
        }

        # when routing via usrloc, log the missed calls also
        if (is_method("INVITE"))
        {
                setflag(FLT_ACCMISSED);
        }
        xlog("L_INFO", "CALL $rm $ci relay\n");
        xlog("L_INFO", "$fU@$fd - Relaying $rm\n");
            route(RELAY);
        exit;
}
</code></pre>

<p>So now all the calls coming with numbers of length 9-15 in the Request URI will be relayed to the FreeSwitch, and FreeSwitch will process the call based on the DialPlan configured in the FreeSwitch. Since i&#8217;m going to use IP authentication, i need to whitelist the Kamailio ip in &#8220;acl_conf.xml&#8221; file in the FreeSwitch autload conf directory, so that FreeSwitch will accept the invites from Kamailio. Again i&#8217;m not defining any Voicemail options here. If we have a Voice mail server, then we can create another route option and when the caller doesn&#8217;t pick the call we can route the call to the Voice Mail server.
For example the below condition will route the failures to FreeSwitch Voice Mailbox.</p>

<pre><code>if(is_method("INVITE"))
        {
            # in case of failure - re-route to FreeSWITCH VoiceMail
            t_on_failure("FAIL_FSVBOX");        # where FSVBOX is a route definition
        }
</code></pre>

<p>Kamailio has a lot of modules which really comes in handy. For example we can use LDAP module to use LDAP as a backend. There is a PRESENCE module which helps us to setup an Instant Messaging server using Kamailio. I&#8217;ll be writing a blog on how to use Kamailio as an IM server soon. One of the main advantage of Kamailio over OpenSIP is the WebSocket support. This just a basic configuration, but we can design much complex system using Kamailio. We can even remove the default route logics, and we can use our own routing logics. Kamailio doesnot depend on default routing logics, it blindly reads the route and executes it for each incoming connections.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/02/14/sip-trunking-using-plivo-and-freeswitch/">SIP Trunking With PLIVO and FreeSwitch</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-02-14T15:05:00+00:00" pubdate data-updated="true">Feb 14<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/freeswitch/'>FreeSwitch</a>, <a class='category' href='/blog/categories/plivo/'>Plivo</a>, <a class='category' href='/blog/categories/siptrunking/'>SIPTrunking</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>It&#8217;s more than a month since i joined the DevOps family at <strong>Plivo</strong>, since i&#8217;m pretty new to the Telecom Technology, i was digging more around it. This time i decided to play around with <strong>FreeSwitch</strong>, a free and open source communications software for the creation of voice and messaging products. Thanks to <a href="http://www.linkedin.com/pub/anthony-minessale-ii/3a/403/942">Anthony Minessale</a> for designing and opensourcing such a powerfull powerfull application. FreeSwitch is well documented and there are pretty good blogs also available on how to setup a PBX using FreeSwitch. This time i&#8217;m going to explain on how to make a private Freeswitch server to use Plivo as a <em>SIP Trunking</em> service.</p>

<p>A bit about <a href="plivo.com">Plivo</a>. Plivo is a cloud based API Platform for building <em>Voice</em> and <em>SMS</em> enabled Applications. Plivo provides <strong>Application Programming Interfaces</strong> (APIs) to <em>make and receive calls, send SMS, make a conference call</em>, and more. These APIs are used in conjunction with XML responses to control the flow of a call or a message. We can create Session Initiation Protocol (SIP) endpoints to perform the telephony operations and the APIs are platform independent and can be used in any programming environment such as PHP, Ruby, Python, etc. It also provides helper libraries for these programming languages.</p>

<p>First we need a valid Plivo account. Once we have the Plivo account, we can log into the Plivo Cloud service. Now go to the &#8221;<strong>Endpoints</strong>&#8221; tab and create a SIP endpoint and attach a DirectDial app to it. Once this is done we can go ahead and start setting up the FreeSwitch instance.</p>

<h3>Installing FreeSwitch</h3>

<p>Clone the Official FreeSwitch Github and Repo and compile from the source.</p>

<pre><code>$ git clone git://git.freeswitch.org/freeswitch.git &amp;&amp; cd freeswitch

$ ./bootstrap.sh &amp;&amp; ./configure --prefix=/usr/local/freeswitch

$ make &amp;&amp; make install

$  make all cd-sounds-install cd-moh-install    # optional, run this if you want IVR and Music on Hold features
</code></pre>

<p>Now if we have more than one ip address on the machine, and if we want to bind to a particular ip, we need to modify two files &#8221;<em>/usr/local/freeswitch/conf/sip_profiles/external.xml</em>&#8221; and &#8221;<em>/usr/local/freeswitch/conf/sip_profiles/internal.xml</em>&#8221;. In both the files, change the parameters &#8221;<strong>name=&#8221;rtp-ip&#8221;</strong>&#8221; and &#8221;<strong>param name=&#8221;sip-ip&#8221;</strong>&#8221; with the <em>bind ip</em> as the values.</p>

<p>By default, Freeswitch will create a set of users, which includes numerical usernames ie, 1000-1019. So we can test the basic connectivity between user&#8217;s by making a call between two user accounts. We can register two of the accounts in two SoftPhones and we can make a test call and make sure that FreeSwitch is working fine. We can use the FS binary file to start FreeSwitch service in forground.</p>

<pre><code>$ /usr/local/freeswitch/bin/freeswitch
</code></pre>

<h3>Configuring Gateway</h3>

<p>Once the FreeSwitch is working fine, we can start configuring the SIP trunking via Plivo. So first we need to create an external gateway to connect to Plivo. I&#8217;m going to use the SIP endpoint created on the Plivo Cloud to initiate the connection. The SIP domain for Plivo is &#8221;<strong>phone.plivo.com</strong>&#8221;. We need to create a gateway config. Go to &#8221;<em>/usr/local/freeswitch/conf/sip_profiles/external/</em>&#8221;, here we can create an XML gateway config file. My config file name is <strong>plivo</strong>. Below is the content for the same.</p>

<pre><code>&lt;include&gt;
  &lt;gateway name="plivo"&gt;
  &lt;param name="realm" value="phone.plivo.com" /&gt;
  &lt;param name="username" value="test88140105181635" /&gt;
  &lt;param name="password" value="secret" /&gt;
  &lt;param name="register" value="false" /&gt;
  &lt;param name="ping" value="5" /&gt;
  &lt;param name="ping-max" value="3" /&gt;
  &lt;param name="retry-seconds" value="5" /&gt;
  &lt;param name="expire-seconds" value="60" /&gt;
  &lt;variables&gt;
        &lt;variable name="verbose_sdp" value="true"/&gt;
  &lt;/variables&gt;
  &lt;/gateway&gt;
&lt;/include&gt;
</code></pre>

<p>There are a lot of other parameters which we can add it here, like caller id etc. Replace the username and password with the Plivo endpoint credentials. If we want to keep this endpoint registered, we can set the register param as true and we can set the expiry time at expire-seconds, so that the Fs will keep on registering the Endpoint with Plivo&#8217;s Registrar server. once the gateway file is created, we can either restart the service or we can run &#8220;reload mod_sofia&#8221; on the FScli. If the FreeSwitch service si started in foreground, we will get the FScli, so we can run the reload command directly on it.</p>

<h3>Setting up Dialplan</h3>

<p>Now we have the Gateway added. Now we need to the setup the Dial Plan to route the outgoing calls through Plivo. Go to &#8221;<em>/usr/local/freeswitch/conf/dialplan/</em>&#8221; folder and add an extension on the &#8221;<strong>public.xml</strong>&#8221; file. Below is a sample extension config.</p>

<pre><code>&lt;extension name="Calls to Plivo"&gt;
      &lt;condition field="destination_number" expression="^(&lt;ur_regex_here&gt;)$"&gt;
        &lt;action application="transfer" data="$1 XML default"/&gt;
      &lt;/condition&gt;
&lt;/extension&gt;
</code></pre>

<p>So now all calls matching to the Regex will be transferred to the default dial plan. Now on the the default dial plan, i&#8217;m creating an exntension and will use the FreeSwitch&#8217;s &#8221;<em>bridge</em>&#8221; application to brdige the call with Plivo using the Plivo Gateway. So on the &#8221;<em>default.xml</em>&#8221; add the below extension.</p>

<pre><code>   &lt;extension name="Dial through Plivo"&gt;
         &lt;condition field="destination_number" expression="^(&lt;ur_regex_here&gt;)$"&gt;
           &lt;action application="bridge" data="sofia/gateway/plivo/$1"/&gt;
         &lt;/condition&gt;
       &lt;/extension&gt;
</code></pre>

<p>Now we can restart the FS service or we can reload &#8220;mod_dialplan_xml&#8221; from the FScli. Once the changes are into effect, we can test whether the call is getting routed via Plivo. Configure a soft phone with a default FS user and make an outbound call which matches the regex that we have mentioned for routing to Plivo. Now if all works we should get a call on the destination number. We can check the FS logs at &#8221;<em>/usr/local/freeswitch/log/freeswitch.log</em>&#8221;.</p>

<p>If the Regex is matched, we can see the below lines in the log.</p>

<pre><code>1f249a72-9abf-4713-ba69-c2881111a0e8 Dialplan: sofia/internal/1001@192.168.56.11 parsing [public-&gt;Calls from BoxB] continue=false
1f249a72-9abf-4713-ba69-c2881111a0e8 EXECUTE sofia/internal/1001@192.168.56.11 transfer(xxxxxxxxxxxx XML default)
1f249a72-9abf-4713-ba69-c2881111a0e8 Dialplan: sofia/internal/1001@192.168.56.11 Regex (PASS) [Dial through Plivo] destination_number(xxxxxxxxxxxx) =~ /^(xxxxxxxxxxxx)$/ break=on-false
1f249a72-9abf-4713-ba69-c2881111a0e8 Dialplan: sofia/internal/1001@192.168.56.11 Action bridge(sofia/external/plivo/xxxxxxxxxxxx@phone.plivo.com)   
1f249a72-9abf-4713-ba69-c2881111a0e8 EXECUTE sofia/internal/1001@192.168.56.11 bridge(sofia/external/plivo/xxxxxxxxxxxx@phone.plivo.com)
1f249a72-9abf-4713-ba69-c2881111a0e8 2014-02-14 06:32:48.244757 [DEBUG] mod_sofia.c:4499 [zrtp_passthru] Setting a-leg inherit_codec=true
1f249a72-9abf-4713-ba69-c2881111a0e8 2014-02-14 06:32:48.244757 [DEBUG] mod_sofia.c:4502 [zrtp_passthru] Setting b-leg absolute_codec_string=GSM@8000h@20i@13200b,PCMA@8000h@20i@64000b,PCMU@8000h@20i@64000b
</code></pre>

<p>We can also mention the CallerID on the Direct Dial app which we have mapped to the SIP endpoint. Now for Incoming calls, create a app that can forward the calls to one of the user&#8217;s present in the FreeSwitch, using the Plivo&#8217;s Dial XML. So the XML should look something like below. I will be writing a more detailed blog about Inbound calls once i&#8217;ve have tested it out completely.</p>

<pre><code>&lt;Response&gt;
  &lt;Dial&gt;
    &lt;User&gt;FSuser@FSserverIP&lt;/User&gt;
  &lt;/Dial&gt;
&lt;/Response&gt;
</code></pre>

<p>But for security, we need to allow connections from Plivo server. So we need to allow those IP&#8217;s on the FS acl. We can allow the IP&#8217;s in the &#8221;<strong>acl.conf.xml</strong>&#8221; file at &#8221;<em>/usr/local/freeswitch/conf/autoload_configs</em>&#8221;. And make sure that the FS server is accessible via a public ip atleast for the Plivo server&#8217;s which will forward the calls.</p>
</div>
  
  


</div>

      </article>
    
    </div>
      <div class="row">
        <ul class="pager">
          
            <li class="previous">
              <a href="/blog/page/3/">&larr; Older</a>
            </li>
          
          
            <li class="next">
              <a class="next" href="/">Newer &rarr;</a>
            </li>
          
        </ul>
      </div>
  </div>
</div>


  <div id="footer-widgets">
  <div class="container">
    <div class="row">
  <div class="span3">
    <h2>recent posts</h2>
    <ul class="recent_posts">
      
        <li>
          <a href="/blog/2015/01/06/setting-up-docker-private-registry/">Setting up Docker Private Registry</a>
        </li>
      
        <li>
          <a href="/blog/2014/12/07/automating-debian-package-management/">Automating Debian Package Management</a>
        </li>
      
        <li>
          <a href="/blog/2014/12/07/managing-debian-apt-repository-via-aptly/">Managing Debian APT repository via Aptly</a>
        </li>
      
        <li>
          <a href="/blog/2014/12/03/building-debian-packages/">Building Debian packages</a>
        </li>
      
        <li>
          <a href="/blog/2014/10/23/sippy-cup-freeswitch-load-test-simplified/">sippy_cup - FreeSwitch Load Test Simplified</a>
        </li>
      
    </ul>
    <h2><a href="/blog/archives">archives</a></h2>
  </div>
  <div class="span3">
    <h2>instagram</h2>
    <div class="instagram"></div>
    <button id="instabutton" class="btn">more</button>
  </div>
  <div class="span4">
    <h2>twitter</h2>
    <a href="https://twitter.com/deepakmdass88" class="twitter-follow-button" data-show-count="true" data-lang="en">Follow @deepakmdass88</a>
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
    <div class="tweet">
    </div>
  </div>
  <div class="span2">
    <h2>found on</h2>
    <a href="https://github.com/deepakmdass88/" rel="tooltip" title="Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a>
    <a href="http://www.linkedin.com/pub/deepak-dass/44/54/602" rel="tooltip" title="Linkedin"><img class="social_icon" title="Linkedin" alt="Linkedin icon" src="/images/glyphicons_377_linked_in.png"></a>
    <a href="http://twitter.com/deepakmdass88" rel="tooltip" title="Twitter"><img class="social_icon" title="Twitter" alt="Twitter icon" src="/images/glyphicons_391_twitter_t.png"></a>
    <a href="https://plus.google.com/105770729176086017609/posts" rel="tooltip" title="Google Plus"><img class="social_icon" title="Google Plus" alt="Google Plus icon" src="/images/glyphicons_386_google_plus.png"></a>
    <a href="http://ttp://www.quora.com/Deepak-M-Dass" rel="tooltip" title="Quora"><img class="social_icon" title="Quora" alt="Quora icon" src="/images/glyphicons_385_quora.png"></a>
    <h2>contact at</h2>
    <a href="mailto:deepakmdass88@gmail.com">deepakmdass88@gmail.com</a>
  </div>
</div>

  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-left">
  <a href="/">Welcome to My Nerd World</a>
  - Copyright &copy; 2015 - Deepak M Das
</p>
<p class="pull-right">
  Powered by <a href="http://octopress.org/">Octopress</a>. Designed by <a href="http://www.AdrianArtiles.com">Adrian Artiles</a>.
</p>

  </div>
</footer>

  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript"></script>
<script>window.jQuery || document.write('<script src="/javascripts/libs/jquery-1.7.2.min.js" type="text/javascript"><\/script>')</script>
<script src="/javascripts/libs/bootstrap.min.js" type="text/javascript"></script>
<script src="/javascripts/jquery.tweet.js" type="text/javascript"></script>
<script src="/javascripts/jquery.instagram.js" type="text/javascript"></script>
<script src="/javascripts/custom.js" type="text/javascript"></script>





</body>
</html>
