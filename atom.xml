<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Welcome to My Nerd World]]></title>
  <link href="http://beingasysadmin.com/atom.xml" rel="self"/>
  <link href="http://beingasysadmin.com/"/>
  <updated>2014-04-07T12:00:32+00:00</updated>
  <id>http://beingasysadmin.com/</id>
  <author>
    <name><![CDATA[Deepak M Das]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Near RealTime Dashboard with Kibana and Elasticsearch]]></title>
    <link href="http://beingasysadmin.com/blog/2014/04/07/near-realtime-dashboard-with-kibana-and-elasticsearch/"/>
    <updated>2014-04-07T09:39:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2014/04/07/near-realtime-dashboard-with-kibana-and-elasticsearch</id>
    <content type="html"><![CDATA[<p>Being in <em>DevOps</em> it&#8217;s always Multi tasking. From Regular customer queries it goes through Monitoring, Troubleshooting etc. And offcourse when things breaks, it really becomes core multi tasking. Especially when you have a really scaling infrastructure, we should really understand what&#8217;s really happening in our infrastructure. Yes we do have many new generation cloud monitoring tools like Sensu, but what if we have a near real time system that can tell us the each and every events happeing in our infrastructure. Logs are the best places where we can keep track of the events, even if the monitoring tools has missed it. We have a lot of log aggregator tools like tool Logstash, Splunk, Apache Kafka etc. And for log based event collection the common choice will be always <strong>Logstash -> StatsD -> Graphite</strong>. And ElasticSearch for indexing these.</p>

<p>My requirement was pretty straight. Record the events, aggregate them and keeps track of them in a timely manner. Kibana uses ElasticSearch facets for aggregating the search query results.Facets provide aggregated data based on a search query. So as a first task, i decided to visualize the location of user&#8217;s who are registering their SIP endpoints on our SIP registrar server. Kibana gives us a good interface for the 2D heat map as well as a new option called <em>BetterMap</em>. Bettermap uses geographic coordinates to create clusters of markers on map and shade them orange, yellow and green depending on the density of the cluster. So from the logs, i just extracted the register events, and used a custom regex patterns to extract the details like the Source IP, usernames etc using logstash. Using the logstash&#8217;s GeoIP filter, the Geo Locations of the IP can be identified. For the BetterMap, we need  coordinates, in geojson format. <em>GeoJSON</em> is <strong>[longitude,latitude]</strong> in an array. From the Geo Locations that we have identified in the GeoIP filter, we can create this GeoJSON for each event that we are receiving. Below is a sample code that i&#8217;ve used  in logstash.conf for creating the GeoJSON in Logstash.</p>

<pre><code>if [source_ip]  {
    geoip {
      type =&gt; "kamailio-registers"
      source =&gt; "source_ip"
      target =&gt; "geoip"
      add_field =&gt; ["[geoip][coordinates]","%{[geoip][longitude]}"]
      add_field =&gt; ["[geoip][coordinates]","%{[geoip][latitude]}"]
    }
    mutate {
      convert =&gt; [ "[geoip][coordinates]", "float" ]
    }
  }
</code></pre>

<p>The above filter will create a GeoJSON array &#8220;geoip.coordinates&#8221;. This array can be used for creating the BetterMap in Kibana. Below are the settings for creating a BetterMap panel in the Kibana dashboard. While adding a new panel, select &#8220;bettermap&#8221; as the panel type, and the co-ordinate filed should be the one which contains the GeoJSON data. Make sure that the data is of the format [longitude,latitude], ie Longitude first and then followed by latitude.</p>

<p><img src="http://beingasysadmin.com/images/bettermap_config.png"></p>

<p><img src="http://beingasysadmin.com/images/bettermap.png"></p>

<p>Moving ahead, i decided to collect the events happening on our various other server&#8217;s. We were one of the earliest companies who started using SIP (Session Initiation Protocol). SIP employs design elements similar to the HTTP request/response transaction model. So similar to web traffic, i&#8217;ve decided to collect events related to 4XX, 5XX and 6XX error responses, as it is very important to us. Once the logs are shipped to logstash, i wrote another custom grok pattern, which extracts the Error Code and Error responses, including the server which returned the same. These data&#8217;s can be used for future analysis also. So i decided to store these on ElasticSearch. So now we have the real time event data&#8217;s stored, but how to visualize. Since i dont have to perform much mathematical analytics with data, i decided to to remove graphite. Kibana has a wonder full GUI for visualizing the data. So decided to go ahead with Kibana. One option is &#8220;histogram&#8221; panel time. Using histogram we can visualize the data via a regular bar graph, as well as using the area graph. There is another panel type called &#8220;terms&#8221; which can be used to display the agrregated events via pie chart, bar chart, or a table. And below is what i achieved with Kibana.</p>

<p><img src="http://beingasysadmin.com/images/SIP-kibana.png"></p>

<p><img src="http://beingasysadmin.com/images/SIP-pie.png"></p>

<p>This is just an inital setup. I&#8217;m going to add more events to this. As of now Kibana + Elasticsearch proves to be a promising combination for displaying all near real time events happening in my Infrastructure.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Event Monitoring using Logstash + StatsD + Riemann]]></title>
    <link href="http://beingasysadmin.com/blog/2014/03/30/event-monitoring-using-logstash-plus-statsd-plus-riemann/"/>
    <updated>2014-03-30T14:48:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2014/03/30/event-monitoring-using-logstash-plus-statsd-plus-riemann</id>
    <content type="html"><![CDATA[<p>Being an OPS guy, i love Logs a lot. Logs contains lots of sensitive events recorded in it. Though a lot of people rely on monitoring tools, there are a lot of scenario where we still can&#8217;t rely on monitoring. In such scenarios, logs are the best sources to identify those events in a near real time fashion. A common scenario is Web Operations, where we need to count the the various 4xx, 5xx, Auth errors experienced by the user&#8217;s. I had a simliar requirement where i need to identify the 4xx, 5xx, 6xx errors and other similar failures on various SIP server&#8217;s. But apart from just visualising these error&#8217;s i also wanted a notification system which can notify me when the value crosses the threshold.</p>

<p><code>Logstash</code> and <code>StasD</code> is a perfect combination for aggregating events from the logs. StatsD has a Graphite backend, where it sends the aggreagated metric values for visualizing. But when we have large number graphs, and offcourse when being a multi tasking Ops guy, it&#8217;s not possible to sit and watch all these graphs. So we need a notification system which alert&#8217;s us when things starts breaking. Here comes <a href="http://riemann.io/">RIEMANN</a>. Riemann aggregates events from your servers and applications with a powerful stream processing language. Riemann is pretty light weight, easy to configure monitoring framework. Logstash sents the filtered events from the logs to StatsD output plugin. Based on the flushInterval, statsD iterates through the received events sents the aggregated metric values to the Graphite. There is also a Riemann output plugin for Logstash, but we need to pass the state/metric to the plugin. In my case, logstash filters the event from the log, so i need to converts these events to time based metric values. Since statsD already has these events converted into time series metrics, i decided to write a small backend for statsD that can send these aggregated metrics to Riemann.</p>

<p>The StatsD backend basically requires to main functions, one is &#8221;<em>flush_stats</em>&#8221; which will get invoked once the flush interval is reached. This function then iterates over the received metrics and passes these aggregated metrics to another function called &#8221;<em>post_stats</em>&#8221;, which sends the metrics to the corresponding aplications. In our case, we need to send the metrics to Riemann. There is a Riemann-Node plugin, which we can utilize here for sending the metrics to Riemann server. Below is the content for the &#8221;<em>flush_stats</em>&#8221; and &#8221;<em>post_stats</em>&#8221; functions. Currently i&#8217;ve added support only for counters. Soo i&#8217;ll be adding support for Counters and Timers also.</p>

<pre><code>flush_stats function
--------------------

var flush_stats = function riemann_flush(ts, metrics) {
var statString = '';
var numStats = 0;
var key;

var counters = metrics.counters;
var gauges = metrics.gauges;
var timers = metrics.timers;
var pctThreshold = metrics.pctThreshold;

for (key in counters) {
    var value = counters[key];
    var valuePerSecond = value / (flushInterval / 1000); // calculate "per second" rate

    statsString = value;
    service = key;
    time_stamp = ts;
    post_stats(statString, service_name, time_stamp);
}
}; 


post_stats function
-------------------

var post_stats = function riemann_post_metrics(statString, service_name, time_stamp) {

riemannStats.last_exception = Math.round(new Date().getTime() / 1000);

client.send(client.Event({
  service: service_name,
  metric:  statsString,
  time: time_stamp
}));
};
</code></pre>

<p>So here i&#8217;m not gonna send the per second metrics. I&#8217;m using the default 10 sec flushInterval. So every seconds StatsD will send the incremented metrics to Riemann. The namespace, sender etc are defined in the logstash conf itself. The full plugin file is available in <a href="https://gist.github.com/deepakmdass88/9851437">here</a></p>

<p>To use this Riemann backend, first we need to copy this file into the backend folder of the StatsD repo folder. Then we need to enable this plugin in the StatsD config file. Below is a sample config file which uses both graphite and Riemann backends.</p>

<pre><code>{
  riemannPort: 5555
, riemannHost: "localhost"
, graphitePort: 2003
, graphiteHost: "localhost"
, port: 8125
, backends: [ "./backends/riemann", "./backends/graphite" ]
}
</code></pre>

<p>So now StatsD will send out the incremented and the per second metric to Graphite and the Riemann backend will send the incremented metric to the Rieman server. No we can define the metric threshold and the notification method on the reimann config file. Below is my reimann metric threshold and notification.</p>

<pre><code>(streams
      (where (&gt;= metric 10)
        (where (service #"SIP")
          (email "deepakmdass88@gmail.com")))))
</code></pre>

<p>So whenever the recived metric value is beyond 10, Riemann will notify the same to my Email. I&#8217;ve done some dry testing with this setup. So far this setup never turned me down. Though there are some tweaks to be done, but this setup really suited to my requirement. being an OPS guy, my primary focus is to detect the outages at a very early stages to minimize the impact. Hope this guy will be an added defence layer for the same.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building IM Server using Kamailio]]></title>
    <link href="http://beingasysadmin.com/blog/2014/02/23/building-im-server-using-kamailio/"/>
    <updated>2014-02-23T14:59:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2014/02/23/building-im-server-using-kamailio</id>
    <content type="html"><![CDATA[<p>Instant messaging (IM) is a type of online chat which offers real-time text transmission over the Internet. Like XMPP, we can also use SIP for real-time text transmission. The Kamailio PRESENCE module helps us built the same locally. We can convert a simple Kamailio server into a full fledged IM server.</p>

<p>By default, all the registered endpoints will send a &#8220;PUBLISH&#8221; requests to the Presence server with their status and Presence server stores the status. Also the endpoints will send a &#8220;SUBSCRIBE&#8221; to other user&#8217;s status. For each &#8220;SUBSCRIBE&#8221; request, the Presence server will sent a &#8220;NOTIFY&#8221; request back with status of the user. This is how our UserAgent keeps track of each user&#8217;s status. Below is simple flow diagram for the same.</p>

<p><img src="http://beingasysadmin.files.wordpress.com/2014/02/screen-shot-2014-02-23-at-4-34-59-pm.png" alt="Alt text" /></p>

<h3>Setting up Kamailio</h3>

<p>In my <a href="" title="http://beingasysadmin.wordpress.com/2014/02/23/integrating-kamailio-with-freeswitch/">previous blog</a>, i&#8217;ve explained how to install Kamailio from source. The only difference is we need to enable two more modules &#8221;<strong>presence</strong>&#8221; and &#8221;<strong>presence_xml</strong>&#8221;. If Kamailio is already installed, we need to add these two modules into <code>modules.lst</code> at the &#8221;<strong>include_modules</strong>&#8221; line. Once the module names are added, we need to run &#8221;<strong>make install</strong>&#8221; to install the two new modules. Once the module is added, we need to enable the module.</p>

<p>Add &#8220;#!define WITH_PRESENCE&#8221; into the &#8220;kamailio.cfg&#8221; file. Then check if there is route logic defined for the &#8220;PRESENCE&#8221; module. By default there is a PRESENCE route defined in the default &#8220;kamailio.cfg&#8221; file. If it&#8217;s not there below is the route logic. Also we need to add &#8220;*route(PRESENCE)<strong>&#8221; in the &#8221;</strong>request_route**&#8221; section.</p>

<pre><code># Presence server route
route[PRESENCE] {
        if(!is_method("PUBLISH|SUBSCRIBE"))
                return;

        if(is_method("SUBSCRIBE") &amp;&amp; $hdr(Event)=="message-summary") {
                route(TOVOICEMAIL);
                # returns here if no voicemail server is configured
                sl_send_reply("404", "No voicemail service");
                exit;
        }

#!ifdef WITH_PRESENCE
        if (!t_newtran())
        {
                sl_reply_error();
                exit;
        }

        if(is_method("PUBLISH"))
        {
                handle_publish();
                t_release();
        } else if(is_method("SUBSCRIBE")) {
                handle_subscribe();
                t_release();
        }
        exit;
#!endif

        # if presence enabled, this part will not be executed
        if (is_method("PUBLISH") || $rU==$null)
        {
                xlog("@ 404 here 3");
                sl_send_reply("404", "Not here");
                exit;
        }
        return;
}
</code></pre>

<p>Now start the Kamailio server. Now we need to add some users. For that we can use &#8221;<strong>kamctl</strong>&#8221; binary.</p>

<pre><code>$ kamctl add user1@192.168.56.100 user1

$ kamctl add user2@192.168.56.100 user2
</code></pre>

<p>Let&#8217;s go ahead and test the settings. For testing we need some IM clients. In my testing, i&#8217;ve used <strong>Jitsi</strong> and <strong>adium</strong> IM clients. Once the accounts are configured on the clients, it&#8217;s better to start a packet capture using wireshark, so that we can see these <strong>PUBLISH</strong>, <strong>SUBSCRIBE</strong> and <strong>NOTIFY</strong> requests between the clients and the Kamailio server. If you are using <strong>Adium</strong>, go to account options and check the &#8220;Publish Status to Everyone&#8221; so that it will start sending <strong>PUBLISH</strong> request to the Kamailio server. Now add the accounts to the Contact lists on the IM clients and then we will be able to see the users status (ONline/OFFline). Now we can start the chat conversation between the user&#8217;s.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Integrating Kamailio with FreeSwitch]]></title>
    <link href="http://beingasysadmin.com/blog/2014/02/23/integrating-kamailio-with-freeswitch/"/>
    <updated>2014-02-23T11:05:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2014/02/23/integrating-kamailio-with-freeswitch</id>
    <content type="html"><![CDATA[<p><code>Kamailio</code> aka <code>OpenSER</code> is one of the most powerfull and popular Open Source SIP server. It can be used as <strong>SIP Proxy/Registrar/LB/Router</strong> etc. It also provides a lot of features <em>like WebSocket support for WebRTC, ; SIMPLE instant messaging and presence with embedded XCAP server and MSRP relay,IMS extensions,ENUM and offcourse AAA (accounting, authentication and authorization)</em> also. Kamailio is a modular system, ie, it has lot of modules which corresponds to particular functions. These modules can be easily installed and can be used easily in Kamailio. In this blog i&#8217;m going to use Kamailio as a proxy server. All the user&#8217;s are created in the Kamailio and FreeSwitch will be acting as a relay server for outbound calls. So Kamailio performs authentication and all the outbound calls wil be relayed to FreeSwitch.</p>

<h3>Installing Kamailio</h3>

<p>Lets download the latest version of Kamailio, now it&#8217;s 4.1</p>

<pre><code>$ wget http://www.Kamailio.org/pub/Kamailio/latest/src/Kamailio-4.1.1_src.tar.gz

$ tar xvzf Kamailio-4.1.1_src.tar.gz &amp;&amp; cd Kamailio-4.1.1
</code></pre>

<p>Before we start the build we need to install the basic dependencies.</p>

<pre><code>$ apt-get install gcc flex bison libmysqlclient-dev make libxml2-dev libssl-dev
</code></pre>

<p>Now we have the dependencies installed, we can go ahead with the build.</p>

<pre><code>$ make cfg      # generates config files for build system
</code></pre>

<p>Now open <code>modules.lst</code> and add the modules to be installed in &#8221;<strong>include_modules</strong>&#8221; section. In my case i&#8217;m going to use MySQL backend so it will be &#8221;<strong>include_modules= db_mysql</strong>&#8221; and then we can run the &#8221;<em>make all</em>&#8221;. The other way is we can mention the modules directly while running the &#8220;make&#8221; rather editing the modules.lst file.</p>

<pre><code>$ make include_modules="db_mysql" cfg
</code></pre>

<p>Now we can install,</p>

<pre><code>$ make install
</code></pre>

<p>The above command will install Kamailio to our system. There are four main binaries for Kamailio,</p>

<pre><code>Kamailio - Kamailio SIP server
kamdbctl - script to create and manage the Databases
kamctl - script to manage and control Kamailio SIP server
kamcmd - CLI - command line tool to interface with Kamailio SIP server
</code></pre>

<p>There is also one configuration file called &#8220;Kamailio.cfg&#8221; which is available by default at <code>/usr/local/etc/Kamailio/Kamailio.cfg</code></p>

<p>Let&#8217;s go ahead with setting up MySQL server for Kamailio.</p>

<pre><code>$ apt-get install mysql-server
</code></pre>

<p>Now edit the <code>/usr/local/etc/Kamailio/kamctlrc</code> Locate DBENGINE variable and set it to MYSQL by making &#8221;<strong>DBENGINE=MYSQL</strong>&#8221;. Now we can use the &#8220;kamdbctl&#8221; binary to craete the default tables and users.</p>

<pre><code>$ /usr/local/sbin/kamdbctl create
</code></pre>

<p>The script will add two users in MySQL:</p>

<ul>
<li><p>Kamailio - (with default password &#8216;Kamailiorw&#8217;) - user which has full access rights to &#8216;Kamailio&#8217; database</p></li>
<li><p>Kamailioro - (with default password &#8216;Kamailioro&#8217;) - user which has read-only access rights to &#8216;Kamailio&#8217; database</p></li>
</ul>


<p>There is a sample init.d script available along with Kamailio, which we can use it. We need to copy the sample init.d file to our system&#8217;s init.d folder. And the same for the system default file.</p>

<pre><code>$ cp /usr/local/src/Kamailio-4.1.1/pkg/Kamailio/deb/precise/Kamailio.init /etc/init.d/Kamailio

$ cp /usr/local/src/Kamailio-4.1.1/pkg/Kamailio/deb/precise/Kamailio.default /etc/default/Kamailio

$ chmod 755 /etc/init.d/Kamailio 
</code></pre>

<p>Edit the new init file and modify the $DAEMON and $CFGFILE values.</p>

<pre><code>DAEMON=/usr/local/sbin/Kamailio
CFGFILE=/usr/local/etc/Kamailio/Kamailio.cfg

$ mkdir -p /var/run/Kamailio    # Directory for the pid file
</code></pre>

<p>Default setting is to run Kamailio as user &#8221;<em>Kamailio</em>&#8221; and group &#8221;<em>Kamailio</em>&#8221;. For that we need to create the user:</p>

<pre><code>$ adduser --quiet --system --group --disabled-password \
      --shell /bin/false --gecos "Kamailio" \
      --home /var/run/Kamailio Kamailio

$ chown Kamailio:Kamailio /var/run/Kamailio
</code></pre>

<h3>Setting up Kamailio</h3>

<p>All the Kamailio configurations are mentioned in only one single file <code>/usr/local/etc/Kamailio/Kamailio.cfg</code>. All the logics are defined in this file, and Kamailio blindly executes this logics and perform the actions. It&#8217;s very important that the logics defined in the config should suit to our VOIP platform requirement.</p>

<p>First we need to enable the modules and the necessary features, so add the below lines in the Kamailio.cfg</p>

<pre><code>#!define WITH_MYSQL
#!define WITH_AUTH
#!define WITH_USRLOCDB
#!define WITH_FREESWITCH
</code></pre>

<p>We need to define the FreeSwitch server IP and port, for that we can add the below parameters in the &#8220;Custom Parameters&#8221; section.</p>

<pre><code>#!ifdef WITH_FREESWITCH
freeswitch.bindip = "192.168.56.100" desc "FreeSWITCH IP Address"
freeswitch.bindport = "5090" desc "FreeSWITCH Port"
#!endif
</code></pre>

<p>Now we can go ahead to the &#8221;<strong>request_route</strong>&#8221; section which performs the routing logic. Here i&#8217;m going to add two more routing logic for the FreeSwitch relay. After the &#8221;<strong>request_route</strong>&#8221; section, we can see the definition for each routing options. Below that we need to add our new route definitions.</p>

<pre><code>route[FSDISPATCH] {
        # dial number selection
        route(FSRELAY);
        exit;
}

route[FSRELAY] {
        $du = "sip:" + $sel(cfg_get.freeswitch.bindip) + ":" + $sel(cfg_get.freeswitch.bindport);
        if($var(newbranch)==1)
        {
                append_branch();
                $var(newbranch) = 0;
        }
        route(RELAY);
        exit;
}
</code></pre>

<p>By default, all the routes mentioned in the &#8220;request_route&#8221; will be executed line by line. There is a default route called &#8221;<strong>Location</strong>&#8221;, which splits the user part from the request URI and verifies if the user exists in the location table. But when we dial an outside number/user, this location check will fail, so i&#8217;m going to add a condition which checks if the user in the request URI contains a number with a length 9-15 will be relayed to the FreeSwitch. Again this is just a simple condition, we can create a more complex condition, like check the domain part, if the domain part contains a domain which doesnot belong to our Domain list, we can either decline the request, or we can relay to FreeSwitch or we can make DNS query and we can make Kamailio to process the request to that domain&#8217;s Proxy server. Like this we can define our own conditions in the config file, and Kamailio will execute it line by line.</p>

<p>I&#8217;m going to add my check condition on the &#8221;<strong>LOCATION</strong>&#8221; route definition.</p>

<pre><code>route[LOCATION] {

    #!ifdef WITH_SPEEDDIAL
        # search for short dialing - 2-digit extension
        if($rU=~"^[0-9][0-9]$")
                if(sd_lookup("speed_dial"))
                        route(SIPOUT);
    #!endif
        if($rU=~"^[0-9]{9,15}$")        # checking for numbers in the Request URI
                route(FSDISPATCH);
    #!ifdef WITH_ALIASDB
        # search in DB-based aliases
        if(alias_db_lookup("dbaliases"))
                route(SIPOUT);
    #!endif
        $avp(oexten) = $rU;
        if (!lookup("location")) {
                xlog("L_INFO", "CALL $rm $ci lookup here\n");
                xlog("L_INFO", "$fU@$fd - Lookup contact location for $rm\n");
                xlog("L_INFO", "rc is $var(rc)");
                switch ($rc) {
                        case -1:
                        case -3:
                                xlog("L_ERR", "$fU@$fd - No contact found\n");
                                send_reply("404", "Not Found here");
                                exit;
                        case -2:
                                send_reply("405", "Method Not Allowed");
                                exit;
                }
        }

        # when routing via usrloc, log the missed calls also
        if (is_method("INVITE"))
        {
                setflag(FLT_ACCMISSED);
        }
        xlog("L_INFO", "CALL $rm $ci relay\n");
        xlog("L_INFO", "$fU@$fd - Relaying $rm\n");
            route(RELAY);
        exit;
}
</code></pre>

<p>So now all the calls coming with numbers of length 9-15 in the Request URI will be relayed to the FreeSwitch, and FreeSwitch will process the call based on the DialPlan configured in the FreeSwitch. Since i&#8217;m going to use IP authentication, i need to whitelist the Kamailio ip in &#8220;acl_conf.xml&#8221; file in the FreeSwitch autload conf directory, so that FreeSwitch will accept the invites from Kamailio. Again i&#8217;m not defining any Voicemail options here. If we have a Voice mail server, then we can create another route option and when the caller doesn&#8217;t pick the call we can route the call to the Voice Mail server.
For example the below condition will route the failures to FreeSwitch Voice Mailbox.</p>

<pre><code>if(is_method("INVITE"))
        {
            # in case of failure - re-route to FreeSWITCH VoiceMail
            t_on_failure("FAIL_FSVBOX");        # where FSVBOX is a route definition
        }
</code></pre>

<p>Kamailio has a lot of modules which really comes in handy. For example we can use LDAP module to use LDAP as a backend. There is a PRESENCE module which helps us to setup an Instant Messaging server using Kamailio. I&#8217;ll be writing a blog on how to use Kamailio as an IM server soon. One of the main advantage of Kamailio over OpenSIP is the WebSocket support. This just a basic configuration, but we can design much complex system using Kamailio. We can even remove the default route logics, and we can use our own routing logics. Kamailio doesnot depend on default routing logics, it blindly reads the route and executes it for each incoming connections.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SIP Trunking with PLIVO and FreeSwitch]]></title>
    <link href="http://beingasysadmin.com/blog/2014/02/14/sip-trunking-using-plivo-and-freeswitch/"/>
    <updated>2014-02-14T15:05:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2014/02/14/sip-trunking-using-plivo-and-freeswitch</id>
    <content type="html"><![CDATA[<p>It&#8217;s more than a month since i joined the DevOps family at <strong>Plivo</strong>, since i&#8217;m pretty new to the Telecom Technology, i was digging more around it. This time i decided to play around with <strong>FreeSwitch</strong>, a free and open source communications software for the creation of voice and messaging products. Thanks to <a href="http://www.linkedin.com/pub/anthony-minessale-ii/3a/403/942">Anthony Minessale</a> for designing and opensourcing such a powerfull powerfull application. FreeSwitch is well documented and there are pretty good blogs also available on how to setup a PBX using FreeSwitch. This time i&#8217;m going to explain on how to make a private Freeswitch server to use Plivo as a <em>SIP Trunking</em> service.</p>

<p>A bit about <a href="plivo.com">Plivo</a>. Plivo is a cloud based API Platform for building <em>Voice</em> and <em>SMS</em> enabled Applications. Plivo provides <strong>Application Programming Interfaces</strong> (APIs) to <em>make and receive calls, send SMS, make a conference call</em>, and more. These APIs are used in conjunction with XML responses to control the flow of a call or a message. We can create Session Initiation Protocol (SIP) endpoints to perform the telephony operations and the APIs are platform independent and can be used in any programming environment such as PHP, Ruby, Python, etc. It also provides helper libraries for these programming languages.</p>

<p>First we need a valid Plivo account. Once we have the Plivo account, we can log into the Plivo Cloud service. Now go to the &#8221;<strong>Endpoints</strong>&#8221; tab and create a SIP endpoint and attach a DirectDial app to it. Once this is done we can go ahead and start setting up the FreeSwitch instance.</p>

<h3>Installing FreeSwitch</h3>

<p>Clone the Official FreeSwitch Github and Repo and compile from the source.</p>

<pre><code>$ git clone git://git.freeswitch.org/freeswitch.git &amp;&amp; cd freeswitch

$ ./bootstrap.sh &amp;&amp; ./configure --prefix=/usr/local/freeswitch

$ make &amp;&amp; make install

$  make all cd-sounds-install cd-moh-install    # optional, run this if you want IVR and Music on Hold features
</code></pre>

<p>Now if we have more than one ip address on the machine, and if we want to bind to a particular ip, we need to modify two files &#8221;<em>/usr/local/freeswitch/conf/sip_profiles/external.xml</em>&#8221; and &#8221;<em>/usr/local/freeswitch/conf/sip_profiles/internal.xml</em>&#8221;. In both the files, change the parameters &#8221;<strong>name=&#8221;rtp-ip&#8221;</strong>&#8221; and &#8221;<strong>param name=&#8221;sip-ip&#8221;</strong>&#8221; with the <em>bind ip</em> as the values.</p>

<p>By default, Freeswitch will create a set of users, which includes numerical usernames ie, 1000-1019. So we can test the basic connectivity between user&#8217;s by making a call between two user accounts. We can register two of the accounts in two SoftPhones and we can make a test call and make sure that FreeSwitch is working fine. We can use the FS binary file to start FreeSwitch service in forground.</p>

<pre><code>$ /usr/local/freeswitch/bin/freeswitch
</code></pre>

<h3>Configuring Gateway</h3>

<p>Once the FreeSwitch is working fine, we can start configuring the SIP trunking via Plivo. So first we need to create an external gateway to connect to Plivo. I&#8217;m going to use the SIP endpoint created on the Plivo Cloud to initiate the connection. The SIP domain for Plivo is &#8221;<strong>phone.plivo.com</strong>&#8221;. We need to create a gateway config. Go to &#8221;<em>/usr/local/freeswitch/conf/sip_profiles/external/</em>&#8221;, here we can create an XML gateway config file. My config file name is <strong>plivo</strong>. Below is the content for the same.</p>

<pre><code>&lt;include&gt;
  &lt;gateway name="plivo"&gt;
  &lt;param name="realm" value="phone.plivo.com" /&gt;
  &lt;param name="username" value="test88140105181635" /&gt;
  &lt;param name="password" value="secret" /&gt;
  &lt;param name="register" value="false" /&gt;
  &lt;param name="ping" value="5" /&gt;
  &lt;param name="ping-max" value="3" /&gt;
  &lt;param name="retry-seconds" value="5" /&gt;
  &lt;param name="expire-seconds" value="60" /&gt;
  &lt;variables&gt;
        &lt;variable name="verbose_sdp" value="true"/&gt;
  &lt;/variables&gt;
  &lt;/gateway&gt;
&lt;/include&gt;
</code></pre>

<p>There are a lot of other parameters which we can add it here, like caller id etc. Replace the username and password with the Plivo endpoint credentials. If we want to keep this endpoint registered, we can set the register param as true and we can set the expiry time at expire-seconds, so that the Fs will keep on registering the Endpoint with Plivo&#8217;s Registrar server. once the gateway file is created, we can either restart the service or we can run &#8220;reload mod_sofia&#8221; on the FScli. If the FreeSwitch service si started in foreground, we will get the FScli, so we can run the reload command directly on it.</p>

<h3>Setting up Dialplan</h3>

<p>Now we have the Gateway added. Now we need to the setup the Dial Plan to route the outgoing calls through Plivo. Go to &#8221;<em>/usr/local/freeswitch/conf/dialplan/</em>&#8221; folder and add an extension on the &#8221;<strong>public.xml</strong>&#8221; file. Below is a sample extension config.</p>

<pre><code>&lt;extension name="Calls to Plivo"&gt;
      &lt;condition field="destination_number" expression="^(&lt;ur_regex_here&gt;)$"&gt;
        &lt;action application="transfer" data="$1 XML default"/&gt;
      &lt;/condition&gt;
&lt;/extension&gt;
</code></pre>

<p>So now all calls matching to the Regex will be transferred to the default dial plan. Now on the the default dial plan, i&#8217;m creating an exntension and will use the FreeSwitch&#8217;s &#8221;<em>bridge</em>&#8221; application to brdige the call with Plivo using the Plivo Gateway. So on the &#8221;<em>default.xml</em>&#8221; add the below extension.</p>

<pre><code>   &lt;extension name="Dial through Plivo"&gt;
         &lt;condition field="destination_number" expression="^(&lt;ur_regex_here&gt;)$"&gt;
           &lt;action application="bridge" data="sofia/gateway/plivo/$1"/&gt;
         &lt;/condition&gt;
       &lt;/extension&gt;
</code></pre>

<p>Now we can restart the FS service or we can reload &#8220;mod_dialplan_xml&#8221; from the FScli. Once the changes are into effect, we can test whether the call is getting routed via Plivo. Configure a soft phone with a default FS user and make an outbound call which matches the regex that we have mentioned for routing to Plivo. Now if all works we should get a call on the destination number. We can check the FS logs at &#8221;<em>/usr/local/freeswitch/log/freeswitch.log</em>&#8221;.</p>

<p>If the Regex is matched, we can see the below lines in the log.</p>

<pre><code>1f249a72-9abf-4713-ba69-c2881111a0e8 Dialplan: sofia/internal/1001@192.168.56.11 parsing [public-&gt;Calls from BoxB] continue=false
1f249a72-9abf-4713-ba69-c2881111a0e8 EXECUTE sofia/internal/1001@192.168.56.11 transfer(xxxxxxxxxxxx XML default)
1f249a72-9abf-4713-ba69-c2881111a0e8 Dialplan: sofia/internal/1001@192.168.56.11 Regex (PASS) [Dial through Plivo] destination_number(xxxxxxxxxxxx) =~ /^(xxxxxxxxxxxx)$/ break=on-false
1f249a72-9abf-4713-ba69-c2881111a0e8 Dialplan: sofia/internal/1001@192.168.56.11 Action bridge(sofia/external/plivo/xxxxxxxxxxxx@phone.plivo.com)   
1f249a72-9abf-4713-ba69-c2881111a0e8 EXECUTE sofia/internal/1001@192.168.56.11 bridge(sofia/external/plivo/xxxxxxxxxxxx@phone.plivo.com)
1f249a72-9abf-4713-ba69-c2881111a0e8 2014-02-14 06:32:48.244757 [DEBUG] mod_sofia.c:4499 [zrtp_passthru] Setting a-leg inherit_codec=true
1f249a72-9abf-4713-ba69-c2881111a0e8 2014-02-14 06:32:48.244757 [DEBUG] mod_sofia.c:4502 [zrtp_passthru] Setting b-leg absolute_codec_string=GSM@8000h@20i@13200b,PCMA@8000h@20i@64000b,PCMU@8000h@20i@64000b
</code></pre>

<p>We can also mention the CallerID on the Direct Dial app which we have mapped to the SIP endpoint. Now for Incoming calls, create a app that can forward the calls to one of the user&#8217;s present in the FreeSwitch, using the Plivo&#8217;s Dial XML. So the XML should look something like below. I will be writing a more detailed blog about Inbound calls once i&#8217;ve have tested it out completely.</p>

<pre><code>&lt;Response&gt;
  &lt;Dial&gt;
    &lt;User&gt;FSuser@FSserverIP&lt;/User&gt;
  &lt;/Dial&gt;
&lt;/Response&gt;
</code></pre>

<p>But for security, we need to allow connections from Plivo server. So we need to allow those IP&#8217;s on the FS acl. We can allow the IP&#8217;s in the &#8221;<strong>acl.conf.xml</strong>&#8221; file at &#8221;<em>/usr/local/freeswitch/conf/autoload_configs</em>&#8221;. And make sure that the FS server is accessible via a public ip atleast for the Plivo server&#8217;s which will forward the calls.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Make Text To Speech Calls from Hip Chat]]></title>
    <link href="http://beingasysadmin.com/blog/2014/01/23/make-text-to-speech-calls-from-hip-chat/"/>
    <updated>2014-01-23T16:18:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2014/01/23/make-text-to-speech-calls-from-hip-chat</id>
    <content type="html"><![CDATA[<p>I&#8217;ve been using <code>HipChat</code> for the last one month. Before that it was IRC always. But when i joined <em>Plivo&#8217;s</em> <strong>DevOps</strong> family, i&#8217;ve got a lot of places to integrate plivo to automate a lot of stuffs. This i was planning to do some thing with Hipchat. There are a couple of HipChat bot&#8217;s available, i decided to use &#8217;<strong>robut</strong>&#8217;, as it is simple, written in Ruby and easy to write plugins also. Robut need only one &#8220;Chatfile&#8221; as its config file. All the Hip Chat account and room details are mentioned in this file. The Plugins are enabled in this file.</p>

<p>First we need to clone the repository.</p>

<pre><code>$ git clone https://github.com/justinweiss/robut.git
</code></pre>

<p>Now i need &#8217;<strong>redis</strong>&#8217; and &#8217;<strong>plivo</strong>&#8217; gems for my new plugin. So need to add the same in the Gemfile. Once the gems are added in to the Gemfile, we need to do a &#8221;<strong>bundle install</strong>&#8221; to install all the necessary gems. The path for the <code>Chatfile</code> can be mentioned in  &#8221;<strong>bin/robut</strong>&#8221; file. Now before starting <strong>robut</strong> we need to add the basic HipChat settings ie, <em>jabberid, nick, password and room</em>. Now wen eed to create a plugin to use with robut. The default plugins are available in &#8221;<em>lib/robut/plugin/</em>&#8221; folder. Below is the plugin which i created for making  Text To Speech calls.</p>

<pre><code>require 'json'
require 'timeout'
require 'securerandom'
require 'plivo'
require 'redis'

class Robut::Plugin::Call
  include Robut::Plugin

  def usage
    "#{at_nick} call &lt;number&gt; &lt;message&gt; "
  end

  def handle(time, sender_nick, message)
    new_msg = message.split(' ')
    if sent_to_me?(message) &amp;&amp; new_msg[1] == 'call'
    num = new_msg[2]
    textt = message.split(' ').drop(3).join(' ')
      begin
    reply("Calling #{num}")
    plivo_auth_id = "XXXXXXXXXXXXXXXXX"
    plivo_auth_token = "XXXXXXXXXXXXXXXXX"
    uuid = SecureRandom.hex
        r = Redis.new(:host =&gt; "redis_host", :port =&gt; redis_port, :password =&gt; "redis_port")
        temp = {
               'text' =&gt; "#{textt}"
               }
    plivo_number = "plivo_number"
        to_number = "#{num}"
        answer_url = "http://polar-everglades-1062.herokuapp.com/#{uuid}"

        call_params = {
                      'from' =&gt; plivo_number,
                      'to' =&gt; to_number,
                      'answer_url' =&gt; answer_url,
                      'answer_method' =&gt; 'GET'
                      }
        r.set "#{uuid}", temp.to_json
        r.expire "#{uuid}", 3000
        sleep 5
        puts "Initiating plivo call"
    p = Plivo::RestAPI.new(plivo_auth_id, plivo_auth_token)
        details = p.make_call(call_params)
    reply("Summary #{details}")
      rescue
        reply("Can't calculate that.")
      end
    end
  end
end
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Phone Call Notification for Sensu using Plivo]]></title>
    <link href="http://beingasysadmin.com/blog/2014/01/12/phone-call-notification-for-sensu-using-plivo/"/>
    <updated>2014-01-12T19:02:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2014/01/12/phone-call-notification-for-sensu-using-plivo</id>
    <content type="html"><![CDATA[<p>It&#8217;s almost 2 weeks since i&#8217;ve joined <a href="http://plivo.com/">Plivo&#8217;s</a> DevOps family. I was spending a lot of time on understanding their API&#8217;s as i&#8217;m new to telephony. Plivo&#8217;s <a href="http://plivo.com/docs/api/">API</a> made telephony so easy that even a person with basic programming language can built powerfull telephony apps to suit to their infrastructure. So when i started playing around with the API, i decided to take it to a different level. I&#8217;m strong lover of Sensu Monitoring tool, so i decided to integrate <em>Plivo</em> with <em>Sensu</em> to built a new Notification system using the Phone call. Many people rely on Pagerduty for the same feature. But that part is completly managed by PagerDuty, where in for the alerts which we sent to PagerDuty, they will notify us via phone call to the phone numbers mentioned on Pager Duty. So i decided to built a similar Handler to Sensu, so that we can have a similar feature on Sensu. In this blog i will explain how i achieved the same with Plivo Framework.</p>

<p><code>Plivo</code> provides Application Programming Interfaces (APIs) to make and receive calls, send SMS, make a conference call, and more. These APIs are used in conjunction with XML responses to control the flow of a call or a message. Developers can create Session Initiation Protocol (SIP) endpoints to perform the telephony operations and the APIs are platform independent and can be used in any programming environment such as PHP, Ruby, Python, etc. It also provides helper libraries for these programming languages.</p>

<p>Here the Sensu will be initiating outbound calls using Plivo&#8217;s <a href="http://plivo.com/docs/api/">Call</a> API, to the required numbers and will read out the Alert using Plivo&#8217;s Text to Speech Engine. First we need an account on Plivo Cloud, we can go to <a href="https://manage.plivo.com/accounts/register/">SignUP</a> page, and need to create an account. By using the default, we can make test calls. But for longer usage, i will suggest to buy some credits for using the service extensively. Once we login with credentials, at the dashboard we can see <strong><em>Plivo AuthID</em></strong> and <strong><em>Plivo AuthToken</em></strong>, which is required to access Plivo&#8217;s API. Now for making out bound calls, we need to use the <a href="http://plivo.com/docs/api/call/">Call</a> API. We also need to provide an &#8221;<strong>answer_url</strong>&#8221; which contains XML instructions to direct the progress of the call. Plivo will fetch the &#8221;<strong>answer_url</strong>&#8221; and executes te XML instructions. So here i will be using <em>Sinatra</em> to create a web app that will returns the XML.  The main challenge is the text to be mentioned in the XML need to retrieved from the alert. So we cannot predefine the text as well as the request url, because it will be in dynamic in nature.</p>

<p><code>Solution</code> :- Here i&#8217;m going to use a publically accessible <em>Redis</em> server which is protected with a password. When Sensu handler receives the alert, it will create a hash from the alert received with a random UUID as the name of the hash. And the same UUID will be used as the request path for the answer_url. And when sinatra recieves a request, by default the requested path wont be existing in the sinatra&#8217;s config file, as it&#8217;s a dynamically generated. So by default Sinatra will return a 404 response. But in Sinatra, there is an option called &#8221;<strong><em>not_found</em></strong>&#8221;, where we can customize the response instead of returning 404 directly. So i will be using the &#8220;not_found&#8221; option and instead of directly returing a 404, i will make sinatra to talk to my redis server. Since the UUID can be fetched from the request URL, and the same is used in the redis as the name of the Hash, Sinatra can get the details of the alert from the Redis. These details are then used to create the XML and will be returned as the response to Plivo. It will be better to go through Plivo&#8217;s API documentation to understand more about the XML responses and outbound calls.</p>

<p>For those who don&#8217;t have a dedicated server to host the Sinatra app and Redis server, <a href="https://devcenter.heroku.com/articles/getting-started-with-ruby">heroku</a> can be used to host the Sinatra app. We can also use the redis addon availabe at the Heroku for our usage.</p>

<h2>Sinatra App</h2>

<p>Below is the code for the Sinatra app. The &#8221;<strong><em>not_found</em></strong>&#8221; option is the one which performs the connection with the Redis.</p>

<pre><code>require 'plivo'
require 'sinatra'
require 'redis'
require 'rest_client'

get '/' do
  play_loop = 1
  lang = "en-US"
  voice = "WOMAN"
  text = "Congratulations! You just made a text to speech app on Plivo cloud!"
  @path = request.path
  puts @path
  speak_params = {
                     'loop' =&gt; play_loop,
                     'language' =&gt; lang,
                     'voice' =&gt; voice,
                     }

  plivo = Plivo::Speak.new(text, speak_params)

  response = Plivo::Response.new()
  response.add(p)
  return response.to_xml
end

not_found do
  @path = request.path
  keyword = @path[1..-1]
  response = Redis.new(:host =&gt; "&lt;redis_host_name&gt;", :port =&gt; &lt;redis_port&gt;, :password =&gt; "&lt;redis_pass&gt;")
  data_red = response.get("#{keyword}")
  if data_red == nil
         return "404 Not Found"
  else
         data = JSON.parse(response.get("#{keyword}"))
         text = data["text"]
         play_loop = 1
         lang = "en-US"
         voice = "WOMAN"
         speak_params = {
                        'loop' =&gt; play_loop,
                        'language' =&gt; lang,
                        'voice' =&gt; voice,
                        }

         plivo = Plivo::Speak.new(text, speak_params)

         response = Plivo::Response.new()
         response.add(p)
         return r.to_xml
   end
end
</code></pre>

<p>The above app will redirect all the non found requests and make a requests to the Redis server using the request.path as the name of the hash. If there is a valid hash table, it will generate a Plivo XML else it will return &#8220;404 Not Found&#8221;.</p>

<h2>Plivo Handler</h2>

<p>Below is the Plivo Handler code. this is the Initial code, need to be tweaked to suit to the Sensu&#8217;s coding standard. Also the settings option need to be added so that all the parameters can be provided in the JSON file of the Handler. But as of now, this handler has been tested with Sensu and it is working perfectly fine.</p>

<pre><code>require 'rubygems'
require 'sensu-handler'
require 'timeout'
require 'securerandom'
require 'plivo'
require 'redis'
require 'timeout'


class PLIVO &lt; Sensu::Handler

  def short_name
    @event['client']['name'] + '/' + @event['check']['name']
  end

  def action_to_string
    @event['action'].eql?('resolve') ? "RESOLVED" : "ALERT"
  end

  def handle
    plivo_auth_id = "XXXXXXXXXXXXXXXX"          # =&gt; available at the plivo dashboard
    plivo_auth_token = "XXXXXXXXXXXXXXXX"       # =&gt; available at the plivo dashboard
    body = &lt;&lt;-BODY.gsub(/^ {14}/, '')
            #{@event['check']['output']}
            Host: #{@event['client']['name']}
           BODY
    uuid = SecureRandom.hex
    r = Redis.new(:host =&gt; "&lt;redis_host_name&gt;", :port =&gt; &lt;redis_port&gt;, :password =&gt; "&lt;redis_pass&gt;")
    temp = {
            'text' =&gt; "#{body}"
            }
    plivo_number = "&lt;YOUR PLIVO NUMBER"
    to_number = "&lt;DESTINATION NUMBER&gt;"
    answer_url = "&lt;YOUR HEROKU APP URL&gt;/#{uuid}"
    call_params = {
                     'from' =&gt; plivo_number,
                     'to' =&gt; to_number,
                     'answer_url' =&gt; answer_url,
                     'answer_method' =&gt; 'GET'
                     }
         r.set "#{uuid}", temp.to_json
         r.expire "#{uuid}", &lt;EXPIRY TTL&gt;
         sleep 5
     begin
       timeout 10 do
         puts "Connecting to Plivo Cloud.."
         plivo = Plivo::RestAPI.new(plivo_auth_id, plivo_auth_token)
     details = plivo.make_call(call_params)
       end
       rescue Timeout::Error
       puts "timed out while attempting to contact Plivo Cloud"
         end
  end
end
</code></pre>

<p>The above handler will make an outbound call to the Destination number and plivo&#8217;s text to speech engine will read out the Alert summary on the call. Now If you want to call multiple user&#8217;s, simply create an array or hash with the contact numbers and iterate over it.</p>

<p>This handler will give us the same feature of Phone Call notification similar to Pager Duty. We just need to pay for the phone call usage. Dedicated to all those who wants an affordable yet a good notification system with Sensu. I will be merging out my code soon to the  sensu-community repository.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TweetGrabber - A Live Tweet Grabber]]></title>
    <link href="http://beingasysadmin.com/blog/2013/09/22/tweetgrabber-a-live-tweet-grabber/"/>
    <updated>2013-09-22T17:30:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/09/22/tweetgrabber-a-live-tweet-grabber</id>
    <content type="html"><![CDATA[<p><code>Twitter</code> has become one of the powerfull social networking site. And people do share a lot of good info&#8217;s here. Even people reports outages of various sites and services in Twitter. Many of the tech companies do have a valid accounts to keep track of user&#8217;s comments on their  services/products and they even interacts with user&#8217;s. Website&#8217;s like &#8220;downrightnow.com&#8221;, uses Twitter as an information source to identify the outage of many famous websites and services. Even in our personal site&#8217;s we use Javascripts to  display our tweets and other statuses. But since the Twitter API version 1.1, which needs ouath, many of the jquery plugins became obsolete. This time i got a requirement to built a system that keeps track of tweets based on custom keywords. But i wanted to shaw all the tweets in a category basis, but on a sinlge page, with some scrolling effects, at the same time the sroller should keeps on updating with tweets on a regular interval basis. Which means i&#8217;m more concerned about the new trends going on in the Twitter.</p>

<p>Since i&#8217;m a <em>Rubyist</em>, i decided to build a Ruby app with <strong>Sinatra</strong> web Frontend, one my favourite web frameworks. I&#8217;m hard lover of Redis, faster since it runs on ram, and i dont want to keep my old tweets. So, My app is pretty simple, there will a <strong>Feeder</strong> which will talk to Twitter&#8217;s API and get the Tweet&#8217;s these tweet&#8217;s will be then stored in Redis, The Sinatra Fronend will fetch the tweets, and will display it in a scrolling fashion on the front end. Since i&#8217;m a CLI junkie, i&#8217;m not familiar with HTMLs and UI&#8217;s, so i decided to go with <em>Twitter Bootstrap</em> to build the HTML pages.</p>

<p>There is a Ruby gem called &#8221;<strong>TweetStream</strong>&#8221; which works very well with Twitter API v1.1. So i&#8217;m going to use this gem to talk to Twitter. Below is the simple architecture diagram for my app.</p>

<p><img src="http://beingasysadmin.com/images/tweetgrabber.png"></p>

<p>Let&#8217;s see each components in detail.</p>

<h3>Feeder</h3>


<p><code>Feeder</code> is a ruby script which constantly talks to Twitter API and grabs the latest streaming tweets based on the search keywords. Add all the grabbed tweets are then stored into the Redis database. Since i&#8217;ve to display teets corresponding to each keyword in separate scrolling fashions, i&#8217;m using separate redis database for each. So the Feeder has multiple ruby methods, where each method will be used  for each keyword and writes into the corresponding Redis DB. Below is one of the Ruby Method in the Feeder script.</p>

<pre><code>                        #######  FEEDER ####### 

TweetStream.configure do |config|
  config.consumer_key       = 'xxxxxxxxxxxxxx'          =&gt; All these cosnumerkey,secret and oauth tokens have to be generated from
  config.consumer_secret    = 'xxxxxxxxxxxxxx'             the Twitter's API site, dev.twitter.com
  config.oauth_token        = 'xxxxxxxxxxxxxx'
  config.oauth_token_secret = 'xxxxxxxxxxxxxx'
  config.auth_method        = :oauth
end

def tweet_general
  TweetStream::Client.new.track('opensource') do |status|   =&gt;  Thiss Tweatstream client will keep tracking the keyword "opensource"
    if ( status.lang == 'en' )
      push(
                'id' =&gt; status[:id],
               'text' =&gt; status.text,
               'username' =&gt; status.user.screen_name,
               'userid' =&gt; status.user[:id],
               'name' =&gt; status.user.name,
               'profile_image_url' =&gt; status.user.profile_image_url,
               'received_at' =&gt; Time.new.to_i,
               'user_link' =&gt; "http://twitter.com/"
             )
        end
  end
end
def push(data)
      @db = Redis.new
      @db.lpush('tweets_general', data.to_json)         =&gt; LPUSHing Tweets into the RedisDB
    end
</code></pre>

<h3>Redis DB</h3>


<p>In Redis, im going to use the <strong>LIST</strong> data type. LIST are simply list of strings, sorted by insertion order. It is possible to add elements to a Redis List pushing new elements on the head (on the left) or on the tail (on the right) of the list.</p>

<p><img src="http://beingasysadmin.com/images/redis-list.png"></p>

<p>So the new tweets i will be pushing in from the head, and prevent the over population, i will be calling <em>LTRIM</em> operation preiodically to clear of the old tweets from the database. All these operations are done from the Feeder by corresponding ruby methods/functions.</p>

<h3>Sinatra Frontend</h3>


<p>I&#8217;m using <strong>SINATRA</strong> for building the frontend. SInce i&#8217;m not much familiar with HTML&#8217;s and UI&#8217;s, i decided to use <em>Twitter Bootstrap</em> for building the layout. And for each category, i&#8217;ve created a collapsible table, so that we can expand and collapse the required tables. Now, the next task is scrolling the tweets, for that i found a jquery plugin called <a href="https://github.com/buildinternet/totem">Totem Ticker</a>.ANd i enabled, refreshing for the div element which contains this scroller, so that after each refresh, the variable which supplies tweets to the scroller, will get updated with the newer tweets from the corresponding Redis DB.</p>

<p><img src="http://beingasysadmin.com/images/layout.png"></p>

<p>As of now the app is working fine. But i&#8217;m planning to extend this to add more features, like adding the keywords dynamically from the Web Frontend, and displaying only those tables with the keywords. I will be digging more take it more powerfull :-). I&#8217;m going to push the working code soon into my GitHub account, so that others can also paly around on this, and can extend it for their own requirements.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CouchDB -  A NoSQL DB with a Powerfull Rest API]]></title>
    <link href="http://beingasysadmin.com/blog/2013/09/19/couchdb-nosql-db-with-a-powerfull-rest-api/"/>
    <updated>2013-09-19T21:37:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/09/19/couchdb-nosql-db-with-a-powerfull-rest-api</id>
    <content type="html"><![CDATA[<p><code>CouchDB</code> is an open source <em>NoSQL</em> database, which comes with a powerfull <strong>Rest API</strong>. It stores data in <em>JSON format</em>. uses <em>Javascripts</em> as its query language using <em>MapReduce</em>. JSON data&#8217;s are very easy to understand and can be parsed very easily. This time i got a requirement to build a commandline utility to display results for an internal monitoring tool. The tool is a bit old, and it does not have any api. And finding latest the result is consuming more time using the front end. But one advantage was the frontend can display results in HTML and XML. So curl was able to query the server using the url and it can display the xml output. The url is unique for each, as it composed of few details like Location, SerialNo, Domain, OS etc, which is unique for each hosts. So i decided to have a local database which contains all these unique host details. For this purpose, i dont need any relational databases, and NoSQL databases are sufficient. So i decided to use CouchDB, because it comes with a wonderfull rest api.</p>

<p>Since i&#8217;m a <em>RUBY</em> lover, i decided to use this couchdb ruby <a href="http://wiki.apache.org/couchdb/Getting_started_with_Ruby">wrapper</a>. SInce my requirement with CouchDB was pretty simple, this wrapper itself was sufficient for me. This wrapper can be used to perform any operations like, reading,writing,deleting Documents as well as Databases. This wrapper basically uses the ruby http library to talk to the CouchDB&#8217;s rest api.  By default couchdb is available in most of the Linux respositories. for windows and mac, the files are available in the CouchDB website itself.</p>

<p>for example, for ubuntu/debian, below command will install couchdb server.</p>

<pre><code>sudo apt-get install couchdb -y
</code></pre>

<p>By default, the couchDB package comes with a web frontend called &#8220;Futon&#8221; which can be accessed via browsers ugin the below url,</p>

<pre><code>http://localhost:5984/_utils/
</code></pre>

<p>Below is the screenshot of the Futon frontend.</p>

<p><img src="http://beingasysadmin.com/images/futon.png"></p>

<p>We can perform all the operations through this web interface also. So i created a database, and created a document for each host with required fields. Now these documents can be accessed via the rest api in JSON format. so that my ruby script can fetch all the necessary data and can compose the exact url for fetching the check results for corresponding hosts.</p>

<p>Below is a sample read operation that we can perform through our ruby scripts. But before using the operation, we need to include the wrapper into our script. For that we need to add &#8216;require ./couch.rb&#8217;, we can use both relative or absolute path. once the wrapper is included,we can start performing operations with the CouchDB.</p>

<pre><code>server = Couch::Server.new("localhost", "5984")
res = server.get("/foo/document_id")
json = res.body
puts json
</code></pre>

<p>The operation will read the document and will convert it into JSON using the json ruby library. Now i can use the JSON data to collect the required fields so that my script can form the exact fetch url for each host and can perform the result fetch.  There is couchdb <a href="https://github.com/phifty/couchdb">gem</a> for ruby, which can perform more operations.</p>

<p>Apart from this, CouchDB has another important feature &#8221;<strong>MAPREDUCE</strong>&#8221;, yes it can perform <strong>MapReduce</strong>, i&#8217;m trying to find if mapreduce can help me, so that i can extend my app to do more things. Once if i get any weird idea with MapReduce i will udapte this blog with that also.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Real Time Web-Monitoring using Lumberjack-Logstash-Statsd-Graphite]]></title>
    <link href="http://beingasysadmin.com/blog/2013/06/27/real-time-web-monitoring-using-lumberjack-logstash-statsd-graphite/"/>
    <updated>2013-06-27T16:29:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/06/27/real-time-web-monitoring-using-lumberjack-logstash-statsd-graphite</id>
    <content type="html"><![CDATA[<p>For the last few days i was playing around with my two of my favourite tools <em>Logstash</em> and <em>StatsD</em>. <strong><em>Logstash, StatsD, Graphite</em></strong> together makes a killer combination. So i decided to test this combination along with Lumberjack for Real time Monitoring. I&#8217;m going to use, <em>Lumberjack</em> as the log shipper from the webserver, and then Logstash will stash the log&#8217;s porperly and and using the statsd output plugin i will ship the metrics to Graphite. In my previous blog, i&#8217;ve explained how to use Lumberjack with Logstash. Lumberjack will be watching my test web server&#8217;s access logs.</p>

<p>By default, i&#8217;m using the combined apache log format, but it doesnot have the original response time for each request as well as the total reponse time. So we need to modify the LogFormat, in order to add the two. Below is the LogFormat which i&#8217;m using for my test setup.</p>

<pre><code>LogFormat "%h %l %u %t \"%r\" %&gt;s %O \"%{Referer}i\" \"%{User-Agent}i\" %D %&gt;D" combined
</code></pre>

<p>Once the LogFormat is modified, restart the apache service in order to make the change to be effective.</p>

<h4>Setting up Logstash Server</h4>

<p>First Download the latest Logstash Jar file from the Logstash <a href="https://logstash.objects.dreamhost.com/release/logstash-1.1.13-flatjar.jar">site</a>. Now we need to create a logstash conf file. By default there is a grok pattern available for apache log called &#8220;COMBINEDAPACHELOG&#8221;, but since we have added the tow new fields for the response time, we need to add the same for grok pattern also. So below is a pattern which is going to be used with Logstash.</p>

<pre><code>pattern =&gt; "%{COMBINEDAPACHELOG} %{NUMBER:resptime} %{NUMBER:resptimefull}"
</code></pre>

<p>So the Logstash conf file will look like this,</p>

<pre><code>input {
      lumberjack {
        type =&gt; "apache-access"
        port =&gt; 4444
        ssl_certificate =&gt; "/etc/ssl/logstash.pub"
        ssl_key =&gt; "/etc/ssl/logstash.key"
  }
}

filter {
  grok {
        type =&gt; "apache-access"
    pattern =&gt; "%{COMBINEDAPACHELOG} %{NUMBER:resptime} %{NUMBER:resptimefull}"
  }
}

output {
  stdout {
    debug =&gt; true
      }
  statsd {
    type =&gt; "apache-access"
    host =&gt; "localhost"
    port =&gt; 8125
    debug =&gt; true
    timing =&gt; [ "apache.servetime", "%{resptimefull}" ]
    increment =&gt; "apache.response.%{response}"
  }
}
</code></pre>

<h4>Setting up STATSD</h4>

<p>Now we can start setting up the StatsD daemon. By default, Ubuntu&#8217;s latest OS ships with newer verision of NodeJS and NPM. So we can install it using APT/Aptitude.</p>

<pre><code>$ apt-get install nodejs npm
</code></pre>

<p>Now clone the StatsD github repository to the local machine.</p>

<pre><code>$ git clone git://github.com/etsy/statsd.git
</code></pre>

<p>Now create a local config file &#8220;localConfig.js&#8221; with the below contents.</p>

<pre><code>{
graphitePort: 2003
, graphiteHost: "127.0.0.1"
, port: 8125
}
</code></pre>

<p>Now we can start the StatsD daemon.</p>

<pre><code>$ node /opt/statsd/stats.js /opt/statsd/localConfig.js
</code></pre>

<p>The above command will start the StatsD in foreground. Now we can go ahead with setting up the Graphite.</p>

<h4>Setting up Graphite</h4>

<p>First, let&#8217;s install the basic python dependencies.</p>

<pre><code>$ apt-get install python-software-properties memcached python-dev python-pip sqlite3 libcairo2 libcairo2-dev python-cairo pkg-config
</code></pre>

<p>Then, we can start installing Carbon and Graphite dependencies.</p>

<pre><code>        cat &gt;&gt; /tmp/graphite_reqs.txt &lt;&lt; EOF
        django==1.3
        python-memcached
        django-tagging
        twisted
        whisper==0.9.9
        carbon==0.9.9
        graphite-web==0.9.9
        EOF

$  pip install -r /tmp/graphite_reqs.txt
</code></pre>

<p>Now we can configure Carbon.</p>

<pre><code>$ cd /opt/graphite/conf/

$ cp carbon.conf.example carbon.conf
</code></pre>

<p>Now we need to create a storage schema.</p>

<pre><code>        cat &gt;&gt; /tmp/storage-schemas.conf &lt;&lt; EOF
        # Schema definitions for Whisper files. Entries are scanned in order,
        # and first match wins. This file is scanned for changes every 60 seconds.
        # [name]
        # pattern = regex
        # retentions = timePerPoint:timeToStore, timePerPoint:timeToStore
        [stats]
        priority = 110
        pattern = ^stats\..*
        retentions = 10s:6h,1m:7d,10m:1y
        EOF


$ cp /tmp/storage-schemas.conf /opt/graphite/conf/storage-schemas.conf
</code></pre>

<p>Also we need to create a log directory for graphite.</p>

<pre><code>$ mkdir -p /opt/graphite/storage/log/webapp
</code></pre>

<p>Now we need to copy over the local settings file and initialize database</p>

<pre><code>$ cd /opt/graphite/webapp/graphite/

$ cp local_settings.py.example local_settings.py

$ python manage.py syncdb
</code></pre>

<p>Fill in the necessary details including the super user details while initializing the database. Once the database is initialized we can start the carbon cache and graphite webgui.</p>

<pre><code>$ /opt/graphite/bin/carbon-cache.py start

$ /opt/graphite/bin/run-graphite-devel-server.py /opt/graphite
</code></pre>

<p>Now we can access the dashboard using the url, &#8220;http://ip-address:8080&#8221;. Once we have started the carbon cache, we can start the Logstash server.</p>

<pre><code>$ java -jar logstash-1.1.13-flatjar.jar agent -f logstash.conf -v
</code></pre>

<p>Once the logstash has loaded all the plugins successfully, we can start shipping logs from the test webserver using Lumberjack. Since i&#8217;ve enabled the STDOUT plugin, i can see the output coming from the Logstash server. Now we can start accessing the real time graph&#8217;s from graphite gui. There are several other alternative for the Graphite GUI like <a href="http://jondot.github.io/graphene/">Graphene</a>, <a href="https://github.com/paperlesspost/graphiti">Graphiti</a>, <a href="https://github.com/erezmazor/graphitus">Graphitus</a>, <a href="https://github.com/ripienaar/gdash">GDash</a>. Anyways Logstash-StatsD-Graphite proves to be a wonderfull combination.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lumberjack - A Light Weight Log shipper for Logstash ]]></title>
    <link href="http://beingasysadmin.com/blog/2013/06/25/lumberjack-a-light-weight-log-shipper-for-logstash/"/>
    <updated>2013-06-25T13:52:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/06/25/lumberjack-a-light-weight-log-shipper-for-logstash</id>
    <content type="html"><![CDATA[<p><strong>Logstash</strong> is one of the coolest projects that i always wanted to play around. Since i&#8217;m a sysadmin, i&#8217;m forced to handle multiple apps, which will logs in different formats. The most weird part is the timestamps, where most of the app uses it&#8217;s own time formats. Logstash helps us to solve such situations, we can remodify the time stamp to a standard time format, we can use the predefined filter&#8217;s for filtering out the log&#8217;s, even we can create our own filter&#8217;s using regex. All the documentations are available in the <a href="http://logstash.net">Logstash website</a> Logstash mainly has 3 parts, 1) <em>INPUT</em> -> from which the log&#8217;s are shipped to Logstash, 2) <em>Filter</em> -> for filtering our incoming log&#8217;s to suit to our needs, 3) <em>Output</em> -> For storing or relaying the Filtered output log&#8217;s to various Applications.</p>

<p>Lumberjack is one such input plugin designed for logstash. Though the plugin is still in beta state, i decided to give it a try. By default we can also use logstash itself for shipping logs to centralized Logstash server, the JVM made it difficult to work with many of my constrained machines. Lumberjack claims to be a light weight log shipper which uses <em>SSL</em> and we can add custom <code>fields</code> for each line of log which we ships.</p>

<h4>Setting up Logstash Server</h4>

<p>Download the latest the logstash jar file from the logstash <a href="https://logstash.objects.dreamhost.com/release/logstash-1.1.13-flatjar.jar">website</a>. Now create a logstash configuration file for the logstash instance. In the config file, we have to enable the <code>lumberjack</code> plugin. Lumberjack uses SSL CA to verify the server. So we need to generate the same for the logstash server. We can use the below mentioned command to generate the SSL certificate and key.</p>

<pre><code>$ openssl req -x509 -newkey rsa:2048 -keyout /etc/ssl/logstash.key -out /etc/ssl/logstash.pub -nodes -days 3650
</code></pre>

<p>Below is the sample logstash conf file which i used for stashing logs from <code>Socklog</code>.</p>

<pre><code>input {

  lumberjack {
    type =&gt; "qmail"
    port =&gt; 4545
    ssl_certificate =&gt; "/etc/ssl/logstash.pub"
        ssl_key =&gt; "/etc/ssl/logstash.key"
  }
}

filter {
  grok {
        type =&gt; "socklog"
        pattern =&gt; "%{DATA:logfacility}: %{SYSLOGTIMESTAMP:timestamp} %{DATA:program}: *"
  }
  mutate {
        replace =&gt; [ "@message", "%{mess}" ]
  }
  date {
        type =&gt; "socklog"
        match =&gt; [ "timestamp", "MMM dd HH:mm:ss" ]
  }
}

output {
  stdout {
    debug =&gt; true
      }
}
</code></pre>

<p>Now we can start the the logstash using the above config.</p>

<pre><code>$ java -jar logstash-1.1.13-flatjar.jar agent -f logstash.conf -v
</code></pre>

<p>Once the logstash has started successfully, we can use netstat to check if it listening on port <em>4545</em>. I&#8217;m currently running logstash in the foreground, below is the logoutput from logstash</p>

<pre><code>Starting lumberjack input listener {:address=&gt;"0.0.0.0:4545", :level=&gt;:info}
Input registered {:plugin=&gt;&lt;LogStash::Inputs::Lumberjack type=&gt;"socklog", ssl_certificate=&gt;"/etc/ssl/logstash.pub", ssl_key=&gt;"/etc/ssl/logstash.key", charset=&gt;"UTF-8", host=&gt;"0.0.0.0"&gt;, :level=&gt;:info}
Match data {:match=&gt;{"@message"=&gt;["%{DATA:logfacility}: %{SYSLOGTIMESTAMP:timestamp} %{DATA:program}: *"]}, :level=&gt;:info}
Grok compile {:field=&gt;"@message", :patterns=&gt;["%{DATA:logfacility}: %{SYSLOGTIMESTAMP:timestamp} %{DATA:program}: *"], :level=&gt;:info}
Output registered {:plugin=&gt;&lt;LogStash::Outputs::Stdout debug_format=&gt;"ruby", message=&gt;"%{@timestamp} %{@source}: %{@message}"&gt;, :level=&gt;:info}
All plugins are started and registered. {:level=&gt;:info}
</code></pre>

<h4>Setting up Lumberjack agent</h4>

<p>On the machine from which we are going to ship the log&#8217;s, clone the Lumberjack github <a href="https://github.com/jordansissel/lumberjack.git">repo</a>.</p>

<pre><code>$ git clone https://github.com/jordansissel/lumberjack.git
</code></pre>

<p>Install the <strong><em>fpm</em></strong> ruby gem, which is required to build the lumberjack package.</p>

<pre><code>$ gem install fpm

$ cd lumberjack &amp;&amp; make

$ make deb   =&gt; This will build a debian package of the lumberjack

$ dpkg -i lumberjack_0.0.30_amd64.deb  =&gt; The package will install all the files to the `/opt/lumberjack`
</code></pre>

<p>Now copy the SSL certificate which we have generated at the Logstash server, to the Lumberjack machine. Once the SSL certificte has been copied, we can start the lumberjack agent.</p>

<pre><code>$ /opt/lumberjack/bin/lumberjack --ssl-ca-path ./ssl/logstash.pub --host logstash.test.com --port 4545 /var/log/socklog/main/current
</code></pre>

<p>Below is the log output from the lumberjack.</p>

<pre><code>2013-06-25T15:04:32.798+0530 Watching 1 files, setting open file limit to 103
2013-06-25T15:04:32.798+0530 Watching 1 files, setting memory usage limit to 1048576 bytes
2013-06-25T15:04:32.878+0530 Connecting to logstash.test.com(192.168.19.19):4545
2013-06-25T15:04:33.186+0530 slow operation (0.307 seconds): connect to 192.168.19.19:4545
2013-06-25T15:04:33.186+0530 Connected successfully to logstash.test.com(192.168.19.19):4545
2013-06-25T15:04:34.653+0530 Declaring window size of 4096
2013-06-25T15:04:36.734+0530 flushing since nothing came in over zmq
</code></pre>

<p>Now we will start getting the output from the Logstash in our screen, since we are using the &#8216;stdout&#8217; output plugin. A very good detailed documentation about Lumberjack and Logstash can be found <a href="http://mzlizz.mit.edu/vmdoh-centralizing-logs-lumberjack-logstash-and-elasticsearch">here</a>, written by <a href="https://portland2013.drupal.org/users/brian-altenhofel">Brian Altenhofel</a>. He had given a talk on this at Drupalcon 2013, Portland. The video for the talk is available <a href="http://youtu.be/p0Av29yaBEI">here</a>. It&#8217;s a very good blog post.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[McoMaster - An HTML5 based GUI for Mcollective with Redis Discovery Method]]></title>
    <link href="http://beingasysadmin.com/blog/2013/05/28/mcomaster-an-html5-based-gui-for-mcollective-with-redis-discovery-method/"/>
    <updated>2013-05-28T09:46:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/05/28/mcomaster-an-html5-based-gui-for-mcollective-with-redis-discovery-method</id>
    <content type="html"><![CDATA[<p><code>Mcollective</code> is a framework to build server orchestration or parallel job execution systems. It&#8217;s completely built on  <strong>RUBY</strong>, and offcourse <a href="https://github.com/puppetlabs/puppetlabs-mcollective">Open Sourced</a>. There are a quite good <a href="https://github.com/puppetlabs/mcollective-plugins">plugins</a> available for mcollective. In my previous blog&#8217;s i&#8217;ve explained on how to setup Mcollective with ActiveMQ and also how to use Mongo Discovery method with Mcollective. In this blog i will explain on how to use Mcollective with Rabbitmq connector and a new project called <a href="http://mcomaster.org">mcomaster</a>, an HTML5 web interface to mcollective, which uses <em>REDIS</em> Discovery method in the backend.</p>

<h3>Setting up RabbitMQ</h3>

<p>First add the <code>RaabitMQ</code> APT repo,</p>

<pre><code>$ echo "deb http://www.rabbitmq.com/debian/ testing main" &gt;&gt; /etc/apt/sources.list

$ curl -L -o ~/rabbitmq-signing-key-public.asc http://www.rabbitmq.com/rabbitmq-signing-key-public.asc 

$ apt-key add ~/rabbitmq-signing-key-public.asc

$ apt-get update
</code></pre>

<p>And install the RabbitMQ server</p>

<pre><code>$ apt-get install rabbitmq-server
</code></pre>

<p>Now using <code>rabbitmqctl</code> create a vhost &#8221;<em>/mcollective</em>&#8221;. Also create a user called &#8221;<strong>mcollective</strong>&#8221; in Rabbitmq using rabbitmqctl and give full permission for this user to the new mcollective vhost.</p>

<pre><code>$ rabbitmqctl add_vhost /mcollective

$ rabbitmqctl add_user mcollective mcollective

$ rabbitmqctl set_permissions -p /mcollective mcollective ".*" ".*" ".*"
</code></pre>

<p>There is also managemnet gui for rabbitmq, we can perform all the above tasks from that also. For that we need enable the management plugin in rabbitmq.</p>

<pre><code>$ rabbitmq-plugins enable rabbitmq_management
</code></pre>

<p>And restart rabbitmq server. After restarting, we can access the gui using &#8221;<em>http://ip:55672</em>&#8221;. Login user is &#8221;<em>guest</em>&#8221;, password is &#8221;<em>guest</em>&#8221;. We need to create two exchanges that the mcollective plugin needs. We can use the <code>rabbitmqadmin</code> command as mentioned in the Mcollective <a href="http://docs.puppetlabs.com/mcollective/reference/plugins/connector_rabbitmq.html">Documentation</a> or we can use the Management GUI for that.</p>

<pre><code>$ rabbitmqadmin declare exchange vhost=/mcollective name=mcollective_broadcast type=topic

$ rabbitmqadmin declare exchange vhost=/mcollective name=mcollective_directed type=direct 
</code></pre>

<h3>Configuring Mcollective</h3>

<p>Now install the mcollective from APT repository. We can use the puppetlabs APT repo for the latest versions.</p>

<pre><code>$ echo "deb http://apt.puppetlabs.com precise main" &gt;&gt; /etc/apt/sources.list

$ wget -o /tmp/pubkey.gpg apt.puppetlabs.com/pubkey.gpg &amp;&amp; apt-key add /tmp/pubkey.gpg

$ apt-get intstall mcollective
</code></pre>

<p>Now go to &#8221;<em>/etc/mcollective/</em>&#8221; folder and open the <code>server.cfg</code> file and add the below lines to enable the rabbitmq connector.</p>

<pre><code>direct_addressing = 1

connector = rabbitmq
plugin.rabbitmq.vhost = /mcollective
plugin.rabbitmq.pool.size = 1       =&gt; increase this value, if you are going to use rabbitmq cluster
plugin.rabbitmq.pool.1.host = stomp.example.com
plugin.rabbitmq.pool.1.port = 61613
plugin.rabbitmq.pool.1.user = mcollective
plugin.rabbitmq.pool.1.password = marionette
</code></pre>

<p>And restart the mcollective service, to make the changes to come effect.</p>

<h3>Setting up McoMaster</h3>

<p>Mcomaster is an HTML5 web interface to mcollective, which uses REDIS discovery method. Thanks to <a href="https://github.com/ajf8/mcomaster">@ajf8</a> for such a wonderful project. Clone the repository from <a href="https://github.com/ajf8/mcomaster">github</a>.</p>

<pre><code>$ git clone https//github.com/ajf8/mcomaster.git
</code></pre>

<p>Now copy the <code>meta.rb</code> registration file to the mcollective extension directory of all the Mcollective servers.</p>

<pre><code>$ cp mcollective/registration/meta.rb /usr/share/mcollective/plugins/mcollective/registration/meta.rb
</code></pre>

<p>We need to enable the registration agent on ALL nodes with the following server.cfg settings</p>

<pre><code>registerinterval = 300
registration = Meta
</code></pre>

<p>Now enable <code>direct addressing</code> by adding the below line in the server.cfg of all the server&#8217;s where we have copied the meta registration plugin.</p>

<pre><code>direct_addressing=1
</code></pre>

<p>We need one mcollective node which receives the registrations and saves them in redis. Ensure that Redis server is installed on that server. We can use the same server where we are going to install mcomaster.</p>

<pre><code>$ cp mcollective/agent/registration.rb /usr/share/mcollective/plugins/mcollective/agent/
</code></pre>

<p>Add the redis server details to the server.cfg on the server which receives the registrations.</p>

<pre><code>plugin.redis.host = localhost
plugin.redis.port = 6379
plugin.redis.db = 0
</code></pre>

<p>Install mcollective clinet on the server where we are installing the mcomaster. Then configure the discovery agent, which will query the redis database for discovery data.</p>

<pre><code>$ cp mcollective/discovery/redisdiscovery.* /usr/share/mcollective/plugins/mcollective/discovery/
</code></pre>

<p>And add the following settings to the <code>client.cfg</code></p>

<pre><code>plugin.redis.host = localhost
plugin.redis.port = 6379
plugin.redis.db = 0
default_discovery_method = redisdiscovery
direct_addressing = yes
</code></pre>

<p>Using <code>mco</code> command we can check the node discovery using redisdiscovery method, which is the default mwthod now.. The old <code>mc</code> method will also works.</p>

<pre><code>root@testcloud:~# mco rpc rpcutil ping --dm redisdiscovery -v
info 2013/05/29 10:08:45: rabbitmq.rb:10:in `on_connecting' TCP Connection attempt 0 to stomp://mcollective@localhost:61613
info 2013/05/29 10:08:45: rabbitmq.rb:15:in `on_connected' Conncted to stomp://mcollective@localhost:61613
Discovering hosts using the redisdiscovery method .... 4

 * [ ============================================================&gt; ] 4 / 4


deeptest                                : OK
    {:pong=&gt;1369802330}

ubuntults                               : OK
    {:pong=&gt;1369802330}

debwheez                                : OK
    {:pong=&gt;1369802330}

testcloud                               : OK
    {:pong=&gt;1369802326}



---- rpcutil#ping call stats ----
           Nodes: 4 / 4
     Pass / Fail: 4 / 0
      Start Time: 2013-05-29 10:08:46 +0530
  Discovery Time: 30.95ms
      Agent Time: 85.24ms
      Total Time: 116.19ms
info 2013/05/29 10:08:46: rabbitmq.rb:20:in `on_disconnect' Disconnected from stomp://mcollective@localhost:61613
</code></pre>

<p>Go to McoMaster repo folder, and run <code>bundle install</code> to install all the dependency gems. Copy th existing application.example.yml and create a new <em>application.yml</em> file. Edit this file and fill in the Username, Password, Redis server details etc. Create a new <code>database.yml</code> and fill in the database details. I&#8217;m using MYSQL as the database backend. Below is my config.</p>

<pre><code>production:
  adapter: mysql
  database: mcollective
  username: mcollective
  password: mcollective
  host: localhost
  socket: "/var/run/mysqld/mysqld.sock"

development:
  adapter: mysql
  database: mcollective
  username: mcollective
  password: mcollective
  host: localhost
  socket: "/var/run/mysqld/mysqld.sock"
</code></pre>

<p>We need to migrate the database,</p>

<pre><code>$ RAILS_ENV=production rake db:reset
</code></pre>

<p>After that, we need to compile the assets,</p>

<pre><code>$ rake assets:precompile
</code></pre>

<p>Let&#8217;s start the server,</p>

<pre><code>$ rails server -e production
</code></pre>

<p>The above command will make the application to listen to default port &#8221;<em>3000</em>&#8221;. Access the McoMaster GUI from the browwser using the username and password added in the &#8221;<em>application.yml</em>&#8221; file. Once we login, we will be able to see the nodes discovered through the redisdiscovery method. If we add the plugins like <em>service</em> and <em>process</em>, we will be able to see the plugins showing up in the mcomaster, and we can also run these plugins with in the McoMaster.</p>

<p>This is a screenshot of the MCoMaster dashboard.</p>

<p><img src="http://beingasysadmin.com/images/mcomaster.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Runing MCollective in a Multi-Ruby Environment]]></title>
    <link href="http://beingasysadmin.com/blog/2013/05/28/runing-mcollective-in-a-multi-ruby-environment/"/>
    <updated>2013-05-28T09:34:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/05/28/runing-mcollective-in-a-multi-ruby-environment</id>
    <content type="html"><![CDATA[<p>Mcollective one of the best open sourced orchestration tools. It&#8217;s build on Ruby and depends on Ruby1.8. But now a days most of the Ruby apps are build on Ruby1.9+, so the default Ruby versions will be 1.9+. This ususally causes problem for Mcollective to run. By default when we install Mcollective from APT, it will install Ruby1.8 also. So the best hack to make Mcollective run under such Multi-Ruby setup is edit the <code>/usr/sbin/mcollectived</code> file,</p>

<p>From <strong>#!/usr/bin/env ruby</strong>*,  Change it to <strong>#!/usr/bin/env ruby1.8</strong>.</p>

<p>This will help the mcollective to work normally. That&#8217;s it, a small hack, but will helps us in Multi Ruby environment</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Plugging QPSMTPD Service with QMAIL]]></title>
    <link href="http://beingasysadmin.com/blog/2013/05/01/plugging-qpsmtpd-service-with-QMAIL/"/>
    <updated>2013-05-01T15:26:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/05/01/plugging-qpsmtpd-service-with-QMAIL</id>
    <content type="html"><![CDATA[<p><em>QPSMTPD</em> is a flexible smtp daemon written in Perl. Apart from the core SMTP features, all functionality is implemented in small <em>extension plugins</em> using the easy to use object oriented plugin API. Basically I uses Qmail Based mail server&#8217;s with a custom <em>qmailqueue</em>. We uses the <em>tcpsvd (TCP/IP Service Daemon)</em> for the smtp service and the mails are passed to a custom <em>qmailqueue</em> which is basically a perl script with custom filters for filtering out the mails. <em>QPSMTPD</em> has a verity of custom plugins which includes <em>SPF</em> check, <em>DKIM</em> check and even for <em>DMARC</em> also. If you are a Perl guy, then you can build custom plugins. The main reason why i got attracted to QPSMTPD was because of its plugin nature.</p>

<p>In this blog i will explain on how to setup <em>QPSMTPD</em> along with <em>QMAIL MTA</em>. As i mentioned earlier, i&#8217;m using a custom qmailqueue, where i have some custom filtering which varies from client to client. So i will be using QPSMTPD to do initail filtering like checking <em>DNSBL/RBL</em>, <em>SPF</em> check, <em>DKIM</em>, <em>DMARC</em> etc. A breif info about various qpsmtpd plugins are available <a href="http://wiki.qpsmtpd.org/plugins">here</a></p>

<p>Ths qpsmtpd source is available in <a href="https://github.com/smtpd/qpsmtpd/">Github</a>. The soucre comes with default run scripts which can be used with <em>runit/daemontools</em>. Clone the qpsmtpd source and install the dependency Perl modules.</p>

<pre><code>$ git clone https://github.com/smtpd/qpsmtpd/

$ cpan -i Net::DNS

$ cpan -i MIME::Base64

$ cpan -i Mail::Header
</code></pre>

<p>Now create a user for the qpsmtp service, say &#8221;<em>smtp</em>&#8221; with home folder as the location of the qpsmtpd folder and chown the qpsmtpd folder using the smptp user. Add sticky bit to the qpsmtpd folder by running <code>chmod o+t qpsmtpd</code>, in order to make supervise start the log process. By deafult inside the source folder there will be a sample config folder called <em>&#8220;config.sample&#8221;</em>. Copy the entire folder and create a new config folder.</p>

<pre><code>$ cp config.sample config
</code></pre>

<p>In the config folder, edit the &#8221;<em>IP</em>&#8221; in order to mention which ip the qpsmtpd daemon should bind. Putting &#8220;0&#8221; will bind to all the interfaces. Now if we go through the qpsmtpd&#8217;s <code>run</code> script in the source folder, it depends on two binaries <em>softlimit</em> and <em>tcpserver</em>. The <em>softlimit</em> binary comes with the <em>daemontools</em> debian package and the <em>tcpserver</em> binary comes with the <em>ucspi-tcp</em> debian package. so let&#8217;s install those two packages.</p>

<pre><code>$ apt-get install ucspi-tcp daemontools runit
</code></pre>

<p>Now start the qpsmtpd server. I&#8217;m using runit for service supervision.</p>

<pre><code>$ update-service --add /usr/local/src/qpsmtpd qpsmtpd
</code></pre>

<p>The above command will add the service. We can check the service status using <code>sv s qpsmtpd</code> command. This will show us whether the serivce is running or not. Now go inside the &#8220;config&#8221; folder and open the &#8220;plugin&#8221; file. This is where we enable the plugins, by addin the plugin names with corresponding options. By default the &#8221;<em>rcpt_ok</em>&#8221; plugin must be enabled. This plugin handles the <em>qmail&#8217;s rcpthosts feature</em>. It accepts the emails for the domains mentioned in the &#8221;<em>rcpthosts</em>&#8221; file present in the config folder. If this is not enabled it will not accept any mails. So the best way to understand how each plugin works is comment out all the plugins except &#8220;rcpt_ok&#8221; and then add the plugins one by one. The plugins are available in the &#8220;plugin&#8221; folder in the qpsmtpd source folder. All the basic info about the plugins are mentioned in the plugin files itself.</p>

<p>Now most commonly used plugins are <em>auth</em> for SMTP AUTH, <em>DNSBL/RBL</em>, <em>spamassassin</em>, etc. We can enable these plugins by adding the names in the <code>config/plugin</code> files. For example, since i&#8217;m using a custom qmailqueue, once the qpsmtpd has accepted the mail, it has to be queued to my custom QMAILQUEUE. So i need to enable the &#8220;queue plugin&#8221;. I can enable the plugin by adding the below line to the plugin file inside the config folder.</p>

<pre><code>queue/qmail-queue /var/qmail/bin/qmail-scanner-queue
</code></pre>

<p>If you are using any other MTA, you can provide the corresponding MTA&#8217;s queue. For example for postfix &#8221;<em>postfix-queue</em>&#8221;, and for exim use &#8221;<em>exim-bsmtp</em>&#8221;, or if you want to use QPSMTPD as a relaying server, you can use &#8221;<em>smtp-forward</em>&#8221; plugin for relaying mails to another SMTP server. So once the mail has been accepted by qpsmtpd, it will queue the mail to my custom qmail queue, and then it will start the mail delivery. Similarly i use ldap backend for smtp authentication. So i need to enable &#8221;<em>auth/auth_ldap_bind</em>&#8221; plugin for this. Like that we can add other plugins too. By default <em>DMARC</em> plugin is not added, but we can get it from <a href="https://github.com/qpsmtpd-dev/qpsmtpd-dev/tree/master/plugins">here</a>.</p>

<p>Use tools like swaks for sending test mails, because plugins like check_basicheaders will not accept mails without proper headers, so using telnet to send mails wont work some times. Swaks is a good tool for sending test mail. We can increase the loglevel, by editing <code>config/loglevel</code> file. It&#8217;s better to increase the log level to debug so that we will get more details of errors. Some plugins needs certain Perl modules, if it&#8217;s missing the error will popup in the qpsmtpd logs, so use cpan and install those perl modules.</p>

<p>By default the run script uses tcpserver to start the service. There many other ways of deployments like <strong><em>forkserver</em></strong>,<strong><em>pre-fork daemon</em></strong>,<strong><em>Apache::Qpsmtpd</em></strong> etc. To use the default <em>TLS</em> plugin, we need to use the &#8221;<em>forkserver model</em>&#8221;. The forke server model script is availbale in the same run script, but it is commented by default. The default spool directory will be a &#8221;<em>tmp</em>&#8221; folder inside the QPUSER&#8217;s ie, the user &#8220;smtp&#8221; home folder. In my case i&#8217;m using a separate folder for spool, <code>/var/spool/qpsmtp</code>, for such cases, edit <code>lib/Qpsmtpd.pm</code> and go to &#8221;<em>spool_dir</em>&#8221; subroutine and add &#8221;<strong><em>$Spool_dir = &#8220;/var/spool/qpsmtpd/&#8221;;&#8221;</em></strong>. Now create the spool directory with owner as &#8221;<em>smtp</em>&#8221; user and folder permission &#8221;<em>0700</em>&#8221; and then restart the qpsmtpd service.</p>

<p>Now to enable TLS, enable the <em>tls</em> plugin in the <code>config/plugin</code> file like this &#8221;<strong>tls cert_path priv_key_path ca_path</strong>&#8221;. If there is no TLS certificate available ,then we can generate using a perl script &#8221;<em>tls_cert</em>&#8221;, which is available at the <em>plugins</em> folder. Now we need to edit the <code>config/tls_before_auth</code> file and put the value <em>&#8220;0&#8221;</em>, otherwise AUTH will not be offered unless TLS/SSL are in place. Now we can try  sending a test mail using swaks with TLS enabled. Below is my swaks output.</p>

<pre><code>=== Trying 192.168.42.189:587...
=== Connected to 192.168.42.189.
&lt;-  220 beingasysadmin.com ESMTP  send us your mail, but not your spam.
 -&gt; EHLO deeptest.beingasysadmin.com
&lt;-  250-beingasysadmin.com Hi deeptest [192.168.42.184]
&lt;-  250-PIPELINING
&lt;-  250-8BITMIME
&lt;-  250-SIZE 5242880
&lt;-  250-STARTTLS
&lt;-  250 AUTH PLAIN LOGIN CRAM-MD5
 -&gt; STARTTLS
&lt;-  220 Go ahead with TLS
=== TLS started w/ cipher xxxxxx-xxx-xxxxxx
=== TLS peer subject DN="/C=XY/ST=unknown/L=unknown/O=QSMTPD/OU=Server/CN=debwheez.beingasysadmin.com/emailAddress=postmaster@debwheez.beingasysadmin.com"
 ~&gt; EHLO deeptest.beingasysadmin.com
&lt;~  250-beingasysadmin.com Hi deeptest [192.168.42.184]
&lt;~  250-PIPELINING
&lt;~  250-8BITMIME
&lt;~  250-SIZE 5242880
&lt;~  250 AUTH PLAIN LOGIN CRAM-MD5
 ~&gt; AUTH PLAIN AGRlZXBhawBteWRlZXByb290
&lt;~  235 PLAIN authentication successful for deepak - authldap/plain
 ~&gt; MAIL FROM:&lt;deepak@beingasysadmin.com&gt;
&lt;~  250 &lt;deepak@beingasysadmin.com&gt;, sender OK - how exciting to get mail from you!
 ~&gt; RCPT TO:&lt;deepakmdass88@gmail.com&gt;
&lt;~  250 &lt;deepakmdass88@gmail.com&gt;, recipient ok
 ~&gt; DATA
&lt;~  354 go ahead
 ~&gt; Date: Wed, 01 May 2013 23:19:54 +0530
 ~&gt; To: deepakmdass88@gmail.com
 ~&gt; From: deepak@beingasysadmin.com
 ~&gt; Subject: testing TLS + Auth in qpsmtpd
 ~&gt; X-Mailer: swaks v20120320.0 jetmore.org/john/code/swaks/
 ~&gt;
 ~&gt; This is a test mailing
 ~&gt;
 ~&gt; .
&lt;~  250 Queued! 1367430597 qp 9222 &lt;&gt;
 ~&gt; QUIT
&lt;~  221 beingasysadmin.com closing connection. Have a wonderful day.
=== Connection closed with remote host. 
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DKIM Signing in QMAIL]]></title>
    <link href="http://beingasysadmin.com/blog/2013/04/30/dkim-signing-in-qmail/"/>
    <updated>2013-04-30T11:19:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/04/30/dkim-signing-in-qmail</id>
    <content type="html"><![CDATA[<p><em>DKIM</em> and <em>SPF</em> are becoming most commonly adopted methods for email validation. Even if we want to use the DMARC (<em>Domain-based Message Authentication, Reporting &amp; Conformance</em>), we need to configure  SPF and DKIM first. DMARC acts as a layer above the SPF and DKIM. DMARC allows the receiever&#8217;s mail server to check if the Email is aligned properly as per the DMARC policy, and it queries the sender&#8217;s DNS server for the DMARC action, ie, whether to reject or quarantine if alignment fails. The action will be mentioned in the TXT record on the Sender&#8217;s DNS server. There is a good collection of DMARC training videos available in <a href="http://www.maawg.org/activities/training/dmarc-video-1">MAAWG site</a>. We will get a clear idea on how DMARC works from those videos.</p>

<p>In this post, i will explain on how to make Qmail to do DKIM sign on the outgoing mails. There is a <a href="http://www.brandonturner.net/blog/2009/03/dkim-and-domainkeys-for-qmail/">qmail-patch</a> method available, but since i&#8217;m using qmail-1.0.3 with custom patch, i was not able to use the DKIM patch along with my custom patch. So the next method is to use a wrapper around &#8220;qmail-remote&#8221;, since qmail-remote is responsible for delivering remote mails, a wrapper around it will help us to sign the email and then start the remote delivery. There are a few wrappers mentioned in this <a href="http://www.memoryhole.net/qmail/#dkim">site</a>. I&#8217;m going to use this <a href="http://www.memoryhole.net/qmail/qmail-remote.sh">qmail-remote</a> wrapper.</p>

<h4>Initial Settings</h4>

<p>First move the current &#8221;<em>qmail-remote</em>&#8221; binary to &#8221;<em>qmail-remote.orig</em>&#8221;. Now download the wrapper and move it to the <code>/var/qmail/bin/</code> file.</p>

<pre><code>$ mv /var/qmail/bin/qmail-remote /var/qmail/bin/qmail-remote.orig

$ wget -O /var/qmail/bin/qmail-remote "http://www.memoryhole.net/qmail/qmail-remote.sh"

$ chmod 755 /var/qmail/bin/qmail-remote
</code></pre>

<p>This wrapper depends on two programs, 1) <strong><em>dktest</em></strong>, which comes with the <a href="http://domainkeys.sourceforge.net/">libdomainkeys</a>,  2)  <a href="http://www.memoryhole.net/qmail/dkimsign.pl">dkimsign.pl</a>, which is perl script for signing the emails. Both these files, must be available at the path mentioned in the &#8220;qmail-remote&#8221; wrapper file.</p>

<p>Go through the &#8221;<em>dkimsign.pl</em>&#8221; script and install the Perl modules mentioned in it using cpan. There is no official debian package for libdomainkeys, so we need to compile it from the source.</p>

<h4>setting up dktest</h4>

<p>Download the latest source code from the sourceforge <a href="http://sourceforge.net/projects/domainkeys/">link</a>.</p>

<pre><code>$ tar -xzf libdomainkeys-0.69.tar.gz

$ cd libdomainkeys-0.69
</code></pre>

<p>Edit the Makefile and add &#8221;<em>-lresolv</em>&#8221; to the end of the &#8221;<em>LIBS</em>&#8221; line and run <code>make</code></p>

<pre><code>$ install -m 644 libdomainkeys.a /usr/local/lib

$ install -m 644 domainkeys.h dktrace.h /usr/local/include

$ install -m 755 dknewkey /usr/bin

$ install -m 755 dktest /usr/local/bin
</code></pre>

<h4>Generate Domain keys for the domains</h4>

<p>Before we can sign an email, we must create at least one <em>public/private key pair</em>. I&#8217;m going to create a key pair for the domain &#8220;example.com&#8221;.</p>

<pre><code>$ mkdir -p /etc/domainkeys/example.com

$ cd /etc/domainkeys/example.com

$ dknewkey default 1024 &gt; default.pub

$ chown -R root:root /etc/domainkeys

$ chmod 640 /etc/domainkeys/example.com/default

$ chown root:qmail /etc/domainkeys/example.com/default
</code></pre>

<p>It is very important that the default file be readable only by root and the group which qmailr (the qmail-remote user) belongs to. Now add a <em>TXT</em> entry to the DNS for &#8221;<em>default._domainkey.example.com</em>&#8221; containing the quoted part in the <code>/etc/domainkeys/example.com/default.pub</code></p>

<p>Once everything is added, restart the &#8220;qmail-send&#8221; and send a test mail to any non local domain. IF things goes fine, we can see a line like the below in &#8220;qmail-send&#8221; log.</p>

<pre><code>$ @40000000517f518b1e1eb75c delivery 1: success: ktest_---_/tmp/dk2.sign.Gajw948FX1A1L0hugfQ/in_dkimsignpl_---_/tmp/dk2.sign.Gajw948FX1A1L0hugfQ/r74.125.25.27_accepted_message./Remote_host_said:_250_2.0.0_OK_1367298812_ps11si19566038pab.170_-_gsmtp/
</code></pre>

<p>Once the DKIM is working properly, add the SPF entries in our DNS, and we are ready to try out DMARC. DMARC is already in use by mail giants like Google,Yahoo,Paypal,Linkedin etc.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sensu Admin - A GUI for Sensu API]]></title>
    <link href="http://beingasysadmin.com/blog/2013/04/28/sensu-admin-a-gui-for-sensu-api/"/>
    <updated>2013-04-28T18:50:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/04/28/sensu-admin-a-gui-for-sensu-api</id>
    <content type="html"><![CDATA[<p>In my previous post&#8217;s, i&#8217;ve explained on How to setup Sensu server and setting up check&#8217;s and handler&#8217;s. The default dashboard is very simple with limited options, but for those who wants a full fledged dashboard, there is a Rails project in Github <a href="https://github.com/sensu/sensu-admin.git">Sensu-Admin</a>. So let&#8217;s try setting it up.</p>

<p>First clone the repository from Github.</p>

<pre><code>$ git clone https://github.com/sensu/sensu-admin.git
</code></pre>

<p>Now go to sensu-admin folder, and run <code>bundle install</code> to install all the dependency gems. Now go inside the &#8221;<em>config</em>&#8221; folder, edit the &#8221;<em>database.yml</em>&#8221; and fill in the database details. I&#8217;m going to use mysql, below is my database config.</p>

<pre><code>development:
   adapter: mysql2
   database: sensudb
   username: sensu
   password: secreto
   host: localhost
production:
   adapter: mysql2
   database: sensudb
   username: sensu
   password: secreto
   host: localhost
</code></pre>

<p>Now run <code>rake db:migrate</code> and then  <code>rake db:seed</code>. The seed file creates auser account named &#8221;<strong><em>admin@example.com</em></strong>&#8221; with password &#8221;<strong><em>secret</em></strong>&#8221;.</p>

<p>We can start the Rails app by running &#8220;rails s&#8221;, this will start the app using the thin webserver at port <em>3000</em>. Access the dashboard using the url &#8221;<strong><em>http://server_ip:3000</em></strong>&#8221; Login to the dashboard with the admin@example.com and go to the &#8220;*Account&#8221; tab and modify the default user name and password. Now we go through tabs and check if it displays the checks, clients, events etc properly. This is a <a href="http://beingasysadmin.com/images/SensuAdmin.png">screenshot</a> of the SensuAdmin dashboard.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sensu - Setting up Check's and Handler's]]></title>
    <link href="http://beingasysadmin.com/blog/2013/04/25/sensu-setting-up-checks-and-handlers/"/>
    <updated>2013-04-25T12:26:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/04/25/sensu-setting-up-checks-and-handlers</id>
    <content type="html"><![CDATA[<p>In my previous <a href="http://beingasysadmin.com/blog/2013/04/23/sensu-cloud-monitoring-tool/">post</a>, i&#8217;ve explained on how to setup Sensu Server and Client. Now i&#8217;m going to explain how to setup Check&#8217;s and Handler&#8217;s in Sensu. There is a very good collection of <a href="https://github.com/sensu/sensu-community-plugins">sensu-community-plugins</a>.</p>

<h4>Setting up Check&#8217;s</h4>

<p>On the Sensu <em>Client Node</em>,</p>

<p>First clone the plugins repository on the client node. Now install the &#8221;<strong>sensu-plugin</strong>&#8221; gem on the client node. And then copy the required plugins to <code>/etc/sensu/plugins/</code> folder.</p>

<p>On the Sensu <em>Server</em>,</p>

<p>We need to define the check first. Create a json config file for the check in <code>/etc/sensu/conf.d</code>. Following is a sample check config,</p>

<pre><code>     {
    "checks": {
         "snmp_check": {
         "handlers": ["default"],
         "command": "/etc/sensu/plugins/check-snmp.rb -w 10 -c 20",
         "interval": 30,
         "subscribers": [ "snmp" ]
          }
      }
   }
</code></pre>

<p>The above check will be applied to all clients subscribed to &#8221;<em>snmp</em>&#8221; exchange. Based on the interval, Server will publish this check request, which will reach all the clients subscribed to the &#8221;<em>snmp</em>&#8221; exchange using an arbitrary queue. The client will run the command mentioned in the command part, and then it will publish the result back to th server through <em>Result queue</em>. The <a href="https://github.com/deepakmdass88/my-sensu-plugins.git">check_snmp</a> is a small plugin written by me. If we check the sensu-server log, we can see the result coming from the client machine. Below one is a similar log output in my sensu-server log.</p>

<pre><code>{"timestamp":1366968018},"check":{"handlers":["default","mailer"],"command":"/etc/sensu/plugins/check-snmp.rb -w 1 -c 3","interval":100,"subscribers":["snmp"],"name":"snmp_check","issued":1366968407,"executed":1366968028,"output":"CheckSNMP WARNING: Warning state detected\n","status":1,"duration":0.526,"history":["0","0","1"]},"occurrences":1,"action":"create"},"handler":{"type":"pipe","command":"true","name":"default"}}
</code></pre>

<p>The above log line shows us what are handler&#8217;s enabled for this check, what is the executed command, subcribers, name of the check, timestamp at the time when the command was issued, timestamp of the time when the server has received the result, Output of the check command etc. If there is any while executing th check command, we can see the errors popping in the log&#8217;s soon after this line in the server log.</p>

<h4>Setting up Handler&#8217;s</h4>

<p>Sensu has got a very good collection Handler&#8217;s, available at the sensu-community-plugin repo in github. For example there is a hanlder called &#8221;<em>show</em>&#8221;, available at the debug section in Handler&#8217;s, which will display a more debug report about the Event as well as the Sensu server&#8217;s settings. This is the <a href="http://beingasysadmin.com/images/show-handler.png">output</a> which i got after applying &#8221;<em>show</em>&#8221; handler in my serverlog. But it&#8217;s not possible to go check the log&#8217;s continously, so there another plugin called &#8220;mailer&#8221;, which can send email alerts like how nagios does.</p>

<p>So first get the &#8220;mailer&#8221; plugin files from the sensu-community-plugin repo in github.</p>

<pre><code>wget -O /etc/sensu/handlers/mailer.rb https://raw.github.com/sensu/sensu-community-plugins/master/handlers/notification/mailer.rb
wget -O /etc/sensu/conf.d/mailer.json https://raw.github.com/sensu/sensu-community-plugins/master/handlers/notification/mailer.json
</code></pre>

<p>Now edit the mailer.json, and change the settings to fit to our environment. We need to define a new pipe handler for this new handler. So create a file <code>/etc/sensu/conf.d/handler_mailer.json</code>, and add the below lines to it.</p>

<pre><code>        {
    "handlers": {
        "mailer": {
        "type": "pipe",
        "command": "/etc/sensu/handlers/mailer.rb"
        }
          }
      }
</code></pre>

<p>Now go to the one of the check config files, where we want to apply this new &#8220;mailer&#8221; handler.</p>

<pre><code>           {
    "checks": {
         "snmp_check": {
         "handlers": ["default", "mailer"],         
         "command": "/etc/sensu/plugins/check-snmp.rb -w 10 -c 20",
         "interval": 30,
         "subscribers": [ "snmp" ]
          }
      }
   }
</code></pre>

<p>Now restart the sensu-server to make the new changes to come into effect. If everything goes fine, when the sensu detects a state change it will execute this mailer handler, we can also see the below lines in server log.</p>

<pre><code>"action":"create"},"handler":{"type":"pipe","command":"/etc/sensu/handlers/mailer.rb","name":"mailer"
</code></pre>

<p>Sensu is executing the mailer script, and if there is any problem, we will see the corresponding error following the above line, or we will receive the email alert to email id mentioned in the &#8220;mailer.json&#8221; file. But in my case, i was getting an error, when the sensu invoked the &#8220;mailer&#8221; handler.</p>

<pre><code>{"timestamp":"2013-04-25T15:03:32.002132+0530","level":"info","message":"/etc/sensu/handlers/mailer.rb:28:in `handle': undefined method `[]' for nil:NilClass (NoMethodError)"}
{"timestamp":"2013-04-25T15:03:32.002308+0530","level":"info","message":"\tfrom /var/lib/gems/1.9.1/gems/sensu-plugin-0.1.7/lib/sensu-handler.rb:41:in `block in &lt;class:Handler&gt;'"}
</code></pre>

<p>After playing for some time, i came to know that, it was not parsing the options from the mailer.json file, so i manually added the smtp and email settings directly in <em>mailer.rb</em> file. Then it started working fine. I&#8217;m writing a small script which will be using the basic &#8216;net/smtp&#8217; library to send out mails. There are many other cool Handler&#8217;s like sending matrices to Graphite, Logstash, Graylog, sending notifcations to irc,xmpp,campfire etc. Compare to traditional monitoring tools, Sensu is an Amazing tool, we can use any check script&#8217;s, whether it&#8217;s ruby or perl or bash, doesn&#8217;t matter. The one common thing which i heard about other people, was the lack of proper dashboard like the traditional monitoring tools. Though Sensu dashboard is a simple one, i&#8217;m sure it will improve a lot in future.</p>

<p>Since I&#8217;m a CLI Junky, I dont care much about the dashboard thing, apart from that i have many good and interesting stuffs to hang around with Sensu. Cheers to <a href="https://twitter.com/portertech">portertech</a> and <a href="http://www.sonian.com/cloud-monitoring-sensu/">sonian</a> for open sourcing such an amazing tool.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sensu - Cloud Monitoring Tool]]></title>
    <link href="http://beingasysadmin.com/blog/2013/04/23/sensu-cloud-monitoring-tool/"/>
    <updated>2013-04-23T22:20:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/04/23/sensu-cloud-monitoring-tool</id>
    <content type="html"><![CDATA[<p>Monitoring always plays an important role, especially for sysadmins. There are a lot of Monitoring tools available, like Nagios, Zenoss, Icinga etc. <code>Sensu</code> is a new generation Cloud monitoring tool designed by <a href="http://www.sonian.com/cloud-monitoring-sensu/">Sonian</a>. <em>Sensu</em> is bascially written in <strong><em>Ruby</em></strong>, uses <strong><em>RabbitMQ</em></strong> Server as the Message Broker for Message transactions, and <strong>Redis</strong> for storing the data&#8217;s.</p>

<p> Sensu has <strong>3</strong> operation Mode.</p>

<p> 1) <strong>Request-Reply Mode</strong>, where the server will send a check request to the clients through the RabbitMQ and the clients will reply back the results.</p>

<p> 2) <strong>Standalone Mode</strong>, where the server will not send any check request, instead the client itself will run the checks according to interval mentioned, and sends the results to the sensu master through the Result queue in RabbitMQ.</p>

<p> 3) <strong>Push Mode</strong>, where the client will send out results to a specific handler.</p>

<p>So now we can start installing the dependencies for sensu, ie, RabbitMQ and Redis.</p>

<h3>Setting up RabbitMQ</h3>

<p>Let&#8217;s add the <strong>RabbitMQ</strong> official APT repo.</p>

<pre><code>$ echo "deb http://www.rabbitmq.com/debian/ testing main" &gt;/etc/apt/sources.list.d/rabbitmq.list

$ curl -L -o ~/rabbitmq-signing-key-public.asc http://www.rabbitmq.com/rabbitmq-signing-key-public.asc

$ apt-key add ~/rabbitmq-signing-key-public.asc &amp;&amp; apt-get update
</code></pre>

<p>Now we can install RabbitMQ</p>

<pre><code>$ apt-get install rabbitmq-server erlang-nox
</code></pre>

<p>Now we need to generate SSL certificates for RabbitMQ and the sensu clients. We can use RabbitMQ with out ssl also, but it will more secure with SSL, <a href="http://github.com/joemiller">@joemiller</a> has wrote a script to generate the SSL certificates. It&#8217;s avaliable in his GitHub <a href="http://github.com/joemiller/joemiller.me-intro-to-sensu.git">repo</a>. Clone the repo and modify the &#8220;openssl.cnf&#8221; according to our need and then we can go ahead with generating the certificates.</p>

<pre><code>$ git clone git://github.com/joemiller/joemiller.me-intro-to-sensu.git

$ cd joemiller.me-intro-to-sensu/

$ ./ssl_certs.sh clean &amp;&amp; /ssl_certs.sh generate
</code></pre>

<p>Now copy the server key and cert files to the RabbitMQ folder in &#8220;/etc/rabbitmq/&#8221;</p>

<pre><code>$ mkdir /etc/rabbitmq/ssl

$ cp server_key.pem /etc/rabbitmq/ssl/

$ cp server_cert.pem /etc/rabbitmq/ssl/

$ cp testca/cacert.pem /etc/rabbitmq/ssl/
</code></pre>

<p>Now create the RabbitMQ config file, &#8220;/etc/rabbitmq/rabbitmq.config&#8221;, and add the following lines in it.</p>

<pre><code>[
  {rabbit, [
      {ssl_listeners, [5671]},
      {ssl_options, [{cacertfile,"/etc/rabbitmq/ssl/cacert.pem"},
               {certfile,"/etc/rabbitmq/ssl/server_cert.pem"},
               {keyfile,"/etc/rabbitmq/ssl/server_key.pem"},
               {verify,verify_peer},
               {fail_if_no_peer_cert,true}]}
    ]}
].
</code></pre>

<p>Once the config file is created, restart the RabbitmQ server. Now RabbitMQ has a cool management console, we can enable this by running &#8221;<strong><em>rabbitmq-plugins enable rabbitmq_management</em></strong>&#8221; in console. Once the Management console is enabled, we can access it RabbitMQ Web UI: <strong>Username is &#8220;guest&#8221;, password is &#8220;guest&#8221; - http://SENSU-SERVER:55672</strong>. Protocol amqp should be bound to port 5672 and amqp/ssl on port 5671.</p>

<p>Now let&#8217;s create a vhost and user for Sensu in RabbitMQ.</p>

<pre><code> $ rabbitmqctl add_vhost /sensu

 $ rabbitmqctl add_user sensu mypass

 $ rabbitmqctl set_permissions -p /sensu sensu ".*" ".*" ".*"
</code></pre>

<h3>Setting up Redis Server</h3>

<p>Now we can set up Redis server. This will used by Sensu for stroring data&#8217;s. Ubuntu&#8217;s Apt repo ships with latest Redis server, so we can directly install it.</p>

<pre><code>$ apt-get install redis-server
</code></pre>

<h3>Installing Sensu Server</h3>

<p>Sensu has a public repository which can be used to install the necessary sensu packages. First we need to add the repository public key.</p>

<pre><code>$ wget -q http://repos.sensuapp.org/apt/pubkey.gpg -O- | sudo apt-key add -
</code></pre>

<p>Now add the repo sources in APT</p>

<pre><code>$ echo " deb     http://repos.sensuapp.org/apt sensu main" &gt;&gt; /etc/apt/sources.list &amp;&amp; apt-get update

$ apt-get install sensu
</code></pre>

<p>Enable the sensu services to start automatically during system startup.</p>

<pre><code>$ update-rc.d sensu-server defaults

$ update-rc.d sensu-api defaults

$ update-rc.d sensu-client defaults

$ update-rc.d sensu-dashboard defaults
</code></pre>

<p>Copy the client ssl cert and key to <strong>/etc/sensu</strong> folder, say to a subfolder ssl.</p>

<pre><code>$ cp client_key.pem client_cert.pem  /etc/sensu/ssl/
</code></pre>

<p>Now we need setup the sensu master, create a file &#8221;<strong>/etc/sensu/config.json</strong>&#8221; and add the below lines.</p>

<pre><code>         {
        "rabbitmq": {
          "ssl": {
            "private_key_file": "/etc/sensu/ssl/client_key.pem",
            "cert_chain_file": "/etc/sensu/ssl/client_cert.pem"
          },
          "port": 5671,
          "host": "localhost",
          "user": "sensu",
          "password": "mypass",
          "vhost": "/sensu"
        },
        "redis": {
          "host": "localhost",
          "port": 6379
        },
        "api": {
          "host": "localhost",
          "port": 4567
        },
        "dashboard": {
          "host": "localhost",
          "port": 8080,
          "user": "admin",
          "password": "sensu@123"
        },
        "handlers": {
          "default": {
            "type": "pipe",
            "command": "true"
          }
        }
      }
</code></pre>

<p>By default sensu package comes with all sensu-server,sensu-client,sensu-api and sensu-dashboard., If we dont want to use the current machine as a client, we can stop the sensu-client from running, and do not create the client config. But for testing purpose, i&#8217;m going to add the current machine as client also. Create a file &#8221;<strong>/etc/sensu/conf.d/client.json</strong>&#8221; and add the client configuration in JSON format.</p>

<pre><code>        {
          "client": {
          "name": "sensu.test.com",
          "address": "192.168.1.108",
          "subscriptions": [ "vmmaster" ]
         }
       }
</code></pre>

<p>Now restart the sensu-client to affect the changes. The logs are recorded at &#8221;<strong>/var/log/sensu/sensu-client.log</strong>&#8221; file. We can access the sensu-dashboard from &#8220;http://SENSU SERVER:8080&#8221;, with the username and password mentioned in the config.json file.</p>

<h3>Setting up a Separate Sensu-Client Node</h3>

<p>If we want to setup sensu-client on a separate node, just dd the Sensu apt repo, and install the sensu package. After that just enable only the sensu-client service and remove all other sesnu-services. Then create a config.json file and add only the rabbitmq server details in it. Now generate a separate SSL certificate for the new client and use that in the config file.</p>

<pre><code>       {
      "rabbitmq": {
        "ssl": {
          "private_key_file": "/etc/sensu/ssl/client1_key.pem",
          "cert_chain_file": "/etc/sensu/ssl/client1_cert.pem"
        },
        "port": 5671,
        "host": "192.168.1.108",
        "user": "sensu",
        "password": "mypass",
        "vhost": "/sensu"
      }
    }
</code></pre>

<p>Now create the  &#8220;client.json&#8221; in the &#8220;/etc/sensu/conf.d/&#8221; folder.</p>

<pre><code>        {
              "client": {
              "name": "client1.test.com",
              "address": "192.168.1.212",
              "subscriptions": [ "vmmaster" ]
             }
           }
</code></pre>

<p>Restart the the sensu-clinet, and check the &#8220;/var/log/sensu/sensu-client.log&#8221;, if things goes fine, we can see client connecting to the RabbitMQ server also we can see the config is getting applied.</p>

<pre><code>{"timestamp":"2013-04-23T22:53:27.870728+0530","level":"warn","message":"config file applied changes","config_file":"/etc/sensu/conf.d/client.json","changes":{"client":[null,{"name":"client1.test.        com","address":"192.168.1.212","subscriptions":["vmmaster"]}]}}
{"timestamp":"2013-04-23T22:53:27.879671+0530","level":"info","message":"loaded extension","type":"mutator","name":"only_check_output","description":"returns check output"}
{"timestamp":"2013-04-23T22:53:27.883504+0530","level":"info","message":"loaded extension","type":"handler","name":"debug","description":"outputs json event data"}
</code></pre>

<p>Once the Sensu Server and Client are configured successfully, then we can go ahead adding the check&#8217;s. One of the best thing of sensu, all the config&#8217;s are written in JSON format, which very easy for us to create as well as to understand things. In the next blog, i will explain on how to create the check&#8217;s and how to add these check&#8217;s to various clients, and how to add handler&#8217;s like Email alerts, Sending Metrics to graphite.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HARAKA - A NodeJS based SMTP server]]></title>
    <link href="http://beingasysadmin.com/blog/2013/04/14/haraka-a-nodejs-based-smtp-server/"/>
    <updated>2013-04-14T22:51:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/04/14/haraka-a-nodejs-based-smtp-server</id>
    <content type="html"><![CDATA[<p>Today i came across a very interesting project in GITHUB. <a href="https://github.com/baudehlo/Haraka.git">HARAKA</a> is an SMTP server written completely in <em>NodeJS</em>. Like the qpsmtpd, apart from the core SMTP features we can improve the functionality using small plugins. There are very good pluginsi for HARAKA, basically in javascripts. Like Postfix,Qmail, we can easily implements all sorts of checks and features with the help of these plugins.</p>

<p>Setting up <code>HARAKA</code> is very simple. In my setup, i will be using HARAKA as my primary smtp server, where i will implement all my filterings and then i will relay to a qmail server for local delivery. There is plugin written by <a href="https://github.com/madeingnecca/haraka-plugins.git">@madeingnecca</a> in github, for directly delivering the mails to user&#8217;s INBOX (mail box should be in <em>MAILDIR</em> format). In the real server&#8217;s we use LDAP backend for storing all the USER databases. So before putting HARAKA into production, i need a to rebuild the auth plugin so that HARAKA can talk to LDAP for user authentication in SMTP.</p>

<p>So first we need to install <strong>NodeJS</strong> and <strong>NPM (Node Package Manager)</strong>. There are several ways for installing NodeJS. We can compile it from the source, or we can use <strong><em>NVM (Node Version Manager)</em></strong>, or we can install the packages from APT in Debian machines. But i prefer source code, because official APT repo has older versions of NodeJS, which will create compatibility issue. Current version is <em>&#8220;v0.10.4&#8221;</em>. Building NodeJS from source is pretty simple.</p>

<p>Just Download the latest source code from <strong>&#8220;http://nodejs.org/download/&#8221;</strong></p>

<pre><code>$ wget http://nodejs.org/dist/v0.10.4/node-v0.10.4.tar.gz

$ tar xvzf node-v0.10.4.tar.gz &amp;&amp; cd node-v0.10.4

$  ./compile 

$ make &amp;&amp; make install
</code></pre>

<p>Once NodeJS is installed, we can go ahead with <code>HARAKA</code>.</p>

<pre><code>$ git clone https://github.com/baudehlo/Haraka.git
</code></pre>

<p>Now go inside to the <em>Haraka</em> folder and run the below command. All the dependency packages are mentioned in the <strong>package.json</strong> file.</p>

<pre><code>$ npm install
</code></pre>

<p>The above command will install all the necessary modules mentioned in the package.json file and will setup HARAKA. Now we can setup a separate service folder for HARAKA.</p>

<pre><code>$ haraka -i /etc/heraka     
</code></pre>

<p> The above  command will create the heraka folder in <strong>/etc/</strong> and it will create  creates config and plugin directories in there, and automatically sets the host name used by Haraka to the output of the hostname command. Now we need to setup up the <em>port number</em> and <em>ip</em> which HARAKA SMTP service should listen. Go to config folder in the newly created haraka service folder and open the <strong>&#8220;smtp.ini&#8221;</strong> file, and mention the port number and ip.</p>

<p>Now before starting the smtp service, first let&#8217;s disable all the plugins, so that we can go in steps. In the config folder, open the <em>&#8220;plugin&#8221;</em> file, and comment out all the plugins, because by default haraka will not create any plugin scripts, so most of them mentioned in that will not work. So we will start using the plugins, once we have copied the corresponding plugin&#8217;s js files to the plugin directory inside our service directory.</p>

<p>Let&#8217;s try running the <code>HARAKA</code> foreground and see if it starts and listens on the port we mentioned.</p>

<pre><code>$ haraka -c /etc/haraka
</code></pre>

<p>Once <code>HARAKA</code> SMTP service starts, we can see the line &#8221;<strong>[NOTICE] [-] [core] Listening on :::25</strong>&#8221; in the STDOUT, which means HARAKA is listening on port 25. We can just Telnet to port 25 and see if we are getting SMTP banner.</p>

<p>Now we can try out a plugin. Heraka has a <em>spamassassin</em> plugin. So will try it out. So first install spamassassin and start the spam filtering.</p>

<pre><code>$ apt-get install spamassassin spamc
</code></pre>

<p>Now from the plugin folder inside the git source folder of HARAKA, copy the <strong>spamassassin.js</strong> and copy it to the plugin folder of our service directory. By default plugin folder is not created inside the service directory, so create it. Now we need to enable the service. Inside the config folder of our service directory, create a config file <strong>&#8220;spamassassin.ini&#8221;</strong>, and inside the file fill in the necessary details like, <strong>&#8220;reject_thresold&#8221;, &#8220;subject_prefix&#8221;, &#8220;spamd_socket&#8221;</strong>. Now before starting the plugin, we need to add it in the plugin, inside the config folder. Once spamassassin plugin is added, we can start the HARAKA smtp service. If the plugin is added properly, then we can see the below lines in the stdout,</p>

<pre><code>[INFO] [-] [core] Loading plugin: spamassassin
[DEBUG] [-] [core] registered hook data_post to spamassassin.hook_data_post
</code></pre>

<p>Now using swaks, we can send a test mail see, if spam assassin is putting scores for the emails. Like this we can enable all other plugins, based on our needs.</p>

<p>Since i&#8217;m going to relay the mails, i need to make HARAKA to accept mails for all my domains. For that i need to define all my domains on HARAKA. In the config folder, open the file <strong>&#8220;host_list&#8221;</strong>, and add all the domains for which HARAKA should accept mails. There also a regular expression option available for, which can be done in <strong>&#8220;host_list_regex&#8221;</strong> file.</p>

<p>Now we need to add, smtp relay, for that edit the <em>&#8220;smtp_forward.ini&#8221;</em> file and mention the relay host ip, port number and auth details(if required). Now we can restart the <code>HARAKA</code> service and we can check SMTP relay by sending test mails using swaks.</p>

<p>I haven&#8217;t tried the Auth plugin yet, but soon i will be trying it. If possible, i will try to use LDAP backend for authentication, so that HARAKA can be used a full fledged SMTP service. More developments are happening in this, hope it wil become a good competitor &#8230;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Monitoring with ZENOSS]]></title>
    <link href="http://beingasysadmin.com/blog/2013/04/11/monitoring-with-zenoss/"/>
    <updated>2013-04-11T17:03:00+00:00</updated>
    <id>http://beingasysadmin.com/blog/2013/04/11/monitoring-with-zenoss</id>
    <content type="html"><![CDATA[<p>It&#8217;s being a year since i have really played with Centos or any Redhat based Distro&#8217;s. I saw a few videos on <a href="http://www.youtube.com/user/zenoss">youtube</a> realting to <strong>zenoss</strong>, which is a new generation monitoring tool. Later i attended two <strong><em>zenoss</em></strong> webinar&#8217;s, which made to try it out in own infrastructure. In this blog i will explain  how to setup zenoss on a Centos6.4 machine. Make sure that you have atleast 2GB of Ram. Initially i put 1GB of Ram and 2GB of swap in my Centos VM. But when i started the zenoss services, the whole and ram and swap was consumed and finaly i was not able to start the services.</p>

<p>Basicaly zenoss need <strong><em>RabbitMQ messaging server, JAVA6, MYSQL</em></strong> as its dependencies. There is an automated script available from the <a href="http://wiki.zenoss.org/Install_Zenoss">zenos website</a>, which will download and install all necessary dependencies. It&#8217;s a bash script. We can download it from the below link.</p>

<pre><code>$  wget --no-check-certificate https://github.com/zenoss/core-autodeploy/tarball/4.2.3 -O auto.tar.gz
</code></pre>

<p>Once we extract the above tar ball, we can see a bunch of files. <code>zenpack_actions.txt</code> file contains the list of zenpacks which is going to be installed. We can modify it based on our needs.</p>

<p>Once done, we can start the installer script.</p>

<pre><code>$ ./core-autodeploy.sh
</code></pre>

<p>This will start by downloading the zenoss rpm file. Once the installation completed, it was giving an error, saying that <em>&#8220;connection reset&#8221;</em> while installing the zenpacks. I was going through all the log files, finally i found that the error was in the rabbitmq. The zenoss user authentication was failing. Below is the error which iwas getting in the rabbitmq log.</p>

<pre><code>=INFO REPORT==== 10-Apr-2013::09:37:00 ===
accepting AMQP connection &lt;0.3533.0&gt; (127.0.0.1:38662 -&gt; 127.0.0.1:5672)

=ERROR REPORT==== 10-Apr-2013::09:37:03 ===
closing AMQP connection &lt;0.3533.0&gt; (127.0.0.1:38662 -&gt; 127.0.0.1:5672):
{channel0_error,starting,
            {amqp_error,access_refused,
                        "PLAIN login refused: user 'zenoss' - invalid credentials",
                        'connection.start_ok'}}
</code></pre>

<p>The error says that the zenoss user credential is wrong. So using <strong><em>&#8220;rabbitmqctl&#8221;</em></strong> command i reset the zenoss user password. Once the password is changed, we have to mention the new passowrd in the zenoss <code>global.conf</code> file. This file will be present in <strong><em>&#8220;/opt/zenoss/etc&#8221;</em></strong> location. Open the the <code>global.conf</code> file, and replace the <strong><em>amqppassword</em></strong> with the new password. By default during installation, the script generates a base64 encoded random password using the openssl. Once we hab=ve replaced the password, we can start the zenoss service.</p>

<pre><code>$ service zenoss start
</code></pre>

<p>Now while starting the service, zenoss will continue the installing the zenpacks. Once the service is started, we can access the WebGUI from <strong><em>&#8220;http://server_ip:8080&#8221;</em></strong> url. Initially it will ask us to set the password for the admin user as well as to create a secondary user. More over  it will ask us to add hosts to monitor, we can skip this step and move the dashboard. Later on we can add the hosts directly from the infrastructure tab. I&#8217;ve added my vps as well as few of my local server&#8217;s with snmp, so far it is working perfectly. There many cool stuffs inside zenoss, hope this will a cool playground &#8230;</p>
]]></content>
  </entry>
  
</feed>
