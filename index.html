

<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Welcome to My Nerd World</title>
  <meta name="author" content="Deepak M Das">
  <link rel="author" href="humans.txt">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  
    
  
  <meta name="description" content=" ">
  
  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://beingasysadmin.com/">
  <link href="/favicon.png" rel="icon">
  <link href='http://fonts.googleapis.com/css?family=Cantarell' rel='stylesheet' type='text/css'>
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Welcome to My Nerd World" type="application/atom+xml">
  <meta name="og:type" content="website" />
  <meta name="og:site_name" content="Welcome to My Nerd World" />
  <meta name="og:title" content="Welcome to My Nerd World" />
  <meta name="og:description" content=" " />
  <meta name="og:url" content="http://beingasysadmin.com/index.html"/>
  <meta name="url" content="http://beingasysadmin.com/index.html">
  
  <meta name="distribution" content="global">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <div id="front-wrapper">
  <div id="hero">
    <div id="hero-inner" class="container">
      <div class="span10 offset1">
  <h1>
    I&#8217;m <em>Deepak</em>,<br/>
    a <em>Random Sys Admin</em><br/>
    by <em>Trade</em>
  </h1>
</div>

    </div>
  </div>
  <section id="sub-hero">
    <div class="container">
      <div class="row">
  <div class="span4">
    <h2>about me</h2>
    <p>A Random Sys Admin by Trade, Hacker by Choice, Loves Linux, Puppet, Ruby, Monitoring, and a lot.</p>
  </div>
  <div class="span6">
    <h2>open source projects</h2>
    <dl class="dl-horizontal">
	    <dt><a href="https://github.com/deepakmdass88/">TweetGrabber</a><a href="https://github.com/deepakmdass88/" rel="tooltip" title="open sourced at Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a></dt>
	    <dd>A Live Tweet Grabber based built on Ruby+REDIS+SINATRA</dd>
      <dt><a href="https://github.com/deepakmdass88/ruby-virtmgr.git">Ruby-Virtmgr   </a><a href="https://github.com/deepakmdass88/ruby-virtmgr.git" rel="tooltip" title="open sourced at Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a></dt>
      <dd>A simple CLI Ruby app for Managing KVM based VM&#8217;s</dd>
    </dl>
  </div>
  <div class="span2">
    <h2>found on</h2>
    <a href="https://github.com/deepakmdass88/" rel="tooltip" title="Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a>
    <a href="http://www.linkedin.com/pub/deepak-dass/44/54/602" rel="tooltip" title="Linkedin"><img class="social_icon" title="Linkedin" alt="Linkedin icon" src="/images/glyphicons_377_linked_in.png"></a>
    <a href="http://twitter.com/deepakmdass88" rel="tooltip" title="Twitter"><img class="social_icon" title="Twitter" alt="Twitter icon" src="/images/glyphicons_391_twitter_t.png"></a>
    <a href="https://plus.google.com/105770729176086017609/posts" rel="tooltip" title="Google Plus"><img class="social_icon" title="Google Plus" alt="Google Plus icon" src="/images/glyphicons_386_google_plus.png"></a>
    <a href="http://www.quora.com/Deepak-M-Dass" rel="tooltip" title="Quora"><img class="social_icon" title="Quora" alt="Quora icon" src="/images/glyphicons_385_quora.png"></a>
    <h2>contact at</h2>
    <a href="mailto:deepakmdass88@gmail.com">deepakmdass88@gmail.com</a>
  </div>
</div>

    </div>
  </section>
  <div class="container">
    <div class="row">
    
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/06/setting-up-docker-private-registry/">Setting Up Docker Private Registry</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2015-01-06T20:13:00+00:00" pubdate data-updated="true">Jan 6<span>th</span>, 2015</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/docker/'>Docker</a>, <a class='category' href='/blog/categories/docker-registry/'>docker-registry</a>, <a class='category' href='/blog/categories/private-registry/'>private-registry</a>, <a class='category' href='/blog/categories/registry/'>registry</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>Last year Containers based technology showed up big boom. A lot of OpenSource projects and startups wrapped over Docker. Now Docker became a favourite tool for both Dev and Ops guys. I&#8217;m a big fan of Docker and i do all my hacks on containers. This time i decided to play with Docker private registry, so that i sync all my docker clients with a central registry. In this test setup i&#8217;m using Ubuntu 12.04 server with Nginx as a reverse proxy. With the Nginx proxy i can easily enforce basic auth and can protect my private docker registry from unauthorized access.</p>

<h4>Installing Docker Registry</h4>

<p>   Download the latest release of Docker Registry from the Docker&#8217;s <a href="https://github.com/docker/docker-registry/releases">github</a> repo</p>

<pre><code>$ wget https://github.com/docker/docker-registry/archive/0.9.0.tar.gz -O /usr/local/src/0.9.0.tar.gz

$ tar xvzf 0.9.0.tar.gz &amp;&amp; mv docker-registry-0.9.0 docker-registry


Let's install the dependencies,

$ apt-get update &amp;&amp; apt-get install swig python-pip python-dev libssl-dev liblzma-dev libevent1-dev patch


Once the dependencies are installed, lets go ahead and install the docker-registry app

$ cat /usr/local/src/docker-registry/config/boto.cfg &gt; /etc/boto.cfg

$ pip install /usr/local/src/docker-registry/depends/docker-registry-core/

$ pip install file:///usr/local/src/docker-registry

$ patch $(python -c 'import boto; import os; print os.path.dirname(boto.__file__)')/connection.py &lt; /usr/local/src/docker-registry/contrib/boto_header_patch.diff

$ cp /usr/local/src/docker-registry/config/config_sample.yml /usr/local/src/docker-registry/config/config.yml 


We can edit the `config.yml` file, if we want to change the local storage path (default =&gt; /tmp/registry) and also we can use redis as a local cache + sqlite based search backend. The repo already contains a sample init [init](https://raw.githubusercontent.com/docker/docker-registry/master/contrib/docker-registry_debian.sh) script that can be used directly.

$ cp -rvf /usr/local/src/docker-registry/contrib/docker-registry_debian.sh /etc/init.d/docker-registry

Also let's setup a default file, `/etc/default/docker-registry`, so that init script can read the necessary env variables. Below is the content of my default file.

DOCKER_REGISTRY_HOME=/usr/local/src/docker-registry
DOCKER_REGISTRY_CONFIG=/usr/local/src/docker-registry/config/config.yml
SETTINGS_FLAVOR=dev
GUNICORN_OPTS=[--preload]
LOGLEVEL=debug

Let's start the registry service,

$ /etc/init.d/docker-registry start
</code></pre>

<h4>Setting up Docker Client</h4>

<p>   Now we have a private Docker registry running, Now let&#8217;s setup an Nginx proxy, so that we dont have to expose the registry directly to outside world.</p>

<pre><code>$ apt-get install nginx nginx-extras
</code></pre>

<p>   The docker-registry repo also contains basic nginx config that can be used directly.</p>

<pre><code>$ cat /usr/local/src/docker-registry/contrib/nginx/nginx.conf &gt; /etc/nginx/sites-enabled/default

$ cp /usr/local/src/docker-registry/contrib/nginx/docker-registry.conf /etc/nginx/
</code></pre>

<p>   Also create a basic auth file that contains the username password. We can use <code>htpasswd</code> to generate the password. The filename mentioned the nginx config is <code>docker-registry.htpasswd</code></p>

<pre><code>$ echo "dockeradmin:$apr1$.BzsRrxN$fng.12mJL/TJenKjkZSMS0" &gt;&gt; /etc/nginx/docker-registry.htpasswd  # replace the username and password with the one generated by htpasswd
</code></pre>

<p>   Now let&#8217;s generate a self signed SSL certificate that can be used with nginx. There are websites like <a href="https://www.startssl.com/">StartSSL</a> which provides free 1 year SSL certificate.</p>

<pre><code>$ mkdir /opt/certs &amp;&amp; cd /opt/certs

$ openssl genrsa -out devdockerCA.key 2048

$ openssl req -x509 -new -nodes -key devdockerCA.key -days 10000 -out devdockerCA.crt

$ openssl genrsa -out dev-docker-registry.com.key 2048

$ openssl req -new -key dev-docker-registry.com.key -out dev-docker-registry.com.csr

    Country Name (2 letter code) [AU]: US
        State or Province Name (full name) [Some-State]: CA
        Locality Name (eg, city) []: SF
        Organization Name (eg, company) [Internet Widgits Pty Ltd]: Beingasysadmin
        Organizational Unit Name (eg, section) []: tech
        Common Name (e.g. server FQDN or YOUR name) []: docker.example.com
        Email Address []: docker@example.com

        Please enter the following 'extra' attributes
        to be sent with your certificate request
        A challenge password []:                          # leave the password blank
        An optional company name []:

$ openssl x509 -req -in dev-docker-registry.com.csr -CA devdockerCA.crt -CAkey devdockerCA.key -CAcreateserial -out dev-docker-registry.com.crt -days 10000
</code></pre>

<p>   Copy the certificates to the SSL path mentioned the nginx config,</p>

<pre><code>$ cp dev-docker-registry.com.crt /etc/ssl/certs/docker-registry

$ cp dev-docker-registry.com.key /etc/ssl/private/docker-registry
</code></pre>

<p>   Now let&#8217;s restart the <code>nginx</code> process to reflect the changes</p>

<pre><code>$ service nginx restart
</code></pre>

<p>   Now once the nginx is up, we can check the connectivity between docker client and registry server. Since registry is using a self signed certificate, we need to whitelist the CA on the Docker client machine.</p>

<pre><code>$ cd /opt/certs

$ mkdir /usr/local/share/ca-certificates/docker-dev-cert

$ cp devdockerCA.crt /usr/local/share/ca-certificates/docker-dev-cert

$ update-ca-certificates
</code></pre>

<p>   Note: If the CA is not added to trusted list, Docker client wont be able to authenticate against the registry server. Once the CA is added to trusted list, we can test the connectivity between Docker client and Registry server.</p>

<pre><code>$ root@docker:~# docker login https://docker.example.com      # use the nginx basic auth creds here, email can be blank
    Username: dockeradmin
    Password:xxxxxxxx
    Email:                          # This we can leave blank
    Login Succeeded                                         # Successful login message
</code></pre>

<p>   Once the login is succeeded, lets add some base docker images to our Private registry.</p>

<pre><code>$ docker pull ubuntu:12.04                             #  Pulling the latest image of Ubuntu 12.04
    ubuntu:12.04: The image you are pulling has been verified
    ed52aaa56e98: Pull complete
    b875af6dcb23: Pull complete
    41959ee20b93: Pull complete
    f959d044ebdf: Pull complete
    511136ea3c5a: Already exists
    Status: Downloaded newer image for ubuntu:12.04


$ docker images                             # Check the downloaded images
    REPOSITORY                  TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
    ubuntu                      12.04               f959d044ebdf        5 days ago          130.1 MB
</code></pre>

<p>   So, as per the Documentation, we need to create a tag for the image that we are going to push to our private registry. The tag must be of the syntax &#8221;<registry-server-fqdn>/<image-name>:<optional-tag>&#8221;</p>

<pre><code>$ docker tag ubuntu:12.04 docker.example.com/ubuntu:12.04     # We need to create a tag of format "&lt;registry-server-fqdn&gt;/&lt;image-name&gt;:&lt;optional-tag&gt;"


$ docker push docker.example.com/ubuntu:12.04                 # push the image to our new private registry
    The push refers to a repository [docker.example.com/ubuntu] (len: 1)
    Sending image list
    Pushing repository docker.example.com/ubuntu (1 tags)
    Image 511136ea3c5a already pushed, skipping
    ed52aaa56e98: Image successfully pushed
    b875af6dcb23: Image successfully pushed
    41959ee20b93: Image successfully pushed
    f959d044ebdf: Image successfully pushed
    Pushing tag for rev [f959d044ebdf] on {https://docker.example.com/v1/repositories/ubuntu/tags/12.04}
</code></pre>

<p>   Let&#8217;s query the registry API for the pushed image</p>

<pre><code>curl http://localhost:5000/v1/search
    {"num_results": 2, "query": "", "results": [{"description": "", "name": "library/wheezy"}, {"description": "", "name": "library/ubuntu"}]}
</code></pre>

<p>   Currently both the Docker Client and Registry resides on the same machine, we can test push/pull image from a remote machine. The only dependency is we need to add the Self Signed CA to the trusted CA list, otherwise docker client will raise an SSL error while trying to login against the private registry.</p>

<p>   Now let&#8217;s try pulling the images from the private registry.</p>

<pre><code>$ docker pull docker.example.com/ubuntu:12.04
    Pulling repository docker.example.com/ubuntu
    f959d044ebdf: Download complete
    511136ea3c5a: Download complete
    ed52aaa56e98: Download complete
    b875af6dcb23: Download complete
    41959ee20b93: Download complete
    Status: Downloaded newer image for docker.example.com/ubuntu:12.04

$ docker images
    REPOSITORY                  TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
    debian                      wheezy              479215127fa7        5 days ago          84.99 MB
    docker.example.com/wheezy   latest              479215127fa7        5 days ago          84.99 MB
    docker.example.com/ubuntu   12.04               f959d044ebdf        5 days ago          130.1 MB
</code></pre>

<h4>Setting up S3 Backend for Docker Registry</h4>

<p>   Docker registry by default supports S3 backend for storing the images. But if we are using S3, it&#8217;s better to cache the image locally so that we don&#8217;t have to fetch S3 all the time. Redis really comes to the rescue. We can set up Redis Server as an <a href="http://redis.io/topics/lru-cache">LRU Cache</a> and can define the settings in the <code>config.yml</code> of the registry or as an env variable.</p>

<pre><code>$ apt-get install redis-server
</code></pre>

<p>   Once Redis server is installed, we need to define the <code>maxmemory</code> to be allocated for the cache and <code>maxmemory-policy</code> which tells Redis how to clean the old cache when the <code>maxmemory</code> limit is reached. Add below settings to the <code>redis.conf</code> file</p>

<pre><code>maxmemory 2000mb              # i'm allocating 2GB of cache size

maxmemory-policy volatile-lru     # removes the key with an expire set using an LRU algorithm
</code></pre>

<p>   Now let&#8217;s define the env variables so that docker-registry can use them while starting up. Add the below variables to the <code>/etc/default/docker-registry</code> file.</p>

<pre><code>CACHE_REDIS_HOST=localhost
CACHE_REDIS_PORT=6379
CACHE_REDIS_DB=0

CACHE_LRU_REDIS_HOST=localhost
CACHE_LRU_REDIS_PORT=6379
CACHE_LRU_REDIS_DB=0
</code></pre>

<p>   Let&#8217;s start the Docker Registry in foreground and see if it&#8217;s starting with Redis Cache.</p>

<pre><code>root@docker:~# docker-registry

   13/Jan/2015:19:07:42 +0000 INFO: Enabling storage cache on Redis
   13/Jan/2015:19:07:42 +0000 INFO: Redis host: localhost:6379 (db0)
   13/Jan/2015:19:07:42 +0000 INFO: Enabling lru cache on Redis
       13/Jan/2015:19:07:42 +0000 INFO: Redis lru host: localhost:6379 (db0)
       13/Jan/2015:19:07:42 +0000 INFO: Enabling storage cache on Redis
</code></pre>

<p>   The above logs shows us that registry has started with Redis cache. Now we need to setup the S3 backend storage. By default for <code>dev</code> env, defaul backend is file storage. We need to change it to S3 in the <code>config.yml</code></p>

<pre><code>dev: &amp;dev
    &lt;&lt;: *s3                            #by default this will be local, which is local file storage
    loglevel: _env:LOGLEVEL:debug
    debug: _env:DEBUG:true
    search_backend: _env:SEARCH_BACKEND:sqlalchemy
</code></pre>

<p>   Now if we check the <code>config.yml</code>, in the S3 backend section, the mandatory variables are the ones mentioned below. The boto variables are needed only if we are using any non-Amazon S3-compliant object store.</p>

<pre><code>AWS_REGION   =&gt; S3 region where the bucket is located
AWS_BUCKET   =&gt; S3 bucket name
STORAGE_PATH =&gt; the sub "folder" where image data will be stored
AWS_ENCRYPT  =&gt; if true, the container will be encrypted on the server-side by S3 and will be stored in an encrypted form while at rest in S3. Default value is `True`
AWS_SECURE   =&gt; true for HTTPS to S3
AWS_KEY      =&gt; S3 Access key
AWS_SECRET   =&gt; S3 secret key
</code></pre>

<p>   We can define the above variables in the <code>/etc/default/docker-registry</code> file. And we need to restart the registry process to make the changes effective.</p>

<pre><code>$ docker-registry         

    13/Jan/2015:23:40:39 +0000 INFO: Enabling storage cache on Redis
    13/Jan/2015:23:40:39 +0000 INFO: Redis host: localhost:6379 (db0)
    13/Jan/2015:23:40:39 +0000 INFO: Enabling lru cache on Redis
    13/Jan/2015:23:40:39 +0000 INFO: Redis lru host: localhost:6379 (db0)
    13/Jan/2015:23:40:39 +0000 INFO: Enabling storage cache on Redis
    13/Jan/2015:23:40:39 +0000 INFO: Redis config: {'path': '/registry1', 'host': 'localhost', 'password': None, 'db': 0, 'port': 6379}
    13/Jan/2015:23:40:39 +0000 DEBUG: Will return docker-registry.drivers.s3.Storage
    13/Jan/2015:23:40:39 +0000 DEBUG: Using access key provided by client.
    13/Jan/2015:23:40:39 +0000 DEBUG: Using secret key provided by client.
    13/Jan/2015:23:40:39 +0000 DEBUG: path=/
    13/Jan/2015:23:40:39 +0000 DEBUG: auth_path=/my-docker/
    13/Jan/2015:23:40:39 +0000 DEBUG: Method: HEAD
    13/Jan/2015:23:40:39 +0000 DEBUG: Path: /
    13/Jan/2015:23:40:39 +0000 DEBUG: Data:
    13/Jan/2015:23:40:39 +0000 DEBUG: Headers: {}
    13/Jan/2015:23:40:39 +0000 DEBUG: Host: my-docker.s3-us-west-2.amazonaws.com
    13/Jan/2015:23:40:39 +0000 DEBUG: Port: 443
    13/Jan/2015:23:40:39 +0000 DEBUG: Params: {}
    13/Jan/2015:23:40:39 +0000 DEBUG: establishing HTTPS connection: host=my-docker.s3-us-west-2.amazonaws.com, kwargs={'port': 443, 'timeout': 70}
    13/Jan/2015:23:40:39 +0000 DEBUG: Token: None
    13/Jan/2015:23:40:39 +0000 DEBUG: StringToSign:
    HEAD


    Tue, 13 Jan 2015 23:40:39 GMT
    /my-docker/
    13/Jan/2015:23:40:39 +0000 DEBUG: Signature:
    AWS XXXXXXXXXXXXXXXXXXXX:********************
    13/Jan/2015:23:40:39 +0000 DEBUG: Final headers: {'Date': 'Tue, 13 Jan 2015 23:40:39 GMT', 'Content-Length': '0', 'Authorization': u'AWS XXXXXXXXXXXXXXXXXXXX:********************', 'User-Agent': 'Boto/2.34.0 Python/2.7.3 Linux/3.8.0-44-generic'}
    13/Jan/2015:23:40:39 +0000 DEBUG: Response headers: [('x-amz-id-2', '*************************************************'), ('server', 'AmazonS3'), ('transfer-encoding', 'chunked'), ('x-amz-request-id', 'XXXXXXXXXXXXXX'), ('date', 'Tue, 13 Jan 2015 23:40:40 GMT'), ('content-type', 'application/xml')]
    13/Jan/2015:23:40:39 +0000 INFO: Boto based storage initialized
    2015-01-13 23:40:39 [21909] [INFO] Starting gunicorn 19.1.0
    2015-01-13 23:40:39 [21909] [INFO] Listening at: http://0.0.0.0:5000 (21909)
    2015-01-13 23:40:39 [21909] [INFO] Using worker: gevent
    2015-01-13 23:40:39 [21919] [INFO] Booting worker with pid: 21919
    2015-01-13 23:40:39 [21920] [INFO] Booting worker with pid: 21920
    2015-01-13 23:40:39 [21921] [INFO] Booting worker with pid: 21921
    2015-01-13 23:40:39 [21922] [INFO] Booting worker with pid: 21922
    2015-01-13 23:40:39 [21909] [INFO] 4 workers
</code></pre>

<p>   So now we have the Docker registry with S3 backend and Redis cache. Let&#8217;s push one of our local image see if registry can upload it to the S3 bucket.</p>

<pre><code>$ docker push docker.example.com/mydocker/debian:wheezy

    The push refers to a repository [docker.example.com/mydocker/debian] (len: 1)
    Sending image list
    Pushing repository docker.example.com/mydocker/debian (1 tags)
    511136ea3c5a: Image successfully pushed
    1aeada447715: Image successfully pushed
    479215127fa7: Image successfully pushed
    3192d5ea7137: Image successfully pushed
    Pushing tag for rev [3192d5ea7137] on {https://docker.example.com/v1/repositories/mydocker/debian/tags/wheezy}
</code></pre>

<p>   Now let&#8217;s check debug logs to have a more glimpse on what&#8217;s happening in the background.</p>

<pre><code># Sample output of S3 image upload

    Tue, 13 Jan 2015 20:05:05 GMT
    /my-docker/registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/json
    13/Jan/2015:20:05:05 +0000 DEBUG: Signature:
    AWS XXXXXXXXXXXXXXXXXXXX:********************
    13/Jan/2015:20:05:05 +0000 DEBUG: Final headers: {'Date': 'Tue, 13 Jan 2015 20:05:05 GMT', 'Content-Length': '0', 'Authorization': u'AWS   XXXXXXXXXXXXXXXXXXXX:********************', 'User-Agent': 'Boto/2.34.0 Python/2.7.3 Linux/3.8.0-44-generic'}
    13/Jan/2015:20:05:05 +0000 DEBUG: Response headers: [('x-amz-id-2', '*****************************************************'), ('server', 'AmazonS3'), ('transfer-encoding', 'chunked'), ('x-amz-request-id', 'XXXXXXXXXXXXX'), ('date', 'Tue, 13 Jan 2015 20:05:05 GMT'), ('content-type', 'application/xml')]
    13/Jan/2015:20:05:05 +0000 DEBUG: path=/registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/_inprogress
    13/Jan/2015:20:05:05 +0000 DEBUG: auth_path=/my-docker/registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/_inprogress
    13/Jan/2015:20:05:05 +0000 DEBUG: Method: PUT
    13/Jan/2015:20:05:05 +0000 DEBUG: Path: /registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/_inprogress
    13/Jan/2015:20:05:05 +0000 DEBUG: Data:
    13/Jan/2015:20:05:05 +0000 DEBUG: Headers: {'Content-MD5': u'xxxxxxxxxxxxxxx', 'Content-Length': '4', 'Expect': '100-Continue', 'x-amz-server-side-encryption': 'AES256', 'Content-Type': 'application/octet-stream', 'User-Agent': 'Boto/2.34.0 Python/2.7.3 Linux/3.8.0-44-generic'}
    13/Jan/2015:20:05:05 +0000 DEBUG: Host: my-docker.s3-us-west-2.amazonaws.com
    13/Jan/2015:20:05:05 +0000 DEBUG: Port: 443
    13/Jan/2015:20:05:05 +0000 DEBUG: Params: {}
    13/Jan/2015:20:05:05 +0000 DEBUG: Token: None
    13/Jan/2015:20:05:05 +0000 DEBUG: StringToSign:
    PUT
    xxxxxxxxxxxxxxx
    application/octet-stream
    Tue, 13 Jan 2015 20:05:05 GMT
    x-amz-server-side-encryption:AES256
    /my-docker/registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/_inprogress
    13/Jan/2015:20:05:05 +0000 DEBUG: Signature:
    AWS XXXXXXXXXXXXXXXXXXXX:********************
    13/Jan/2015:20:05:05 +0000 DEBUG: Final headers: {'Content-MD5': 'xxxxxxxxxxxxx', 'Content-Length': '4', 'Expect': '100-Continue', 'Date': 'Tue,              13 Jan 2015 20:05:05 GMT', 'x-amz-server-side-encryption': 'AES256', 'Content-Type': 'application/octet-stream', 'Authorization': u'AWS XXXXXXXXXXXXXXXXXXXX:********************', 'User-Agent': 'Boto/2.34.0 Python/2.7.3 Linux/3.8.0-44-generic'}
    13/Jan/2015:20:05:05 +0000 DEBUG: Response headers: [('content-length', '0'), ('x-amz-id-2', '*************************************'), ('server', 'AmazonS3'), ('x-amz-request-id', 'xxxxxxxxxxxxxx'), ('etag', '"b326b5062b2f0e69046810717534cb09"'), ('date', 'Tue, 13 Jan 2015 20:05:06 GMT'), ('x-amz-server-side-encryption', 'AES256')]
    13/Jan/2015:20:05:05 +0000 DEBUG: path=/registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/_checksum
    13/Jan/2015:20:05:05 +0000 DEBUG: auth_path=/my-docker/registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/_checksum
    13/Jan/2015:20:05:05 +0000 DEBUG: Method: HEAD
    13/Jan/2015:20:05:05 +0000 DEBUG: Path: /registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/_checksum
    13/Jan/2015:20:05:05 +0000 DEBUG: Data:
    13/Jan/2015:20:05:05 +0000 DEBUG: Headers: {}
    13/Jan/2015:20:05:05 +0000 DEBUG: Host: my-docker.s3-us-west-2.amazonaws.com
    13/Jan/2015:20:05:05 +0000 DEBUG: Port: 443
    13/Jan/2015:20:05:05 +0000 DEBUG: Params: {}
    13/Jan/2015:20:05:05 +0000 DEBUG: Token: None
    13/Jan/2015:20:05:05 +0000 DEBUG: StringToSign:
    HEAD

    Tue, 13 Jan 2015 20:05:05 GMT
    /my-docker/registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/_checksum
    13/Jan/2015:20:05:05 +0000 DEBUG: Signature:
    AWS XXXXXXXXXXXXXXXXXXXX:********************
    13/Jan/2015:20:05:05 +0000 DEBUG: Final headers: {'Date': 'Tue, 13 Jan 2015 20:05:05 GMT', 'Content-Length': '0', 'Authorization': u'AWS   XXXXXXXXXXXXXXXXXXXX:********************', 'User-Agent': 'Boto/2.34.0 Python/2.7.3 Linux/3.8.0-44-generic'}
    13/Jan/2015:20:05:05 +0000 DEBUG: Response headers: [('x-amz-id-2', '***************************************************'), ('server', 'AmazonS3'), ('transfer-encoding', 'chunked'), ('x-amz-request-id', 'xxxxxxxxxxxxxxx'), ('date', 'Tue, 13 Jan 2015 20:05:05 GMT'), ('content-type', 'application/xml')]
    13/Jan/2015:20:05:05 +0000 DEBUG: path=/
    13/Jan/2015:20:05:05 +0000 DEBUG: auth_path=/my-docker/
    13/Jan/2015:20:05:05 +0000 DEBUG: path=/?delimiter=/&amp;prefix=registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/_checksum/
    13/Jan/2015:20:05:05 +0000 DEBUG: auth_path=/my-docker/? delimiter=/&amp;prefix=registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/_checksum/
    13/Jan/2015:20:05:05 +0000 DEBUG: Method: GET
    13/Jan/2015:20:05:05 +0000 DEBUG: Path: /?delimiter=/&amp;prefix=registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/_checksum/
    13/Jan/2015:20:05:05 +0000 DEBUG: Data:
    13/Jan/2015:20:05:05 +0000 DEBUG: Headers: {}
    13/Jan/2015:20:05:05 +0000 DEBUG: Host: my-docker.s3-us-west-2.amazonaws.com
    13/Jan/2015:20:05:05 +0000 DEBUG: Port: 443
    13/Jan/2015:20:05:05 +0000 DEBUG: Params: {}
    13/Jan/2015:20:05:05 +0000 DEBUG: Token: None
    13/Jan/2015:20:05:05 +0000 DEBUG: StringToSign:
    13/Jan/2015:20:05:07 +0000 DEBUG: args = {'image_id': u'3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63'}
    13/Jan/2015:20:05:07 +0000 DEBUG: path=/registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/layer
    13/Jan/2015:20:05:07 +0000 DEBUG: auth_path=/my-docker/registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/layer
    13/Jan/2015:20:05:07 +0000 DEBUG: Method: HEAD
    13/Jan/2015:20:05:07 +0000 DEBUG: Path: /registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/layer
    13/Jan/2015:20:05:07 +0000 DEBUG: Data:
    13/Jan/2015:20:05:07 +0000 DEBUG: Headers: {}
    13/Jan/2015:20:05:07 +0000 DEBUG: Host: my-docker.s3-us-west-2.amazonaws.com
    13/Jan/2015:20:05:07 +0000 DEBUG: Port: 443
    13/Jan/2015:20:05:07 +0000 DEBUG: Params: {}
    13/Jan/2015:20:05:07 +0000 DEBUG: Token: None
    13/Jan/2015:20:05:07 +0000 DEBUG: StringToSign:
    HEAD

    Tue, 13 Jan 2015 20:05:07 GMT
    /my-docker/registry1/images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/layer
    13/Jan/2015:20:05:07 +0000 DEBUG: Signature:
    AWS XXXXXXXXXXXXXXXXXXXX:********************
    13/Jan/2015:20:05:07 +0000 DEBUG: Final headers: {'Date': 'Tue, 13 Jan 2015 20:05:07 GMT', 'Content-Length': '0', 'Authorization': u'AWS   XXXXXXXXXXXXXXXXXXXX:********************', 'User-Agent': 'Boto/2.34.0 Python/2.7.3 Linux/3.8.0-44-generic'}   
    13/Jan/2015:20:05:07 +0000 DEBUG: Token: None
    13/Jan/2015:20:05:07 +0000 DEBUG: StringToSign:
    HEAD
</code></pre>

<p>   Now let&#8217;s query the registry for the newly uploaded image</p>

<pre><code>$ curl -k https://dockeradmin:xxxxxxx@docker.example.com/v1/repositories/mydocker/debian/tags/wheezy

    "3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63"

$ curl http://localhost:5000/v1/search
    {"num_results": 1, "query": "", "results": [{"description": "", "name": "mydocker/debian"}]}
</code></pre>

<p>   Also let&#8217;s see if our Redis server is caching the image.</p>

<pre><code>$ redis 127.0.0.1:6379&gt; keys *
 1) "cache_path:/tmp/registryrepositories/mydocker/debian/tag_wheezy"
 2) "cache_path:/registry1images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/ancestry"
 3) "cache_path:/registry1images/1aeada4477158496dc31ee5c6e7174240140d83fddf94bc57fc02bee1b04e44f/json"
 4) "cache_path:/registryimages/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/ancestry"
 5) "diff-worker"
 6) "cache_path:/registry1images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json"
 7) "cache_path:/registry1repositories/mydocker/debian/_index_images"
 8) "cache_path:/registryimages/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/_inprogress"
 9) "cache_path:/registry1images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/_checksum"
10) "cache_path:/registry1repositories/mydocker/debian/tagwheezy_json"
11) "cache_path:/registryimages/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/_checksum"
12) "cache_path:/registry1images/479215127fa7b852902ed734f3a7ac69177c0d4d9446ad3a1648938230c3c8ab/ancestry"
13) "cache_path:/registry1repositories/mydocker/debian/tag_wheezy"
14) "cache_path:/registry1images/1aeada4477158496dc31ee5c6e7174240140d83fddf94bc57fc02bee1b04e44f/ancestry"
15) "cache_path:/registry1images/3192d5ea7137e4f47f4624a5cc7786af2159a44f49511aeed28aa672416cec63/json"
16) "cache_path:/registryrepositories/mydocker/debian/_index_images"
17) "cache_path:/registryimages/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json"
18) "cache_path:/registry1images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/ancestry"
19) "cache_path:/registry1images/479215127fa7b852902ed734f3a7ac69177c0d4d9446ad3a1648938230c3c8ab/_checksum"
20) "cache_path:/registry1images/1aeada4477158496dc31ee5c6e7174240140d83fddf94bc57fc02bee1b04e44f/_checksum"
21) "cache_path:/registry1images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/_checksum"
22) "cache_path:/registry1images/479215127fa7b852902ed734f3a7ac69177c0d4d9446ad3a1648938230c3c8ab/json"
</code></pre>

<p>   Now for those who want to have a Continous Integration system, we can set up Jenkins to build the autmated images and upload to our Private registry and use Mesos/CoreOS to deploy the image through out our infrastructure in a fully automated fashion.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/12/07/automating-debian-package-management/">Automating Debian Package Management</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-12-07T08:00:00+00:00" pubdate data-updated="true">Dec 7<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/debian/'>Debian</a>, <a class='category' href='/blog/categories/apt/'>apt</a>, <a class='category' href='/blog/categories/aptly/'>aptly</a>, <a class='category' href='/blog/categories/debian-packaging/'>debian-packaging</a>, <a class='category' href='/blog/categories/jenkins/'>jenkins</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>With the rise of CI tools like Jenkins/Gitlab and the rise of Config management tools, Continous integration became so flexible. Now if we check, most of the teams are using Git for Version control for their projects either via Github or Gitlab etc&#8230; and uses CI tools like Jenkins/Travis etc  to build and test the packages automatically whenever any change is pushed to the repo. An d finally once hte build is a success the packages are pushed to repo so that config management systems like Salt/Puppet/Ansible can go ahead and perform the upgrade. In my previos blogs, i&#8217;ve explained on how to build a Debian package and how to create and manage APT repo&#8217;s via aptly. In this i&#8217;m going to automate the entire process.</p>

<p>So the flow is like, We have a github repo, and once a changed is pushed to the repo, github will send a hook to our Jenkins server which inturn triggers the Jenkins package build. Once the package has been succesfuly built, jenkins will automatically add the new packages to our repo and publish the same to our APT repo via <code>aptly</code></p>

<h4>Installing Jenkins</h4>

<p>First let&#8217;s setup a Jenkins build server.</p>

<pre><code>$ wget -q -O - https://jenkins-ci.org/debian/jenkins-ci.org.key | sudo apt-key add -

$ echo "deb http://pkg.jenkins-ci.org/debian binary/" &gt; /etc/apt/sources.list.d/jenkins.list

$ apt-get update &amp;&amp; apt-get install jenkins

$ /etc/init.d/jenkins restart
</code></pre>

<p>Once this Jenkins service is started we can access it via &#8221;<em>http://jenkins-server-ip:8080</em>&#8221;. By default there is no authentication, so accessing the URL will open up the Jenkins UI.</p>

<h4>Creating a Build Job in Jenkins</h4>

<p>In order to use a Git repo, we have to instsall the Git plugin first. Go to &#8221;<em>Manage Jenkins</em>&#8221; - > &#8221;<em>Manage Plugins</em>&#8221; - > &#8221;<em>Available</em>&#8221; and search for &#8221;<em>GIT plugin</em>&#8221; and install it. Once the Git plugin has been installed we can create a new build job.</p>

<p>Click on &#8221;<em>New Item</em>&#8221; on the Home Page and Select &#8221;<em>Freestyle Project</em>&#8221; and Click on &#8220;OK&#8221;. On the Next page, we need to configure all the necessary steps for build job. Fill in the necessary details like Project Name, Description etc. Under &#8220;Source Code Management&#8221;, select <code>Git</code> and enter the Repo URL. Make sure that the jenkins user has access to the repo. We can also use Deploy keys, but i&#8217;ve generated a separate ssh key for Jenkins user and the same has been added to Github. Under &#8221;<em>Build Triggers</em>&#8221; select &#8216;Build when a change is pushed to GitHub&#8217; so that Jenkins will start the build job everytime when a change has been pushed to repo.</p>

<p><img src="/images/aptly1.png"></p>

<p><img src="/images/aptly2.png"></p>

<p>Under the <code>Build</code> section, Click on &#8221;<em>Add build step</em>&#8221; and select &#8217;<em>Execute shell</em>&#8217; and let&#8217;s add our package build script which is stage 1.</p>

<pre><code>set -e
set -x
set -u
debuild -us -uc
</code></pre>

<p>Stage 2 is adding the newly build pacakge to our apt repo and publish the udpate to our repo so that our APT will server the new package</p>

<pre><code>aptly repo add myapt ../openvpn*.deb
/usr/bin/env script -qfc "aptly publish -passphrase=&lt;GPG passphrase&gt; update myapt"
</code></pre>

<p><img src="/images/aptly3.png"></p>

<p>If you see my above command, i&#8217;ve used the <code>script</code> command. This is because, i was getting the error <code>aptly stderr: gpg: cannot open tty /dev/tty': No such device or address</code> whenever i try to update a repo. This is <a href="https://github.com/smira/aptly/issues/144">bug</a> in aptly. The fix has been placed on the Master branch but its not yet released. The <code>script</code> command is a temporary work around for this bug.</p>

<p>Now we have a Build job ready. We can manually trigger a build to test if the Job is working fine. If the build is successfull, we are done with our build server. Now the final step is Configuring Github to send a trigger whenever any change is pushed to Github.</p>

<h4>Configuring Github Triggers</h4>

<p>Go the Github repo and Click on the Repo settings. Open &#8221;<em>Webhooks and Services</em>&#8221; and select &#8221;<em>Add Service</em>&#8221; and select &#8221;<em>GitHub plugin</em>&#8220;.Now it will ask for Jenkin&#8217;s Hook URL, which is &#8221;<em>http://<Name of Jenkins server>:8080/github-webhook/</em>&#8221; and add the service. Once the service is set, we can click on &#8220;Test service&#8221; to check if the webhook is working fine.</p>

<p><img src="/images/aptly4.png"></p>

<p>Once the test hook is created, go to the Jenkins job page and select &#8221;<em>GitHub Hook Log</em>&#8221;. The test hook should get displayed there. If not there is something wrong on the config.</p>

<p><img src="/images/aptly5.png"></p>

<p>Now we have a fully automated build and release management. Config management tools like Salt/Ansible etc.. can go ahead and start the deployment process.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/12/07/managing-debian-apt-repository-via-aptly/">Managing Debian APT Repository via Aptly</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-12-07T07:04:00+00:00" pubdate data-updated="true">Dec 7<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/apt-repo/'>apt-repo</a>, <a class='category' href='/blog/categories/aptly/'>aptly</a>, <a class='category' href='/blog/categories/debian/'>debian</a>, <a class='category' href='/blog/categories/pacakging/'>pacakging</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>In my previous <a href="https://beingasysadmin.wordpress.com/2014/12/03/building-a-debian-package/">blog</a>, i&#8217;ve explained how to build a Debian pacakge from source. In this blog i&#8217;m to explain how to create and manage our own apt repository. Enter <a href="aptly.info">aptly</a>,is a swiss army knife for Debian repository management: it allows us to mirror remote repositories, manage local package repositories, take snapshots, pull new versions of packages along with dependencies, publish as Debian repository. Aptly can upload the repo to Amazon S3, but we need to install APT S3 <a href="https://github.com/castlabs/apt-s3">support</a>, in order to use it from S3.</p>

<p>First, let&#8217;s install aptly on our build server. A more detailed documentation on installation is available in the <a href="http://www.aptly.info/download/">website</a></p>

<pre><code>$ echo "deb http://repo.aptly.info/ squeeze main" &gt; /etc/apt/sources.list

$ gpg --keyserver keys.gnupg.net --recv-keys 2A194991

$ gpg -a --export 2A194991 | sudo apt-key add -

$ apt-get update &amp;&amp; apt-get install aptly
</code></pre>

<p>Let&#8217;s create a repo,</p>

<pre><code>$ aptly repo create -distribution=wheezy -component=main my-repo    # where my-repo is the name of the repository
</code></pre>

<p>Once the repo is created, we can start adding our newly created packages to our new repo.</p>

<pre><code>$ aptly repo add &lt;repo name&gt; &lt;your debian file&gt;    # in my case aptly repo add myrepo openvpn_2.3.6_amd64.deb
</code></pre>

<p>The above command will add the new package to the repo. Now in order to make this repo usable, we need to publish this repo. A valid GPG key is required for publishing the repo. So let&#8217;s create the gpg key for aptly.</p>

<pre><code>$ gpg --gen-key

$ gpg --export --armor &lt;email-id-used-fo-gpg-key-creation&gt; &gt; myrepo-pubkey.asc   # creates a pubkey that distributed

$ gpg --send-key KEYNAME     # This command can be used if we want to send the key to a public server, we can also pass --keyserver &lt;server-url&gt;, if we want to specifiy a specific keyserver
</code></pre>

<p>Once we have our GPG key, we can publish our repo. By default aptly can publish the repo to S3 or it can publish it locally and we can use any webserver to servce this repo.</p>

<pre><code>$ aptly publish --distribution="wheezy" repo my-repo
</code></pre>

<p>Once published, we can point the webserver to &#8220;~/.aptly/&#8221;, where our repo files will be created. Aptly also comes with an embedded webserver which can be invoked by running <code>aptly serve</code>. Aptly really makes the repo management so easy. We can actually integrate this into our jenkins job so that each time when we build a package, we can directly add and upload the same to our repository.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/12/03/building-debian-packages/">Building Debian Packages</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-12-03T22:32:00+00:00" pubdate data-updated="true">Dec 3<span>rd</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/apt/'>apt</a>, <a class='category' href='/blog/categories/debhelper/'>debhelper</a>, <a class='category' href='/blog/categories/debian/'>debian</a>, <a class='category' href='/blog/categories/debuild/'>debuild</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>Installing applications via packages saves us a lot of time. Especially being an OPS oriented guy, compiling applications from source is somtimes pain and time consuming. Especially the dependencies. But later, after the rise of config management system, people started creating corresponding automated scripts that will install necessary dependencies and the ususal <code>make &amp;&amp; make install</code>. But if you check applications like Freeswitch was taking 15+min to finish compiliations, which is defintely a bad idea when you want to deploy the a new patch on a cluster. In such cases packages are really a life saver. Build the packages once as per our requirement and deploy it throughout the infrastructure. Now with the tools like jenkins,TravisCI etc we can attain a good level of CI.</p>

<p>In this blog, i&#8217;m going to explain on how to build a debian package from scratch. First let&#8217;s install two main dependencies for a build machine</p>

<pre><code>$ apt-get install devscripts build-essential
</code></pre>

<p>For the past few days i was playing with <em>OpenVPN</em> and <em>SoftEther</em>. I&#8217;m going to build a simple debian package for OpenVPN from source. The current stable version of OpenVPN is available 2.3.6. First let&#8217;s get the OpenVPN source code.</p>

<pre><code>$ wget http://swupdate.openvpn.org/community/releases/openvpn-2.3.6.tar.gz

$ tar xvzf openvpn-2.3.6.tar.gz &amp;&amp; cd openvpn-2.3.6
</code></pre>

<p>Now for building a package, first we need to create a <code>debian</code> folder. And in this folder we are going to place all necessary files required for building a package.</p>

<pre><code>$ mkdir debian
</code></pre>

<p>As per the Debian pacakging <a href="https://www.debian.org/doc/manuals/maint-guide/dreq.en.html">Documentation</a>, the mandatory files are <a href="https://www.debian.org/doc/manuals/maint-guide/dreq.en.html#rules">rules</a> <a href="https://www.debian.org/doc/manuals/maint-guide/dreq.en.html#control">control</a>, <a href="https://www.debian.org/doc/manuals/maint-guide/dreq.en.html#changelog">changelog</a>. <code>Changlog</code> file content should match the exact syntax, otherwise packaging will fail at the initial stage itself. There are some more optional files that we can <a href="https://www.debian.org/doc/manuals/maint-guide/dother.en.html">use</a>. Below are the files present in my <code>debian</code> folder</p>

<pre><code>changelog           =&gt; Changelog for my Package
control             =&gt; Contains Details about the package including the dependencies
dirs                =&gt; specifies any directories which we need but which are not created by the normal installation procedure, handled by 'dh_installdirs'
openvpn.default     =&gt; this file will be copied to /etc/default/openvpn
openvpn.init        =&gt; this file will be copied to /etc/init.d/openvpn, handled by 'dh_installinit'
postinst.debhelper  =&gt; Any action that need to be performed once the package installation is completed, like creating a specific user, starting service etc
postrm.debhelper        =&gt; Any action that need to be performed once the package removal is completed, like deleting a specific user
prerm.debhelper     =&gt; Any action that need to be performed before the package removal is initiated, like stopping the service
rules           =&gt; Contains rules for build procedure
</code></pre>

<p>In my case i wanted to install the openvpn on a custom location say &#8217;<strong>/opt/openvpn</strong>&#8217;. So if we are building from scratch manually, we can mention the prefix like &#8217;<strong>./configure &#8211;prefix=/opt/openvpn</strong>&#8217;. but in the build process, <code>dh_auto_configure</code> is running our &#8217;<strong>./configure</strong>&#8217; operation with dfault option ie, no custom prefix. So we need to overide this process if we want to have a custom prefix. Below is the content of my <code>rules</code> file.</p>

<pre><code># rules file

    #!/usr/bin/make -f
    # vim: tabstop=4 softtabstop=4 noexpandtab fileencoding=utf-8

    # Uncomment this to turn on verbose mode.
    export DH_VERBOSE=1

    DEB_DIR=$(CURDIR)/debian/openvpn

    %:
        dh $@
    override_dh_auto_configure:                      # override of configure
        ./configure --prefix=/opt/openvpn
</code></pre>

<p>Once we have all the necessary files in place, we can start the build process. Make sure that all the dependency packages mentioned in the <code>control</code> file is installed on the build server.</p>

<pre><code>    $ debuild -us -uc
</code></pre>

<p>If the build command is completed successfully, we will see the deb package as well as the source package just above our openvpn source folderm which is the default path where <code>dh_builddeb</code> places the files. We can overide the same too.</p>

<pre><code>#!/usr/bin/make -f
# vim: tabstop=4 softtabstop=4 noexpandtab fileencoding=utf-8

# Uncomment this to turn on verbose mode.
export DH_VERBOSE=1

DEB_DIR=$(CURDIR)/debian/openvpn

%:
    dh $@
override_dh_auto_configure:
    ./configure --prefix=/opt/openvpn
override_dh_builddeb:
    dh_builddeb --destdir=./deb-pkg/
</code></pre>

<p>So now we have the Debian package. We can test installing it manually via &#8217;<strong>dpkg -i</strong>&#8217;. This was just a go thorugh on how to build a simple debian package. In my next blog, i&#8217;ll be discussing about how to create and manage a private apt repository using a awsme tool called <a href="http://www.aptly.info/">aptly</a></p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/23/sippy-cup-freeswitch-load-test-simplified/">Sippy_cup - FreeSwitch Load Test Simplified</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-10-23T02:35:00+00:00" pubdate data-updated="true">Oct 23<span>rd</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/docker/'>Docker</a>, <a class='category' href='/blog/categories/freeswitch/'>Freeswitch</a>, <a class='category' href='/blog/categories/ruby/'>ruby</a>, <a class='category' href='/blog/categories/sipp/'>sipp</a>, <a class='category' href='/blog/categories/sipp/'>sipp</a>, <a class='category' href='/blog/categories/sippy-cup/'>sippy_cup</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>Ever since the entry of <code>Docker</code>, everyone is busy porting their applications to Docker Containers. Now with the tools like Mesos, CoreOS etc we can easily achieve scalability also. @Plivo we always dedicate ourselves to play around such new technologies. In my previous blog posts, i&#8217;ve explained how to containerize the Freeswitch, how to perform some basic load test using simple dialplans etc. My previous load tests required a bunch of basic Freeswitch servers to originate calls to flood the calls to the FreeSwitch container. So this time i&#8217;m going to use a simple method, which everyone can use even from their laptops.</p>

<p>Enter <code>SIPp</code>. SIPp is a free Open Source test tool / traffic generator for the SIP protocol. But the main issue for beginer like me is in generating a proper XML for SIPp that can match to my exact production scenarios. After googling, i came across a super simple ruby wrapper over SIPp called <a href="https://github.com/mojolingo/sippy_cup">sippy_cup</a>.  SIPpy_cup is a simple ruby wrapper over SIPp. We just need to create a simple yaml file and sippy_cup parses this yml file and generates the XML equivalent which will be then used to generate calls. sippy_cup can also be used to generate only the XML file for SIPp.</p>

<p>Setting up sippy_cup is very simple. There are only two dependencies</p>

<pre><code>      1) ruby (2.1.2 recomended)
      2) SIPp
</code></pre>

<p>Another important dependency is our <em>local internet bandwidth</em>. Flooding too many calls will definitely result in network bottlenecks, which i faced when i generated 1k calls from my laptop. Now let&#8217;s install SIPp.</p>

<pre><code>sudo apt-get install pcaputils libpcap-dev libncurses5-dev

wget 'http://sourceforge.net/projects/sipp/files/sipp/3.2/sipp.svn.tar.gz/download'

tar zxvf sipp.svn.tar.gz

# compile sipp
make

# compile sipp with pcapplay support
make pcapplay
</code></pre>

<p>Once we have installed SIPp and ruby, we can install sippy_cup via ruby gems.</p>

<pre><code>gem install sippy_cup
</code></pre>

<h3>Configuring sippy_cup</h3>

<p>First we need to create yml file for our call flow. There is a good documentation available on the <a href="https://github.com/mojolingo/sippy_cup/blob/develop/README.markdown">Readme</a> on various options that can be used to create the yml to suit to our call flow. My call flow is pretty simple, i&#8217;ve a DialPlan in my Docker FS, which will play an mp3 file. So below is a simple yml config for this call flow</p>

<pre><code>source: &lt;local_machine_ip&gt;
destination: &lt;docker_fs_ip&gt;:&lt;fs_port&gt;
max_concurrent: &lt;no_of_concurrent_calls&gt;
calls_per_second: &lt;calls_per_second&gt;
number_of_calls: &lt;total_no_of_calls&gt;
to_user: &lt;to_number&gt;            # =&gt; should match the FS Dialplan
steps:                  # call flow steps
  - invite              # Initial Call INVITE
  - wait_for_answer         # Waiting for Answer, handles 100, 180/183 and finally 200 OK
  - ack_answer              # ACK for the 200 OK
  - sleep 1000              # Sleeps for 1000 seconds
  - send_bye                # Sends BYE signal to FS
</code></pre>

<p>Now let&#8217;s run sippy_cup using our config yml</p>

<pre><code>sippy_cup -r test.yml
</code></pre>

<p>Below is the output of a sample load test. Total 20 calls with 10 concurrent calls</p>

<pre><code>         INVITE ----------&gt;      20        1         0
         100 &lt;----------         20        0         0         0
         180 &lt;----------         0         0         0         0
         183 &lt;----------         0         0         0         0
         200 &lt;----------  E-RTD1 20        0         0         0
         ACK ----------&gt;         20        0
              [ NOP ]
         Pause [    30.0s]       20                            0
         BYE ----------&gt;         20        0
------------------------------ Test Terminated --------------------------------


----------------------------- Statistics Screen ------- [1-9]: Change Screen --
  Start Time             | 2014-10-22   19:12:40.494470 1414030360.494470
  Last Reset Time        | 2014-10-22   19:13:45.355358 1414030425.355358
  Current Time           | 2014-10-22   19:13:45.355609 1414030425.355609
-------------------------+---------------------------+--------------------------
  Counter Name           | Periodic value            | Cumulative value
-------------------------+---------------------------+--------------------------
  Elapsed Time           | 00:00:00:000000           | 00:01:04:861000
  Call Rate              |    0.000 cps              |    0.308 cps
-------------------------+---------------------------+--------------------------
  Incoming call created  |        0                  |        0
  OutGoing call created  |        0                  |       20
  Total Call created     |                           |       20
  Current Call           |        0                  |
-------------------------+---------------------------+--------------------------
  Successful call        |        0                  |       20
  Failed call            |        0                  |        0
-------------------------+---------------------------+--------------------------
  Response Time 1        | 00:00:00:000000           | 00:00:01:252000
  Call Length            | 00:00:00:000000           | 00:00:31:255000
------------------------------ Test Terminated --------------------------------


I, [2014-10-22T19:13:45.357508 #17234]  INFO -- : Test completed successfully!
</code></pre>

<p>I tried to perform a large scale load test by making 1k calls with 250 concurrent calls. My local internet was flooding with network traffic as there was real Media packets coming from the servers, though it bottlenecked my internet, but still i was able to make 994 successfull calls. I suggest to do such heavy load test on machines wich has good network throughput. Below are the output for this test.</p>

<pre><code>------------------------------ Scenario Screen -------- [1-9]: Change Screen --
  Call-rate(length)   Port   Total-time  Total-calls  Remote-host
   5.0(0 ms)/1.000s   8836     585.61 s         1000  54.235.170.44:5060(UDP)

  Call limit reached (-m 1000), 0.507 s period  1 ms scheduler resolution
  6 calls (limit 250)                    Peak was 176 calls, after 150 s
  0 Running, 8 Paused, 1 Woken up
  604 dead call msg (discarded)          0 out-of-call msg (discarded)
  3 open sockets
  1490603 Total RTP pckts sent           0.000 last period RTP rate (kB/s)

                                 Messages  Retrans   Timeout   Unexpected-Msg
         INVITE ----------&gt;      1000      332       0
         100 &lt;----------         954       53        0         0
         180 &lt;----------         0         0         0         0
         183 &lt;----------         0         0         0         0
         200 &lt;------2014-10-22  19:19:23.202714 1414030763.202714: Dead call 990-17510@192.168.1.146 (successful), 

received 'SIP/2.0 200 OK
Via: SIP/2.0/UDP 192.168.1.146:8836;received=208.66.27.62;branch=z9hG4bK-17510-990-8
From: "sipp" &lt;sip:sipp@192.168.1.146&gt;;tag=990
To: &lt;sip:14158872327@54.235.170.44:5060&gt;;tag=9p6t351mvXZXg
Call-ID: 990-17510@192.168.1.146
CSeq: 2 BYE
User-Agent: Plivo
Allow: INVITE, ACK, BYE, CANCEL, OPTIONS, MESSAGE, INFO, UPDATE, REFER, NOTIFY
Supported: timer, precondition, path, replaces
Conte----  E-RTD1        994       126       0         0
         ACK ----------&gt;         994       126
              [ NOP ]
       Pause [    30.0s]         994                           0
         BYE ----------&gt;         994       0
------------------------------ Test Terminated --------------------------------


----------------------------- Statistics Screen ------- [1-9]: Change Screen --
  Start Time             | 2014-10-22   19:15:29.941276 1414030529.941276
  Last Reset Time        | 2014-10-22   19:25:15.056475 1414031115.056475
  Current Time           | 2014-10-22   19:25:15.564038 1414031115.564038
-------------------------+---------------------------+--------------------------
  Counter Name           | Periodic value            | Cumulative value
-------------------------+---------------------------+--------------------------
  Elapsed Time           | 00:00:00:507000           | 00:09:45:622000
  Call Rate              |    0.000 cps              |    1.708 cps
-------------------------+---------------------------+--------------------------
  Incoming call created  |        0                  |        0
  OutGoing call created  |        0                  |     1000
  Total Call created     |                           |     1000
  Current Call           |        6                  |
-------------------------+---------------------------+--------------------------
  Successful call        |        0                  |      994
  Failed call            |        0                  |        0
-------------------------+---------------------------+--------------------------
  Response Time 1        | 00:00:00:000000           | 00:00:01:670000
  Call Length            | 00:00:00:000000           | 00:00:31:673000
------------------------------ Test Terminated --------------------------------
</code></pre>

<p>Sippy_cup is definitely a good tool for all beginers who finds really hard time to work around with SIPp XML&#8217;s. I&#8217;m really excited to see how Docker is going to contirbute to VOIP world.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/14/monitoring-redis-using-collectd-and-elk/">Monitoring Redis Using CollectD and ELK</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-10-14T23:57:00+00:00" pubdate data-updated="true">Oct 14<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/elk/'>ELK</a>, <a class='category' href='/blog/categories/collectd/'>collectd</a>, <a class='category' href='/blog/categories/collectd-redis/'>collectd-redis</a>, <a class='category' href='/blog/categories/redis/'>redis</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>Redis is an open-source, networked, in-memory, key-value data store. It&#8217;s being heavily used every where from Web stack to Monitoring to Message queues. Monitoring tools like Sensu already has some good scripts to Monitor Redis. Last Month during <a href="http://in.pycon.org/funnel/2014/245-sharq-an-api-queueing-system-built-at-plivo">PyCon 2014</a> <a href="https://twitter.com/plivo">@Plivo</a>, opensourced a new rate limited queue called <a href="http://sharq.io/">SHARQ</a> which is based on Redis. So apart from just Monitoring checks, we decided to have a tsdb of what&#8217;s happening in our Redis Cluster. Since we are heavily using ELK stack to visualize our infrastructure, we decided to go ahead with the same.</p>

<h3>CollectD Redis Plugin</h3>

<p>There is a cool CollectD <a href="https://github.com/powdahound/redis-collectd-plugin">plugin</a> for Redis. It pulls a verity of Data from Redis which includes, Memory used, Commands Processed, No. of Connected Clients and slaves, No. of blocked Clients, No. of Keys stored/db, uptime and challenges since last save. The installation is pretty simple and straight forward.</p>

<pre><code>$ apt-get update &amp;&amp; apt-get install collectd

$ git clone https://github.com/powdahound/redis-collectd-plugin.git /tmp/redis-collectd-plugin
</code></pre>

<p>Now place the <code>redis_info.py</code> file onto the collectd folder and enable the <em>Python</em> Plugins so that collectd can use this python file. Below is our collectd conf</p>

<pre><code>Hostname    "&lt;redis-server-fqdn&gt;"
Interval 10
Timeout 4
Include "/etc/collectd/filters.conf"
Include "/etc/collectd/thresholds.conf"
LoadPlugin network
ReportStats true

        LogLevel info

Include "/etc/collectd/redis.conf"      # This is the configuration for the Redis plugin
&lt;Plugin network&gt;
    Server "&lt;logstash-fqdn&gt;" "&lt;logstash-collectd-port&gt;"
&lt;/Plugin&gt;
</code></pre>

<p>Now copy the redis python plugin and the conf file to collectd folder.</p>

<pre><code>$ mkdir /etc/collectd/plugin            # This is where we are going to place our custom plugins

$ cp /tmp/redis-collectd-plugin/redis_info.py /etc/collectd/plugin/

$ cp /tmp/redis-collectd-plugin/redis.conf /etc/collectd/
</code></pre>

<p>By default, the plugin folder in the <code>redis.conf</code> is defined as <em>&#8216;/opt/collectd/lib/collectd/plugins/python&#8217;</em>. Make sure to replace this with the location where we are copying the plugin file, in our case <strong>&#8220;/etc/collectd/plugin&#8221;</strong>. Now lets restart the collectd daemon to enable the redis plugin.</p>

<pre><code>$ /etc/init.d/collectd stop

$ /etc/init.d/collectd start
</code></pre>

<p>In my previous <a href="http://beingasysadmin.wordpress.com/2014/05/11/extending-elk-stack-to-voip-infrastructure/">Blog</a>, i&#8217;ve mentioned how to enable and use the ColectD input plugin in Logstash and to use Kibana to plot the data coming from the collectd. Below are the Data&#8217;s that we are receiving from the CollectD on Logstash,</p>

<pre><code>  1) type_instance: blocked_clients
  2) type_instance: evicted_keys
  3) type_instance: connected_slaves
  4) type_instance: commands_processed
  5) type_instance: connected_clients
  6) type_instance: used_memory 
  7) type_instance: &lt;dbname&gt;-keys
  8) type_instance: changes_since_last_save
  9) type_instance: uptime_in_seconds
10) type_instance: connections_received
</code></pre>

<p>Now we need to Visualize these via Kibana. Lets create some ElasticSearch queries so that visualize them directly. Below are some sample queries created in Kibana UI.</p>

<pre><code>1) type_instance: "commands_processed" AND host: "&lt;redis-host-fqdn&gt;"
2) type_instance: "used_memory" AND host: "&lt;redis-host-fqdn&gt;"
3) type_instance: "connections_received" AND host: "&lt;redis-host-fqdn&gt;"
4) type_instance: "&lt;dbname&gt;-keys" AND host: "&lt;redis-host-fqdn&gt;"
</code></pre>

<p><img src="/images/Kibana_ES_Query.png"></p>

<p>Now We have some sample queries, lets visualize them.</p>

<p><img src="/images/ES_graph.png"></p>

<p><img src="/images/ES_graph2.png"></p>

<p>Now create histograms in the same procedure by changing the Selected Queries.</p>

<p><img src="/images/ES_graph3.png"></p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/31/mesos-with-native-docker-support/">Mesos With Native Docker Support</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-08-31T17:34:00+00:00" pubdate data-updated="true">Aug 31<span>st</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/docker/'>Docker</a>, <a class='category' href='/blog/categories/marathon/'>Marathon</a>, <a class='category' href='/blog/categories/mesos/'>Mesos</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>In my previous <a href="http://beingasysadmin.wordpress.com/2014/06/27/managing-docker-clusters-using-mesos-and-marathon/">blog</a>, i&#8217;ve explained how to manage Docker cluster using Mesos and Marathon. Now Mesos has released 0.20 with Native Docker support. Till Mesos 0.19, we had to use mesosphere&#8217;s <a href="https://github.com/mesosphere/deimos">Deimos</a> for running Docker containers on Mesos slaves. Now from Mesos 0.20, Mesos has the ability to run Docker containers directly. As per Mesos Documentation we can run containers in two ways. 1) as a Task and 2) as an Executor. Currently Mesos 0.20 supports only the host (&#8211;net=host) Docker networking mode.</p>

<p>Install the latest Mesos 0.20 using the mesosphere&#8217;s Mesos <a href="http://mesosphere.io/downloads/">Debian</a> package. Once we have setup the Zookeeper and Docker, we need to make a few changes to enable the Mesos Native Docker support. We need to start all the Mesos-Slave with <code>--containerizers=docker,mesos</code> flag. For enabling this flag, we need to create a file <code>containerizers</code> in <em>/etc/mesos-slave/containerizers</em> with content &#8220;docker,mesos&#8221;. Mesos slave process, when starting up, will read the folder and enable this flag.</p>

<pre><code>$ echo 'docker,mesos' &gt; /etc/mesos-slave/containerizers

$ service mesos-master restart
</code></pre>

<p>I was trying to start a container with Port <strong>9999</strong> via Marathon REST API. But the task was keep on failing. Up on investigating the logs, i found that the slave when sends the allocatable resource details to mesos master was <strong>total allocatable: cpus(<em>):0.8; mem(</em>):801; disk(<em>):35164; ports(</em>):[31000-31099, 31101-32000]</strong>. So the allowed port range was [31000-31099, 31101-32000]. So if we wnat to use any other custom port, we need to define the same in the <em>/etc/mesos-slave</em> folder by creating a config file.</p>

<pre><code>    $ echo "ports(*):[31000-31099, 31101-32000, 9998-9999]" &gt; "/etc/mesos-slave/resources"
</code></pre>

<p>Now, my new allocatable resource details sent to the Mesos master became <strong>total allocatable: ports(<em>):[31000-31099, 31101-32000, 9998-9998]; cpus(</em>):0.7; mem(<em>):801; disk(</em>):35164</strong>. The above config is crucial if we want to allocate our own custom range of ports. So this gave a good idea on how to control my resource allocation by creating custom configuration files. Now let&#8217;s restart Mesos-slave process.</p>

<pre><code>    $ service mesos-master restart

    $ ps axf | grep mesos-slave | grep -v grep
     1889 ?        Ssl    0:26 /usr/local/sbin/mesos-slave --master=zk://localhost:2181,localhost:2182,localhost:2183/mesos --log_dir=/var/log/mesos --containerizers=docker,mesos --resources=ports(*):[31000-31099, 31101-32000, 9998-9999]
     1900 ?        S      0:00  \_ logger -p user.info -t mesos-slave[1889]
     1901 ?        S      0:00  \_ logger -p user.err -t mesos-slave[1889]
</code></pre>

<p>Now we have the Mesos slave running with Native Docker support. But the current stable version of Marathon still don&#8217;t support the native Docker feature of Mesos. So we need to setup Marathon 0.7 from scratch. I&#8217;ve written a <a href="https://beingasysadmin.wordpress.com/2014/08/31/upgrading-marthon-for-mesos-native-docker-support/">blog</a> for upgrading Marathon from 0.6.x to 0.7. Once we have setup the Marathon 0.7, we can start using Mesos with the Native Docker support. Let&#8217;s create a new App in Marathon via its REST API.</p>

<p>create a JSON file in the new container format. say ubuntu.json</p>

<pre><code>{
    "id": "mesos-docker-test",
    "container": {
        "docker": {
            "image": "ubuntu:14.04"
        },
        "type": "DOCKER",
        "volumes": []
    },
    "cmd": "while sleep 10; do date -u +%T; done",
    "cpus": 0.2,
    "mem": 200,
    "ports": [9999],
    "requirePorts": true,
    "instances": 1
}
</code></pre>

<p>Now, we can use the Marathon API to launch the Docker task,</p>

<pre><code>$ curl -X POST -H "Content-Type: application/json" localhost:8080/v2/apps -d@ubuntu.json

{"id":"/mesos-docker-test","cmd":"while sleep 10; do date -u +%T; done","args":null,"user":null,"env":{},"instances":1,"cpus":0.2,"mem":200.0,"disk":0.0,"executor":"","constraints":[],"uris":[],"storeUrls":[],"ports":[9999],"requirePorts":true,"backoffSeconds":1,"backoffFactor":1.15,"container":{"type":"DOCKER","volumes":[],"docker":{"image":"ubuntu:14.04"}},"healthChecks":[],"dependencies":[],"upgradeStrategy":{"minimumHealthCapacity":1.0},"version":"2014-08-31T16:03:50.593Z"}

$ docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
25ba3950a9e1        ubuntu:14.04        /bin/sh -c 'while sl   2 minutes ago       Up 2 minutes                            mesos-a19af637-ffb9-4d3c-8b61-778d47087ace
</code></pre>

<p>Mesos UI</p>

<p><img src="/images/mcomaster.png"></p>

<p>Marathon UI</p>

<p><img src="/images/mcomaster.png"></p>

<p>With the new Native Docker support, Mesos is becoming more friendly with Docker. And this is a great achievement for people like me who are trying to use Mesos and Docker in our Infrastructure. A big kudos to Mesos and Mesosphere team for their great work :)</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/31/upgrading-marthon-for-mesos-native-docker-support/">Upgrading Marthon for Mesos Native Docker Support</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-08-31T15:27:00+00:00" pubdate data-updated="true">Aug 31<span>st</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/docker/'>Docker</a>, <a class='category' href='/blog/categories/marathon/'>Marathon</a>, <a class='category' href='/blog/categories/mesos/'>Mesos</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>A few days ago Mesos <code>0.20</code> version was released with native Docker support. Till that, Mesos Docker integration was performed using Mesosphere&#8217;s <a href="">Deimos</a> application. But with the new Mesos, there is no need for any external application for launching Docker containers in Mesos Slaves. As per Mesos <a href="http://mesos.apache.org/documentation/latest/docker-containerizer/">Documentation</a> we can run containers in two ways. 1) as a Task and 2) as an Executor. Currently Mesos 0.20 supports only the <code>host (--net=host)</code> Docker networking mode.</p>

<p>The current stable release of Marathon is 0.6X. The debian package provided by the Mesosphere also provides the 0.6x version. The container format has been completely changed in  0.7.x version. So we cannot use the 0.6.x version along with Mesos 0.20. More details are available in the Marathon <a href="https://mesosphere.github.io/marathon/docs/upgrade/06xto070.html">Documentation</a></p>

<p>The current Mesos Master branch is <code>version := "0.7.0-SNAPSHOT"</code>. So we need to compile Marathon from scratch. The only dependency for compiling Marathon is scala-sbt. Get the latest Debian package for sbt from <a href="http://www.scala-sbt.org/0.13/tutorial/Installing-sbt-on-Linux.html">here</a>. The current sbt version is 0.13.5.</p>

<pre><code>$ wget http://dl.bintray.com/sbt/debian/sbt-0.13.5.deb

$ dpkg -i sbt-0.13.5.deb
</code></pre>

<p>Now let&#8217;s clone the Marathon githib repo.</p>

<pre><code>$ git clone https://github.com/mesosphere/marathon.git &amp;&amp; cd marathon

$ sbt assembly

$ ./bin/build-distribution  # for building jar file
</code></pre>

<p>The above build command will create an executable jar file &#8220;marathon-runnable.jar&#8221; under the &#8220;target&#8221; folder. We can use this jar file along with Marathon 0.6.x start script. If we check the upstart script of Marathon 0.6.x, it uses <code>/usr/local/bin/marathon</code> binary for starting the service. So we need to edit two lines in this file to use our latest version 0.7&#8217;s jar file.</p>

<p>First let&#8217;s move our jar file to say &#8220;/opt/ folder.</p>

<pre><code>$ cp marathon-runable.jar /opt/marathon.jar
</code></pre>

<p>Now let&#8217;s edit the Marathon binary. Make the below changes in the binary file,</p>

<pre><code>marathon_jar="/opt/marathon.jar" # Line number 21

and

exec java "${vm_opts[@]}" -jar "$marathon_jar" "$@"  # -jar option added to use the jar file instead if the default -cp option. Line number 62
</code></pre>

<p>Now let&#8217;s restart the Marathon service.</p>

<pre><code>$ service marathon restart
</code></pre>

<p>Now let&#8217;s use the <code>ps</code> command and verify the service status.</p>

<pre><code>$ ps axf | grep marathon | grep -v grep

22653 ?        Ssl    6:09 java -Xmx512m -Djava.library.path=/usr/local/lib -Djava.util.logging.SimpleFormatter.format=%2$s %5$s%6$s%n -jar /opt/marathon.jar --zk zk://localhost:2181,localhost:2182,localhost:2183/marathon --master zk://localhost:2181,localhost:2182,localhost:2183/mesos
22663 ?        S      0:00  \_ logger -p user.info -t marathon[22653]
22664 ?        S      0:00  \_ logger -p user.notice -t marathon[22653]
</code></pre>

<p>Now Marathon is running with the version 0.7 jar file. Let&#8217;s verify by creating a new Docker task. Make sure that the Mesos version 0.20 is running on the host and not the older versions of Mesos. More detail about the new container format is available in the Marathon <a href="https://mesosphere.github.io/marathon/docs/upgrade/06xto070.html">Documentation</a> page.</p>

<p>create a JSON file in the new container format. say <code>ubuntu.json</code></p>

<pre><code>{
    "id": "mesos-docker-test",
    "container": {
        "docker": {
            "image": "ubuntu:14.04"
        },
        "type": "DOCKER",
        "volumes": []
    },
    "cmd": "while sleep 10; do date -u +%T; done",
    "cpus": 0.2,
    "mem": 200,
    "ports": [9999],
    "requirePorts": false,
    "instances": 1
}
</code></pre>

<p>Now, we can use the Marathon API to launch the Docker task,</p>

<pre><code>$ curl -X POST -H "Content-Type: application/json" localhost:8080/v2/apps -d@ubuntu.json

{"id":"/mesos-docker-test","cmd":"while sleep 10; do date -u +%T; done","args":null,"user":null,"env":{},"instances":1,"cpus":0.2,"mem":200.0,"disk":0.0,"executor":"","constraints":[],"uris":[],"storeUrls":[],"ports":[9999],"requirePorts":false,"backoffSeconds":1,"backoffFactor":1.15,"container":{"type":"DOCKER","volumes":[],"docker":{"image":"ubuntu:14.04"}},"healthChecks":[],"dependencies":[],"upgradeStrategy":{"minimumHealthCapacity":1.0},"version":"2014-08-31T16:03:50.593Z"}

$ docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
25ba3950a9e1        ubuntu:14.04        /bin/sh -c 'while sl   2 minutes ago       Up 2 minutes                            mesos-a19af637-ffb9-4d3c-8b61-778d47087ace
</code></pre>

<p>Here i&#8217;ve used Port number 9999. But by default, in Mesos 0.20, default port range resourced for Mesos master is [31000-32000]. Please refer my next Blog post on how to modify and set a custom port range.</p>

<p>With the new Native Docker support, Mesos is becoming more friendly with Docker. And this is a great achievement for people like me who are trying to use Mesos and Docker in our Infrastructure. A big kudos to Mesos and Mesosphere team for their great work :)</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/25/marathon-event-bus/">Marathon Event Bus</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-08-25T13:02:00+00:00" pubdate data-updated="true">Aug 25<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/marathon/'>Marathon</a>, <a class='category' href='/blog/categories/mesos/'>Mesos</a>, <a class='category' href='/blog/categories/mesosphere/'>Mesosphere</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>It&#8217;s almost 3 months since i&#8217;ve started with Mesos + Marathon. Yesterday, i was going through the <a href="https://mesosphere.github.io/marathon/docs">Marathon Doc Site</a>, and found that Marathon has a cool internal Event Bus. Currently Marathon has an ibuilt HTTP callback subscriber that POSTs events in JSON format to one or more endpoints. So i decided to give it a try to my Mesos test environment. More documentations of the <a href="https://mesosphere.github.io/marathon/docs/event-bus.html">Event Bus</a> are available in the Documentation page.</p>

<p>Currently, in my test environment all the Mesos and Marathon are installed from the Mesosphere <a href="https://mesosphere.io/learn/install_ubuntu_debian/">Debian</a> packages. Whether we use the packaged <strong>init</strong> or <strong>upstart</strong> script, both of them are directly calling the <code>/usr/local/bin/marathon</code> binary. For the Event Bus with http callback, we need to enable two flags (&#8211;event_subscriber http_callback &#8211;http_endpoints http://host1/foo,http://host2/bar) while starting the Marathon service ie, <em>&#8211;event_subscriber</em>, the type of subscriber that we are going to use and <em>&#8211;http_endpoints</em>, endpoints corresponding to the subscriber.</p>

<p>From the marathon binary, i found the it looks for files under the <code>/etc/marathon/conf/</code> folder, where each file is the <strong>name of the flag</strong> to be enabled and the <strong>content of the file is the flag values</strong>. So in our case, we need two files inside the &#8220;/etc/marathon/conf/&#8221;, 1) <code>event_subscriber</code> and 2) <code>http_endpoints</code></p>

<pre><code>root@vagrant-ubuntu-trusty-64:/etc/marathon/conf# ls /etc/marathon/conf/
event_subscriber  http_endpoints
</code></pre>

<p>And the content of these files are,</p>

<pre><code>root@vagrant-ubuntu-trusty-64:/etc/marathon/conf# cat /etc/marathon/conf/event_subscriber
http_callback

root@vagrant-ubuntu-trusty-64:/etc/marathon/conf# cat /etc/marathon/conf/http_endpoints
http://localhost:1234/
</code></pre>

<p>Now for test purpose, lets start a minimal webserver using <code>netcat</code> listening to port <em>1234</em></p>

<pre><code>$ nc -l -p 1234
</code></pre>

<p>Now netcat is listening on port 1234. Lets restart Marathon with our new flags.</p>

<pre><code>$ service marathon restart
</code></pre>

<p>Now lets check if the flags are enabled properly,</p>

<pre><code>root@vagrant-ubuntu-trusty-64:/etc/marathon/conf# ps axf | grep marathon | grep -v grep
2780 ?        Sl     0:36 java -Xmx512m -Djava.library.path=/usr/local/lib -Djava.util.logging.SimpleFormatter.format=%2$s %5$s%6$s%n -cp /usr/local/bin/marathon mesosphere.marathon.Main --zk zk://localhost:2181,localhost:2182,localhost:2183/marathon --master zk://localhost:2181,localhost:2182,localhost:2183/mesos --http_endpoints http://localhost:1234/ --event_subscriber http_callback
2791 ?        S      0:00  \_ logger -p user.info -t marathon[2780]
2792 ?        S      0:00  \_ logger -p user.notice -t marathon[2780]
</code></pre>

<p>From the above <em>ps</em> command, it can be seen that the flags are enabled properly. Now i&#8217;m going to start a docker container. So as per the Event Bus documentation, Callbacks are Fired every time Marathon receives an API request that modifies an app (create, update, delete)</p>

<pre><code>$ curl -X POST -H "Content-Type: application/json" localhost:8080/v2/apps -d@ubuntu.json
</code></pre>

<p>where <code>ubuntu.json</code> is,</p>

<pre><code>{
    "container": {
    "image": "docker:///libmesos/ubuntu",
    "options" : []
  },
  "id": "ubuntu2",
  "instances": "1",
  "cpus": ".3",
  "mem": "200",
  "uris": [ ],
  "ports": [9999],
  "cmd": "while sleep 10; do date -u +%T; done"
}
</code></pre>

<p>Once the Marathon API is fired, we will get a POST request on our netcat with the Event details. Below is the Event received on my netcat server.</p>

<pre><code>root@vagrant-ubuntu-trusty-64:/var/tmp# nc -l -p 1234
POST / HTTP/1.1
Host: localhost:1234
Accept: application/json
User-Agent: spray-can/1.2.1
Content-Type: application/json; charset=UTF-8
Content-Length: 427

{"clientIp":"127.0.0.1","uri":"/v2/apps","appDefinition":{"id":"ubuntu2","cmd":"while sleep 10; do date -u +%T; done","env":{},"instances":1,"cpus":0.3,"mem":200.0,"disk":0.0,"executor":"","constraints":[],"uris":[],"ports":[9999],"taskRateLimit":1.0,"container":{"image":"docker:///libmesos/ubuntu","options":[]},"healthChecks":[],"version":{"dateTime":{}}},"eventType":"api_post_event","timestamp":"2014-08-25T13:23:27.059Z"}
</code></pre>

<p>This Event bus is really usefull as it notifies us on all the Event changes happening on our Mesos cluster. We can also buit an Event notification system based on these callbacks. Though the current Events has very minimal details, i&#8217;m sure that more Event types will get added soon into Marathon.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/15/managing-docker-cluster-using-multiple-mesos-masters/">Managing Docker Cluster Using Multiple Mesos Masters</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-08-15T16:44:00+00:00" pubdate data-updated="true">Aug 15<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/docker/'>Docker</a>, <a class='category' href='/blog/categories/marathon/'>Marathon</a>, <a class='category' href='/blog/categories/mesos/'>Mesos</a>, <a class='category' href='/blog/categories/zookeeper/'>Zookeeper</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>In my previous <a href="http://beingasysadmin.wordpress.com/2014/06/27/managing-docker-clusters-using-mesos-and-marathon/">blog</a>, i&#8217;ve described how to manage Docker cluster using Mesos and Marathon. It was using a single Mesos Master. But for production, we cannot go with a single Mesos master, as it will result in a sinlge point of failure. Mesos supports Multi master via Zookeeper. So in this blog i&#8217;m going to explain how to setup a Multi Master Mesos Cluster using Zookeper for Automatic promotion of Mesos master when the current Active Mesos Master fails. This time i&#8217;m going to setup 3 Zookeeper services and 2 Mesos Master on a single Vagrant box.</p>

<h1>Setting up ZooKeeper Cluster</h1>

<p>First lets download the the latest stable Zookeeper source code.</p>

<pre><code>$ wget http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/stable/zookeeper-3.4.6.tar.gz
</code></pre>

<p>Now extract the tar file and create 3 copies of the same, say zookeeper1, zookeeper2 and zookeeper3. Also Zookeeper needs Java on the machine, so let&#8217;s install java dependencies.</p>

<pre><code>$ apt-get install openjdk-7-jdk openjdk-7-jre
</code></pre>

<p>Now inside, the extracted zookeeper source folder, we need to create a config file. So in our case, inside each Zookeeper folder, we need a <code>zoo.cfg</code> file.</p>

<p>For zookeeper1 folder,</p>

<pre><code># content of zoo.cfg
tickTime=2000
dataDir=/var/lib/zookeeper1/
clientPort=2181
initLimit=5
syncLimit=2
server.1=localhost:2888:3888
server.2=localhost:2889:3889
server.3=localhost:2890:3890
</code></pre>

<p>For zookeeper2 folder,</p>

<pre><code># content of zoo.cfg
tickTime=2000
dataDir=/var/lib/zookeeper2/
clientPort=2182
initLimit=5
syncLimit=2
server.1=localhost:2888:3888
server.2=localhost:2889:3889
server.3=localhost:2890:3890
</code></pre>

<p>For zookeeper3 folder,</p>

<pre><code>    # content of zoo.cfg
tickTime=2000
dataDir=/var/lib/zookeeper3/
clientPort=2183
initLimit=5
syncLimit=2
server.1=localhost:2888:3888
server.2=localhost:2889:3889
server.3=localhost:2890:3890
</code></pre>

<p>Here, since the 3 zookeeper services are running on the same host, i&#8217;ve assigned separate ports. If the zookeeper are running on separate instances, then we can have the same ports for all of the zookeeper nodes. There are two port numbers. The first followers use to connect to the leader, and the second is for leader election.</p>

<p>Now we can start the Zookeeper in Foreground using the <code>./bin/zkServer.sh start-foreground</code> from each of the 3 zookeeper folders. <code>netstat -nltp | grep 288</code> will display the port of the zookeeper service which is the current master among the cluster. We can also check the connectivity using the zookeepr client binary available in the zookeeper source folder. Once zookeeper cluster is UP, we can go ahead setting up Mesos.</p>

<h1>Setting up Multiple Mesos Masters</h1>

<p>Mesossphere team has already built packages for Mesos,Marathon and Deimos. So one of my Mesos master will be from the package. But i also wanted to play with the Mesos Source, so i decided to build Mesos from the source. So my second Mesos Master will be from the scratch.</p>

<p>First Master from the Mesosphere package.</p>

<pre><code>$ apt-key adv --keyserver keyserver.ubuntu.com --recv E56151BF

$ DISTRO=$(lsb_release -is | tr '[:upper:]' '[:lower:]')

$ CODENAME=$(lsb_release -cs)

# Add the repository
$ echo "deb http://repos.mesosphere.io/${DISTRO} ${CODENAME} main" | sudo tee /etc/apt/sources.list.d/mesosphere.list

$ apt-get -y update

$ apt-get -y install mesos marathon deimos
</code></pre>

<p>Now we need to define the Zookeeper cluster details so that Mesos master and slave can connect. Edit <code>/etc/mesos/zk</code> and add <code>zk://localhost:2181,localhost:2182,localhost:2183/mesos</code>. More details about Zookeeper URL is available <a href="https://github.com/deepakmdass88/mesos-doc-test/blob/master/Using-ZooKeeper.textile">here</a>. Also More details about installing Mesos from Mesosphere&#8217;s Debian package is available in their <a href="http://mesosphere.io/docs/getting-started/debian-install/">website</a>.</p>

<p>So now we have one master ready, we can start the Mesos Master, Mesos Slave and Marathon. ANd esnure that they can connect to our Zookeeper cluster properly. We can also see the connection status in the stdout of the zookeeper process as they are running in foreground.</p>

<p>Now building the second Mesos master from scratch. First let&#8217;s download the Mesos Source Code.</p>

<pre><code>$ wget http://archive.apache.org/dist/mesos/0.19.0/mesos-0.19.0.tar.gz

$ tar xvzf mesos-0.19.0.tar.gz &amp;&amp; cd mesos-0.19.0

$ ./bootstrap &amp;&amp; ./configure --prefix=/usr/local/mesos

$ make &amp;&amp; make install
</code></pre>

<p>Once the Compilation is succeeded, we can start the new Mesos Master service. The default port 5050 is used by the existing master, so we need to run this new service on a different port.</p>

<pre><code>$ /usr/local/mesos/sbin/mesos-master --zk=zk://localhost:2181,localhost:2182,localhost:2183/mesos --port=5054 --quorum=1 --registry=in_memory --work_dir=/var/lib/mesos/
</code></pre>

<p>Once the new Mesos Master is up, we can create a test container via Marathon Rest API. Create a simple json file called ubuntu.json</p>

<pre><code>{
    "container": {
    "image": "docker:///libmesos/ubuntu",
    "options" : []
  },
  "id": "ubuntu",
  "instances": "1",
  "cpus": ".3",
  "mem": "200",
  "uris": [ ],
  "cmd": "while sleep 10; do date -u +%T; done"
}


$ curl -X POST -H "Content-Type: application/json" localhost:8080/v2/apps -d@ubuntu.json
</code></pre>

<p>This will launch a single container. We can check the status of the container via the Marathon UI as well as via RestAPI also. Now comes the critical part for Production, What happens when the master fails. Mesos Documentation says, if the current master fails, in a Multi master Mesos Cluster, Zookeeper will elect a new Master, in our case the second Mesos master service. And Mesos Claims that the running services will not get crashed and the status will be taken by the newly promoted master. In our case, we have a Docker container running as a long running service.</p>

<p><img src="https://beingasysadmin.files.wordpress.com/2014/06/marathon-1.png?w=6400"></p>

<p>First i&#8217;ll stop one of the Mesos master service,</p>

<pre><code>$ service mesos-master stop
</code></pre>

<p>Now immidiately on the stdout of the Zookeeper, we can see the logs corresponding to the Election process. Below is the same,</p>

<pre><code>I0814 19:41:07.889044  9093 contender.cpp:243] New candidate (id='7') has entered the contest for leadership
2014-08-14 19:41:07,896:9088(0x7ff8e2ffd700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:2183], sessionId=0x347d606d0d90003, negotiated timeout=10000
I0814 19:41:07.899509  9092 group.cpp:310] Group process ((10)@10.0.2.15:5054) connected to ZooKeeper
I0814 19:41:07.900049  9092 group.cpp:784] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0814 19:41:07.900069  9092 group.cpp:382] Trying to create path '/mesos' in ZooKeeper
I0814 19:41:07.915621  9092 detector.cpp:135] Detected a new leader: (id='6')
I0814 19:41:07.915699  9092 group.cpp:655] Trying to get '/mesos/info_0000000006' in ZooKeeper
I0814 19:41:07.920266  9094 network.hpp:423] ZooKeeper group memberships changed
I0814 19:41:07.920910  9094 group.cpp:655] Trying to get '/mesos/log_replicas/0000000003' in ZooKeeper
I0814 19:41:07.924144  9095 network.hpp:461] ZooKeeper group PIDs: { log-replica(1)@10.0.2.15:5054 }
I0814 19:41:07.926575  9092 detector.cpp:377] A new leading master (UPID=master@10.0.2.15:5054) is detected
I0814 19:41:07.926679  9092 master.cpp:957] The newly elected leader is master@10.0.2.15:5054 with id 20140814-194033-251789322-5050-8808
</code></pre>

<p>As per the above logs, the second Mesos Master running at port 5054 was promoted as New Mesos Master. This will be reflected to Marathon also. We can go to the Marthon UI and make sure that the Docker process that we have started using the old Mesos master is still running fine. We can even try making some scaling changes and can make sure that the new master can alter the process.</p>

<p>In my testing, i&#8217;ve tried stoping each of the service and ensured that only one Mesos master is running and also tried scaling the Apps from one Master service to make sure that the new Master was able to keep track of all these changes. The results were quite promising. Mesos + Marrthon + Docker indeed is killer combo. We can really built a cross vendor independent cluster with scaling capabilites.</p>
</div>
  
  


</div>

      </article>
    
    </div>
      <div class="row">
        <ul class="pager">
          
            <li class="previous">
              <a href="/blog/page/2/">&larr; Older</a>
            </li>
          
          
        </ul>
      </div>
  </div>
</div>


  <div id="footer-widgets">
  <div class="container">
    <div class="row">
  <div class="span3">
    <h2>recent posts</h2>
    <ul class="recent_posts">
      
        <li>
          <a href="/blog/2015/01/06/setting-up-docker-private-registry/">Setting up Docker Private Registry</a>
        </li>
      
        <li>
          <a href="/blog/2014/12/07/automating-debian-package-management/">Automating Debian Package Management</a>
        </li>
      
        <li>
          <a href="/blog/2014/12/07/managing-debian-apt-repository-via-aptly/">Managing Debian APT repository via Aptly</a>
        </li>
      
        <li>
          <a href="/blog/2014/12/03/building-debian-packages/">Building Debian packages</a>
        </li>
      
        <li>
          <a href="/blog/2014/10/23/sippy-cup-freeswitch-load-test-simplified/">sippy_cup - FreeSwitch Load Test Simplified</a>
        </li>
      
    </ul>
    <h2><a href="/blog/archives">archives</a></h2>
  </div>
  <div class="span3">
    <h2>instagram</h2>
    <div class="instagram"></div>
    <button id="instabutton" class="btn">more</button>
  </div>
  <div class="span4">
    <h2>twitter</h2>
    <a href="https://twitter.com/deepakmdass88" class="twitter-follow-button" data-show-count="true" data-lang="en">Follow @deepakmdass88</a>
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
    <div class="tweet">
    </div>
  </div>
  <div class="span2">
    <h2>found on</h2>
    <a href="https://github.com/deepakmdass88/" rel="tooltip" title="Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a>
    <a href="http://www.linkedin.com/pub/deepak-dass/44/54/602" rel="tooltip" title="Linkedin"><img class="social_icon" title="Linkedin" alt="Linkedin icon" src="/images/glyphicons_377_linked_in.png"></a>
    <a href="http://twitter.com/deepakmdass88" rel="tooltip" title="Twitter"><img class="social_icon" title="Twitter" alt="Twitter icon" src="/images/glyphicons_391_twitter_t.png"></a>
    <a href="https://plus.google.com/105770729176086017609/posts" rel="tooltip" title="Google Plus"><img class="social_icon" title="Google Plus" alt="Google Plus icon" src="/images/glyphicons_386_google_plus.png"></a>
    <a href="http://ttp://www.quora.com/Deepak-M-Dass" rel="tooltip" title="Quora"><img class="social_icon" title="Quora" alt="Quora icon" src="/images/glyphicons_385_quora.png"></a>
    <h2>contact at</h2>
    <a href="mailto:deepakmdass88@gmail.com">deepakmdass88@gmail.com</a>
  </div>
</div>

  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-left">
  <a href="/">Welcome to My Nerd World</a>
  - Copyright &copy; 2015 - Deepak M Das
</p>
<p class="pull-right">
  Powered by <a href="http://octopress.org/">Octopress</a>. Designed by <a href="http://www.AdrianArtiles.com">Adrian Artiles</a>.
</p>

  </div>
</footer>

  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript"></script>
<script>window.jQuery || document.write('<script src="/javascripts/libs/jquery-1.7.2.min.js" type="text/javascript"><\/script>')</script>
<script src="/javascripts/libs/bootstrap.min.js" type="text/javascript"></script>
<script src="/javascripts/jquery.tweet.js" type="text/javascript"></script>
<script src="/javascripts/jquery.instagram.js" type="text/javascript"></script>
<script src="/javascripts/custom.js" type="text/javascript"></script>





</body>
</html>
