

<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Welcome to My Nerd World</title>
  <meta name="author" content="Deepak M Das">
  <link rel="author" href="humans.txt">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  
    
  
  <meta name="description" content=" ">
  
  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://beingasysadmin.com/">
  <link href="/favicon.png" rel="icon">
  <link href='http://fonts.googleapis.com/css?family=Cantarell' rel='stylesheet' type='text/css'>
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Welcome to My Nerd World" type="application/atom+xml">
  <meta name="og:type" content="website" />
  <meta name="og:site_name" content="Welcome to My Nerd World" />
  <meta name="og:title" content="Welcome to My Nerd World" />
  <meta name="og:description" content=" " />
  <meta name="og:url" content="http://beingasysadmin.com/index.html"/>
  <meta name="url" content="http://beingasysadmin.com/index.html">
  
  <meta name="distribution" content="global">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <div id="front-wrapper">
  <div id="hero">
    <div id="hero-inner" class="container">
      <div class="span10 offset1">
  <h1>
    I&#8217;m <em>Deepak</em>,<br/>
    a <em>Random Sys Admin</em><br/>
    by <em>Trade</em>
  </h1>
</div>

    </div>
  </div>
  <section id="sub-hero">
    <div class="container">
      <div class="row">
  <div class="span4">
    <h2>about me</h2>
    <p>A Random Sys Admin by Trade, Hacker by Choice, Loves Linux, Puppet, Ruby, Monitoring, and a lot.</p>
  </div>
  <div class="span6">
    <h2>open source projects</h2>
    <dl class="dl-horizontal">
	    <dt><a href="https://github.com/deepakmdass88/">TweetGrabber</a><a href="https://github.com/deepakmdass88/" rel="tooltip" title="open sourced at Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a></dt>
	    <dd>A Live Tweet Grabber based built on Ruby+REDIS+SINATRA</dd>
      <dt><a href="https://github.com/deepakmdass88/ruby-virtmgr.git">Ruby-Virtmgr   </a><a href="https://github.com/deepakmdass88/ruby-virtmgr.git" rel="tooltip" title="open sourced at Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a></dt>
      <dd>A simple CLI Ruby app for Managing KVM based VM&#8217;s</dd>
    </dl>
  </div>
  <div class="span2">
    <h2>found on</h2>
    <a href="https://github.com/deepakmdass88/" rel="tooltip" title="Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a>
    <a href="http://www.linkedin.com/pub/deepak-dass/44/54/602" rel="tooltip" title="Linkedin"><img class="social_icon" title="Linkedin" alt="Linkedin icon" src="/images/glyphicons_377_linked_in.png"></a>
    <a href="http://twitter.com/deepakmdass88" rel="tooltip" title="Twitter"><img class="social_icon" title="Twitter" alt="Twitter icon" src="/images/glyphicons_391_twitter_t.png"></a>
    <a href="https://plus.google.com/105770729176086017609/posts" rel="tooltip" title="Google Plus"><img class="social_icon" title="Google Plus" alt="Google Plus icon" src="/images/glyphicons_386_google_plus.png"></a>
    <a href="http://www.quora.com/Deepak-M-Dass" rel="tooltip" title="Quora"><img class="social_icon" title="Quora" alt="Quora icon" src="/images/glyphicons_385_quora.png"></a>
    <h2>contact at</h2>
    <a href="mailto:deepakmdass88@gmail.com">deepakmdass88@gmail.com</a>
  </div>
</div>

    </div>
  </section>
  <div class="container">
    <div class="row">
    
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/14/monitoring-redis-using-collectd-and-elk/">Monitoring Redis Using CollectD and ELK</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-10-14T23:57:00+00:00" pubdate data-updated="true">Oct 14<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/elk/'>ELK</a>, <a class='category' href='/blog/categories/collectd/'>collectd</a>, <a class='category' href='/blog/categories/collectd-redis/'>collectd-redis</a>, <a class='category' href='/blog/categories/redis/'>redis</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>Redis is an open-source, networked, in-memory, key-value data store. It&#8217;s being heavily used every where from Web stack to Monitoring to Message queues. Monitoring tools like Sensu already has some good scripts to Monitor Redis. Last Month during <a href="http://in.pycon.org/funnel/2014/245-sharq-an-api-queueing-system-built-at-plivo">PyCon 2014</a> <a href="https://twitter.com/plivo">@Plivo</a>, opensourced a new rate limited queue called <a href="http://sharq.io/">SHARQ</a> which is based on Redis. So apart from just Monitoring checks, we decided to have a tsdb of what&#8217;s happening in our Redis Cluster. Since we are heavily using ELK stack to visualize our infrastructure, we decided to go ahead with the same.</p>

<h3>CollectD Redis Plugin</h3>

<p>There is a cool CollectD <a href="https://github.com/powdahound/redis-collectd-plugin">plugin</a> for Redis. It pulls a verity of Data from Redis which includes, Memory used, Commands Processed, No. of Connected Clients and slaves, No. of blocked Clients, No. of Keys stored/db, uptime and challenges since last save. The installation is pretty simple and straight forward.</p>

<pre><code>$ apt-get update &amp;&amp; apt-get install collectd

$ git clone https://github.com/powdahound/redis-collectd-plugin.git /tmp/redis-collectd-plugin
</code></pre>

<p>Now place the <code>redis_info.py</code> file onto the collectd folder and enable the <em>Python</em> Plugins so that collectd can use this python file. Below is our collectd conf</p>

<pre><code>Hostname    "&lt;redis-server-fqdn&gt;"
Interval 10
Timeout 4
Include "/etc/collectd/filters.conf"
Include "/etc/collectd/thresholds.conf"
LoadPlugin network
ReportStats true

        LogLevel info

Include "/etc/collectd/redis.conf"      # This is the configuration for the Redis plugin
&lt;Plugin network&gt;
    Server "&lt;logstash-fqdn&gt;" "&lt;logstash-collectd-port&gt;"
&lt;/Plugin&gt;
</code></pre>

<p>Now copy the redis python plugin and the conf file to collectd folder.</p>

<pre><code>$ mkdir /etc/collectd/plugin            # This is where we are going to place our custom plugins

$ cp /tmp/redis-collectd-plugin/redis_info.py /etc/collectd/plugin/

$ cp /tmp/redis-collectd-plugin/redis.conf /etc/collectd/
</code></pre>

<p>By default, the plugin folder in the <code>redis.conf</code> is defined as <em>&#8216;/opt/collectd/lib/collectd/plugins/python&#8217;</em>. Make sure to replace this with the location where we are copying the plugin file, in our case <strong>&#8220;/etc/collectd/plugin&#8221;</strong>. Now lets restart the collectd daemon to enable the redis plugin.</p>

<pre><code>$ /etc/init.d/collectd stop

$ /etc/init.d/collectd start
</code></pre>

<p>In my previous <a href="http://beingasysadmin.wordpress.com/2014/05/11/extending-elk-stack-to-voip-infrastructure/">Blog</a>, i&#8217;ve mentioned how to enable and use the ColectD input plugin in Logstash and to use Kibana to plot the data coming from the collectd. Below are the Data&#8217;s that we are receiving from the CollectD on Logstash,</p>

<pre><code>  1) type_instance: blocked_clients
  2) type_instance: evicted_keys
  3) type_instance: connected_slaves
  4) type_instance: commands_processed
  5) type_instance: connected_clients
  6) type_instance: used_memory 
  7) type_instance: &lt;dbname&gt;-keys
  8) type_instance: changes_since_last_save
  9) type_instance: uptime_in_seconds
10) type_instance: connections_received
</code></pre>

<p>Now we need to Visualize these via Kibana. Lets create some ElasticSearch queries so that visualize them directly. Below are some sample queries created in Kibana UI.</p>

<pre><code>1) type_instance: "commands_processed" AND host: "&lt;redis-host-fqdn&gt;"
2) type_instance: "used_memory" AND host: "&lt;redis-host-fqdn&gt;"
3) type_instance: "connections_received" AND host: "&lt;redis-host-fqdn&gt;"
4) type_instance: "&lt;dbname&gt;-keys" AND host: "&lt;redis-host-fqdn&gt;"
</code></pre>

<p><img src="/images/Kibana_ES_Query.png"></p>

<p>Now We have some sample queries, lets visualize them.</p>

<p><img src="/images/ES_graph.png"></p>

<p><img src="/images/ES_graph2.png"></p>

<p>Now create histograms in the same procedure by changing the Selected Queries.</p>

<p><img src="/images/ES_graph3.png"></p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/31/mesos-with-native-docker-support/">Mesos With Native Docker Support</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-08-31T17:34:00+00:00" pubdate data-updated="true">Aug 31<span>st</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/docker/'>Docker</a>, <a class='category' href='/blog/categories/marathon/'>Marathon</a>, <a class='category' href='/blog/categories/mesos/'>Mesos</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>In my previous <a href="http://beingasysadmin.wordpress.com/2014/06/27/managing-docker-clusters-using-mesos-and-marathon/">blog</a>, i&#8217;ve explained how to manage Docker cluster using Mesos and Marathon. Now Mesos has released 0.20 with Native Docker support. Till Mesos 0.19, we had to use mesosphere&#8217;s <a href="https://github.com/mesosphere/deimos">Deimos</a> for running Docker containers on Mesos slaves. Now from Mesos 0.20, Mesos has the ability to run Docker containers directly. As per Mesos Documentation we can run containers in two ways. 1) as a Task and 2) as an Executor. Currently Mesos 0.20 supports only the host (&#8211;net=host) Docker networking mode.</p>

<p>Install the latest Mesos 0.20 using the mesosphere&#8217;s Mesos <a href="http://mesosphere.io/downloads/">Debian</a> package. Once we have setup the Zookeeper and Docker, we need to make a few changes to enable the Mesos Native Docker support. We need to start all the Mesos-Slave with <code>--containerizers=docker,mesos</code> flag. For enabling this flag, we need to create a file <code>containerizers</code> in <em>/etc/mesos-slave/containerizers</em> with content &#8220;docker,mesos&#8221;. Mesos slave process, when starting up, will read the folder and enable this flag.</p>

<pre><code>$ echo 'docker,mesos' &gt; /etc/mesos-slave/containerizers

$ service mesos-master restart
</code></pre>

<p>I was trying to start a container with Port <strong>9999</strong> via Marathon REST API. But the task was keep on failing. Up on investigating the logs, i found that the slave when sends the allocatable resource details to mesos master was <strong>total allocatable: cpus(<em>):0.8; mem(</em>):801; disk(<em>):35164; ports(</em>):[31000-31099, 31101-32000]</strong>. So the allowed port range was [31000-31099, 31101-32000]. So if we wnat to use any other custom port, we need to define the same in the <em>/etc/mesos-slave</em> folder by creating a config file.</p>

<pre><code>    $ echo "ports(*):[31000-31099, 31101-32000, 9998-9999]" &gt; "/etc/mesos-slave/resources"
</code></pre>

<p>Now, my new allocatable resource details sent to the Mesos master became <strong>total allocatable: ports(<em>):[31000-31099, 31101-32000, 9998-9998]; cpus(</em>):0.7; mem(<em>):801; disk(</em>):35164</strong>. The above config is crucial if we want to allocate our own custom range of ports. So this gave a good idea on how to control my resource allocation by creating custom configuration files. Now let&#8217;s restart Mesos-slave process.</p>

<pre><code>    $ service mesos-master restart

    $ ps axf | grep mesos-slave | grep -v grep
     1889 ?        Ssl    0:26 /usr/local/sbin/mesos-slave --master=zk://localhost:2181,localhost:2182,localhost:2183/mesos --log_dir=/var/log/mesos --containerizers=docker,mesos --resources=ports(*):[31000-31099, 31101-32000, 9998-9999]
     1900 ?        S      0:00  \_ logger -p user.info -t mesos-slave[1889]
     1901 ?        S      0:00  \_ logger -p user.err -t mesos-slave[1889]
</code></pre>

<p>Now we have the Mesos slave running with Native Docker support. But the current stable version of Marathon still don&#8217;t support the native Docker feature of Mesos. So we need to setup Marathon 0.7 from scratch. I&#8217;ve written a <a href="https://beingasysadmin.wordpress.com/2014/08/31/upgrading-marthon-for-mesos-native-docker-support/">blog</a> for upgrading Marathon from 0.6.x to 0.7. Once we have setup the Marathon 0.7, we can start using Mesos with the Native Docker support. Let&#8217;s create a new App in Marathon via its REST API.</p>

<p>create a JSON file in the new container format. say ubuntu.json</p>

<pre><code>{
    "id": "mesos-docker-test",
    "container": {
        "docker": {
            "image": "ubuntu:14.04"
        },
        "type": "DOCKER",
        "volumes": []
    },
    "cmd": "while sleep 10; do date -u +%T; done",
    "cpus": 0.2,
    "mem": 200,
    "ports": [9999],
    "requirePorts": true,
    "instances": 1
}
</code></pre>

<p>Now, we can use the Marathon API to launch the Docker task,</p>

<pre><code>$ curl -X POST -H "Content-Type: application/json" localhost:8080/v2/apps -d@ubuntu.json

{"id":"/mesos-docker-test","cmd":"while sleep 10; do date -u +%T; done","args":null,"user":null,"env":{},"instances":1,"cpus":0.2,"mem":200.0,"disk":0.0,"executor":"","constraints":[],"uris":[],"storeUrls":[],"ports":[9999],"requirePorts":true,"backoffSeconds":1,"backoffFactor":1.15,"container":{"type":"DOCKER","volumes":[],"docker":{"image":"ubuntu:14.04"}},"healthChecks":[],"dependencies":[],"upgradeStrategy":{"minimumHealthCapacity":1.0},"version":"2014-08-31T16:03:50.593Z"}

$ docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
25ba3950a9e1        ubuntu:14.04        /bin/sh -c 'while sl   2 minutes ago       Up 2 minutes                            mesos-a19af637-ffb9-4d3c-8b61-778d47087ace
</code></pre>

<p>Mesos UI</p>

<p><img src="/images/mcomaster.png"></p>

<p>Marathon UI</p>

<p><img src="/images/mcomaster.png"></p>

<p>With the new Native Docker support, Mesos is becoming more friendly with Docker. And this is a great achievement for people like me who are trying to use Mesos and Docker in our Infrastructure. A big kudos to Mesos and Mesosphere team for their great work :)</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/31/upgrading-marthon-for-mesos-native-docker-support/">Upgrading Marthon for Mesos Native Docker Support</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-08-31T15:27:00+00:00" pubdate data-updated="true">Aug 31<span>st</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/docker/'>Docker</a>, <a class='category' href='/blog/categories/marathon/'>Marathon</a>, <a class='category' href='/blog/categories/mesos/'>Mesos</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>A few days ago Mesos <code>0.20</code> version was released with native Docker support. Till that, Mesos Docker integration was performed using Mesosphere&#8217;s <a href="">Deimos</a> application. But with the new Mesos, there is no need for any external application for launching Docker containers in Mesos Slaves. As per Mesos <a href="http://mesos.apache.org/documentation/latest/docker-containerizer/">Documentation</a> we can run containers in two ways. 1) as a Task and 2) as an Executor. Currently Mesos 0.20 supports only the <code>host (--net=host)</code> Docker networking mode.</p>

<p>The current stable release of Marathon is 0.6X. The debian package provided by the Mesosphere also provides the 0.6x version. The container format has been completely changed in  0.7.x version. So we cannot use the 0.6.x version along with Mesos 0.20. More details are available in the Marathon <a href="https://mesosphere.github.io/marathon/docs/upgrade/06xto070.html">Documentation</a></p>

<p>The current Mesos Master branch is <code>version := "0.7.0-SNAPSHOT"</code>. So we need to compile Marathon from scratch. The only dependency for compiling Marathon is scala-sbt. Get the latest Debian package for sbt from <a href="http://www.scala-sbt.org/0.13/tutorial/Installing-sbt-on-Linux.html">here</a>. The current sbt version is 0.13.5.</p>

<pre><code>$ wget http://dl.bintray.com/sbt/debian/sbt-0.13.5.deb

$ dpkg -i sbt-0.13.5.deb
</code></pre>

<p>Now let&#8217;s clone the Marathon githib repo.</p>

<pre><code>$ git clone https://github.com/mesosphere/marathon.git &amp;&amp; cd marathon

$ sbt assembly

$ ./bin/build-distribution  # for building jar file
</code></pre>

<p>The above build command will create an executable jar file &#8220;marathon-runnable.jar&#8221; under the &#8220;target&#8221; folder. We can use this jar file along with Marathon 0.6.x start script. If we check the upstart script of Marathon 0.6.x, it uses <code>/usr/local/bin/marathon</code> binary for starting the service. So we need to edit two lines in this file to use our latest version 0.7&#8217;s jar file.</p>

<p>First let&#8217;s move our jar file to say &#8220;/opt/ folder.</p>

<pre><code>$ cp marathon-runable.jar /opt/marathon.jar
</code></pre>

<p>Now let&#8217;s edit the Marathon binary. Make the below changes in the binary file,</p>

<pre><code>marathon_jar="/opt/marathon.jar" # Line number 21

and

exec java "${vm_opts[@]}" -jar "$marathon_jar" "$@"  # -jar option added to use the jar file instead if the default -cp option. Line number 62
</code></pre>

<p>Now let&#8217;s restart the Marathon service.</p>

<pre><code>$ service marathon restart
</code></pre>

<p>Now let&#8217;s use the <code>ps</code> command and verify the service status.</p>

<pre><code>$ ps axf | grep marathon | grep -v grep

22653 ?        Ssl    6:09 java -Xmx512m -Djava.library.path=/usr/local/lib -Djava.util.logging.SimpleFormatter.format=%2$s %5$s%6$s%n -jar /opt/marathon.jar --zk zk://localhost:2181,localhost:2182,localhost:2183/marathon --master zk://localhost:2181,localhost:2182,localhost:2183/mesos
22663 ?        S      0:00  \_ logger -p user.info -t marathon[22653]
22664 ?        S      0:00  \_ logger -p user.notice -t marathon[22653]
</code></pre>

<p>Now Marathon is running with the version 0.7 jar file. Let&#8217;s verify by creating a new Docker task. Make sure that the Mesos version 0.20 is running on the host and not the older versions of Mesos. More detail about the new container format is available in the Marathon <a href="https://mesosphere.github.io/marathon/docs/upgrade/06xto070.html">Documentation</a> page.</p>

<p>create a JSON file in the new container format. say <code>ubuntu.json</code></p>

<pre><code>{
    "id": "mesos-docker-test",
    "container": {
        "docker": {
            "image": "ubuntu:14.04"
        },
        "type": "DOCKER",
        "volumes": []
    },
    "cmd": "while sleep 10; do date -u +%T; done",
    "cpus": 0.2,
    "mem": 200,
    "ports": [9999],
    "requirePorts": false,
    "instances": 1
}
</code></pre>

<p>Now, we can use the Marathon API to launch the Docker task,</p>

<pre><code>$ curl -X POST -H "Content-Type: application/json" localhost:8080/v2/apps -d@ubuntu.json

{"id":"/mesos-docker-test","cmd":"while sleep 10; do date -u +%T; done","args":null,"user":null,"env":{},"instances":1,"cpus":0.2,"mem":200.0,"disk":0.0,"executor":"","constraints":[],"uris":[],"storeUrls":[],"ports":[9999],"requirePorts":false,"backoffSeconds":1,"backoffFactor":1.15,"container":{"type":"DOCKER","volumes":[],"docker":{"image":"ubuntu:14.04"}},"healthChecks":[],"dependencies":[],"upgradeStrategy":{"minimumHealthCapacity":1.0},"version":"2014-08-31T16:03:50.593Z"}

$ docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
25ba3950a9e1        ubuntu:14.04        /bin/sh -c 'while sl   2 minutes ago       Up 2 minutes                            mesos-a19af637-ffb9-4d3c-8b61-778d47087ace
</code></pre>

<p>Here i&#8217;ve used Port number 9999. But by default, in Mesos 0.20, default port range resourced for Mesos master is [31000-32000]. Please refer my next Blog post on how to modify and set a custom port range.</p>

<p>With the new Native Docker support, Mesos is becoming more friendly with Docker. And this is a great achievement for people like me who are trying to use Mesos and Docker in our Infrastructure. A big kudos to Mesos and Mesosphere team for their great work :)</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/25/marathon-event-bus/">Marathon Event Bus</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-08-25T13:02:00+00:00" pubdate data-updated="true">Aug 25<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/marathon/'>Marathon</a>, <a class='category' href='/blog/categories/mesos/'>Mesos</a>, <a class='category' href='/blog/categories/mesosphere/'>Mesosphere</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>It&#8217;s almost 3 months since i&#8217;ve started with Mesos + Marathon. Yesterday, i was going through the <a href="https://mesosphere.github.io/marathon/docs">Marathon Doc Site</a>, and found that Marathon has a cool internal Event Bus. Currently Marathon has an ibuilt HTTP callback subscriber that POSTs events in JSON format to one or more endpoints. So i decided to give it a try to my Mesos test environment. More documentations of the <a href="https://mesosphere.github.io/marathon/docs/event-bus.html">Event Bus</a> are available in the Documentation page.</p>

<p>Currently, in my test environment all the Mesos and Marathon are installed from the Mesosphere <a href="https://mesosphere.io/learn/install_ubuntu_debian/">Debian</a> packages. Whether we use the packaged <strong>init</strong> or <strong>upstart</strong> script, both of them are directly calling the <code>/usr/local/bin/marathon</code> binary. For the Event Bus with http callback, we need to enable two flags (&#8211;event_subscriber http_callback &#8211;http_endpoints http://host1/foo,http://host2/bar) while starting the Marathon service ie, <em>&#8211;event_subscriber</em>, the type of subscriber that we are going to use and <em>&#8211;http_endpoints</em>, endpoints corresponding to the subscriber.</p>

<p>From the marathon binary, i found the it looks for files under the <code>/etc/marathon/conf/</code> folder, where each file is the <strong>name of the flag</strong> to be enabled and the <strong>content of the file is the flag values</strong>. So in our case, we need two files inside the &#8220;/etc/marathon/conf/&#8221;, 1) <code>event_subscriber</code> and 2) <code>http_endpoints</code></p>

<pre><code>root@vagrant-ubuntu-trusty-64:/etc/marathon/conf# ls /etc/marathon/conf/
event_subscriber  http_endpoints
</code></pre>

<p>And the content of these files are,</p>

<pre><code>root@vagrant-ubuntu-trusty-64:/etc/marathon/conf# cat /etc/marathon/conf/event_subscriber
http_callback

root@vagrant-ubuntu-trusty-64:/etc/marathon/conf# cat /etc/marathon/conf/http_endpoints
http://localhost:1234/
</code></pre>

<p>Now for test purpose, lets start a minimal webserver using <code>netcat</code> listening to port <em>1234</em></p>

<pre><code>$ nc -l -p 1234
</code></pre>

<p>Now netcat is listening on port 1234. Lets restart Marathon with our new flags.</p>

<pre><code>$ service marathon restart
</code></pre>

<p>Now lets check if the flags are enabled properly,</p>

<pre><code>root@vagrant-ubuntu-trusty-64:/etc/marathon/conf# ps axf | grep marathon | grep -v grep
2780 ?        Sl     0:36 java -Xmx512m -Djava.library.path=/usr/local/lib -Djava.util.logging.SimpleFormatter.format=%2$s %5$s%6$s%n -cp /usr/local/bin/marathon mesosphere.marathon.Main --zk zk://localhost:2181,localhost:2182,localhost:2183/marathon --master zk://localhost:2181,localhost:2182,localhost:2183/mesos --http_endpoints http://localhost:1234/ --event_subscriber http_callback
2791 ?        S      0:00  \_ logger -p user.info -t marathon[2780]
2792 ?        S      0:00  \_ logger -p user.notice -t marathon[2780]
</code></pre>

<p>From the above <em>ps</em> command, it can be seen that the flags are enabled properly. Now i&#8217;m going to start a docker container. So as per the Event Bus documentation, Callbacks are Fired every time Marathon receives an API request that modifies an app (create, update, delete)</p>

<pre><code>$ curl -X POST -H "Content-Type: application/json" localhost:8080/v2/apps -d@ubuntu.json
</code></pre>

<p>where <code>ubuntu.json</code> is,</p>

<pre><code>{
    "container": {
    "image": "docker:///libmesos/ubuntu",
    "options" : []
  },
  "id": "ubuntu2",
  "instances": "1",
  "cpus": ".3",
  "mem": "200",
  "uris": [ ],
  "ports": [9999],
  "cmd": "while sleep 10; do date -u +%T; done"
}
</code></pre>

<p>Once the Marathon API is fired, we will get a POST request on our netcat with the Event details. Below is the Event received on my netcat server.</p>

<pre><code>root@vagrant-ubuntu-trusty-64:/var/tmp# nc -l -p 1234
POST / HTTP/1.1
Host: localhost:1234
Accept: application/json
User-Agent: spray-can/1.2.1
Content-Type: application/json; charset=UTF-8
Content-Length: 427

{"clientIp":"127.0.0.1","uri":"/v2/apps","appDefinition":{"id":"ubuntu2","cmd":"while sleep 10; do date -u +%T; done","env":{},"instances":1,"cpus":0.3,"mem":200.0,"disk":0.0,"executor":"","constraints":[],"uris":[],"ports":[9999],"taskRateLimit":1.0,"container":{"image":"docker:///libmesos/ubuntu","options":[]},"healthChecks":[],"version":{"dateTime":{}}},"eventType":"api_post_event","timestamp":"2014-08-25T13:23:27.059Z"}
</code></pre>

<p>This Event bus is really usefull as it notifies us on all the Event changes happening on our Mesos cluster. We can also buit an Event notification system based on these callbacks. Though the current Events has very minimal details, i&#8217;m sure that more Event types will get added soon into Marathon.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/15/managing-docker-cluster-using-multiple-mesos-masters/">Managing Docker Cluster Using Multiple Mesos Masters</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-08-15T16:44:00+00:00" pubdate data-updated="true">Aug 15<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/docker/'>Docker</a>, <a class='category' href='/blog/categories/marathon/'>Marathon</a>, <a class='category' href='/blog/categories/mesos/'>Mesos</a>, <a class='category' href='/blog/categories/zookeeper/'>Zookeeper</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>In my previous <a href="http://beingasysadmin.wordpress.com/2014/06/27/managing-docker-clusters-using-mesos-and-marathon/">blog</a>, i&#8217;ve described how to manage Docker cluster using Mesos and Marathon. It was using a single Mesos Master. But for production, we cannot go with a single Mesos master, as it will result in a sinlge point of failure. Mesos supports Multi master via Zookeeper. So in this blog i&#8217;m going to explain how to setup a Multi Master Mesos Cluster using Zookeper for Automatic promotion of Mesos master when the current Active Mesos Master fails. This time i&#8217;m going to setup 3 Zookeeper services and 2 Mesos Master on a single Vagrant box.</p>

<h1>Setting up ZooKeeper Cluster</h1>

<p>First lets download the the latest stable Zookeeper source code.</p>

<pre><code>$ wget http://mirrors.ukfast.co.uk/sites/ftp.apache.org/zookeeper/stable/zookeeper-3.4.6.tar.gz
</code></pre>

<p>Now extract the tar file and create 3 copies of the same, say zookeeper1, zookeeper2 and zookeeper3. Also Zookeeper needs Java on the machine, so let&#8217;s install java dependencies.</p>

<pre><code>$ apt-get install openjdk-7-jdk openjdk-7-jre
</code></pre>

<p>Now inside, the extracted zookeeper source folder, we need to create a config file. So in our case, inside each Zookeeper folder, we need a <code>zoo.cfg</code> file.</p>

<p>For zookeeper1 folder,</p>

<pre><code># content of zoo.cfg
tickTime=2000
dataDir=/var/lib/zookeeper1/
clientPort=2181
initLimit=5
syncLimit=2
server.1=localhost:2888:3888
server.2=localhost:2889:3889
server.3=localhost:2890:3890
</code></pre>

<p>For zookeeper2 folder,</p>

<pre><code># content of zoo.cfg
tickTime=2000
dataDir=/var/lib/zookeeper2/
clientPort=2182
initLimit=5
syncLimit=2
server.1=localhost:2888:3888
server.2=localhost:2889:3889
server.3=localhost:2890:3890
</code></pre>

<p>For zookeeper3 folder,</p>

<pre><code>    # content of zoo.cfg
tickTime=2000
dataDir=/var/lib/zookeeper3/
clientPort=2183
initLimit=5
syncLimit=2
server.1=localhost:2888:3888
server.2=localhost:2889:3889
server.3=localhost:2890:3890
</code></pre>

<p>Here, since the 3 zookeeper services are running on the same host, i&#8217;ve assigned separate ports. If the zookeeper are running on separate instances, then we can have the same ports for all of the zookeeper nodes. There are two port numbers. The first followers use to connect to the leader, and the second is for leader election.</p>

<p>Now we can start the Zookeeper in Foreground using the <code>./bin/zkServer.sh start-foreground</code> from each of the 3 zookeeper folders. <code>netstat -nltp | grep 288</code> will display the port of the zookeeper service which is the current master among the cluster. We can also check the connectivity using the zookeepr client binary available in the zookeeper source folder. Once zookeeper cluster is UP, we can go ahead setting up Mesos.</p>

<h1>Setting up Multiple Mesos Masters</h1>

<p>Mesossphere team has already built packages for Mesos,Marathon and Deimos. So one of my Mesos master will be from the package. But i also wanted to play with the Mesos Source, so i decided to build Mesos from the source. So my second Mesos Master will be from the scratch.</p>

<p>First Master from the Mesosphere package.</p>

<pre><code>$ apt-key adv --keyserver keyserver.ubuntu.com --recv E56151BF

$ DISTRO=$(lsb_release -is | tr '[:upper:]' '[:lower:]')

$ CODENAME=$(lsb_release -cs)

# Add the repository
$ echo "deb http://repos.mesosphere.io/${DISTRO} ${CODENAME} main" | sudo tee /etc/apt/sources.list.d/mesosphere.list

$ apt-get -y update

$ apt-get -y install mesos marathon deimos
</code></pre>

<p>Now we need to define the Zookeeper cluster details so that Mesos master and slave can connect. Edit <code>/etc/mesos/zk</code> and add <code>zk://localhost:2181,localhost:2182,localhost:2183/mesos</code>. More details about Zookeeper URL is available <a href="https://github.com/deepakmdass88/mesos-doc-test/blob/master/Using-ZooKeeper.textile">here</a>. Also More details about installing Mesos from Mesosphere&#8217;s Debian package is available in their <a href="http://mesosphere.io/docs/getting-started/debian-install/">website</a>.</p>

<p>So now we have one master ready, we can start the Mesos Master, Mesos Slave and Marathon. ANd esnure that they can connect to our Zookeeper cluster properly. We can also see the connection status in the stdout of the zookeeper process as they are running in foreground.</p>

<p>Now building the second Mesos master from scratch. First let&#8217;s download the Mesos Source Code.</p>

<pre><code>$ wget http://archive.apache.org/dist/mesos/0.19.0/mesos-0.19.0.tar.gz

$ tar xvzf mesos-0.19.0.tar.gz &amp;&amp; cd mesos-0.19.0

$ ./bootstrap &amp;&amp; ./configure --prefix=/usr/local/mesos

$ make &amp;&amp; make install
</code></pre>

<p>Once the Compilation is succeeded, we can start the new Mesos Master service. The default port 5050 is used by the existing master, so we need to run this new service on a different port.</p>

<pre><code>$ /usr/local/mesos/sbin/mesos-master --zk=zk://localhost:2181,localhost:2182,localhost:2183/mesos --port=5054 --quorum=1 --registry=in_memory --work_dir=/var/lib/mesos/
</code></pre>

<p>Once the new Mesos Master is up, we can create a test container via Marathon Rest API. Create a simple json file called ubuntu.json</p>

<pre><code>{
    "container": {
    "image": "docker:///libmesos/ubuntu",
    "options" : []
  },
  "id": "ubuntu",
  "instances": "1",
  "cpus": ".3",
  "mem": "200",
  "uris": [ ],
  "cmd": "while sleep 10; do date -u +%T; done"
}


$ curl -X POST -H "Content-Type: application/json" localhost:8080/v2/apps -d@ubuntu.json
</code></pre>

<p>This will launch a single container. We can check the status of the container via the Marathon UI as well as via RestAPI also. Now comes the critical part for Production, What happens when the master fails. Mesos Documentation says, if the current master fails, in a Multi master Mesos Cluster, Zookeeper will elect a new Master, in our case the second Mesos master service. And Mesos Claims that the running services will not get crashed and the status will be taken by the newly promoted master. In our case, we have a Docker container running as a long running service.</p>

<p><img src="https://beingasysadmin.files.wordpress.com/2014/06/marathon-1.png?w=6400"></p>

<p>First i&#8217;ll stop one of the Mesos master service,</p>

<pre><code>$ service mesos-master stop
</code></pre>

<p>Now immidiately on the stdout of the Zookeeper, we can see the logs corresponding to the Election process. Below is the same,</p>

<pre><code>I0814 19:41:07.889044  9093 contender.cpp:243] New candidate (id='7') has entered the contest for leadership
2014-08-14 19:41:07,896:9088(0x7ff8e2ffd700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:2183], sessionId=0x347d606d0d90003, negotiated timeout=10000
I0814 19:41:07.899509  9092 group.cpp:310] Group process ((10)@10.0.2.15:5054) connected to ZooKeeper
I0814 19:41:07.900049  9092 group.cpp:784] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0814 19:41:07.900069  9092 group.cpp:382] Trying to create path '/mesos' in ZooKeeper
I0814 19:41:07.915621  9092 detector.cpp:135] Detected a new leader: (id='6')
I0814 19:41:07.915699  9092 group.cpp:655] Trying to get '/mesos/info_0000000006' in ZooKeeper
I0814 19:41:07.920266  9094 network.hpp:423] ZooKeeper group memberships changed
I0814 19:41:07.920910  9094 group.cpp:655] Trying to get '/mesos/log_replicas/0000000003' in ZooKeeper
I0814 19:41:07.924144  9095 network.hpp:461] ZooKeeper group PIDs: { log-replica(1)@10.0.2.15:5054 }
I0814 19:41:07.926575  9092 detector.cpp:377] A new leading master (UPID=master@10.0.2.15:5054) is detected
I0814 19:41:07.926679  9092 master.cpp:957] The newly elected leader is master@10.0.2.15:5054 with id 20140814-194033-251789322-5050-8808
</code></pre>

<p>As per the above logs, the second Mesos Master running at port 5054 was promoted as New Mesos Master. This will be reflected to Marathon also. We can go to the Marthon UI and make sure that the Docker process that we have started using the old Mesos master is still running fine. We can even try making some scaling changes and can make sure that the new master can alter the process.</p>

<p>In my testing, i&#8217;ve tried stoping each of the service and ensured that only one Mesos master is running and also tried scaling the Apps from one Master service to make sure that the new Master was able to keep track of all these changes. The results were quite promising. Mesos + Marrthon + Docker indeed is killer combo. We can really built a cross vendor independent cluster with scaling capabilites.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/07/24/load-test-on-docker-freeswitch/">Load Test on Docker Freeswitch</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-07-24T11:45:00+00:00" pubdate data-updated="true">Jul 24<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/docker/'>docker</a>, <a class='category' href='/blog/categories/freeswitch/'>freeswitch</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p><code>Docker</code> is a very powerfull tool for managing Linux containers. In my previous <a href="http://beingasysadmin.wordpress.com/2014/06/16/dockerizing-freeswitch-docker-enters-telephony-world/">blog</a> i&#8217;ve explaind on how to setup a Docker Freeswitch. Docker is very mature now, version 1.0 has already been released. Docker is now supported by all major cloud vendors. Docker was showing promising results when i was performing my initial testing. So this time i decided to perform a heavy load test on the Freeswitch container to ensure that Docker can really enter Telephony. Like any normal sys admin, i was googling for Freeswitch load test, and most of the results were pointing to Sipp, an Open Source test tool / traffic generator for the SIP protocol. For me Sipp didnt helped me as it started throwing errors beyond 320 simultaneous calls. The UDP connections were timing out. I tried increasing the timeout, which didn&#8217;t helped much.</p>

<p>So next choice is to use a Freeswitch itself, to generate calls. Using the FreeSwitch&#8217;s <em>originate</em> command to generate simultaneous calls and hit the Docker Freeswitch container. I also decided to collect all system metrics, so that i knows how the machine behaves under various load tests conditions. For this i deciced to use <em>CollectD</em> and <em>Graphite</em> combo. Collectd 5+ has an inbuild graphite plugin which can send the collectd metrics to a graphite server.</p>

<p>I&#8217;ve already setup an Ubuntu-Freeswitch Docker <a href="https://registry.hub.docker.com/u/deepakmdass88/fs-ubuntu/">image</a>. First we need to pull the images from the Docker hub.</p>

<pre><code>$ docker pull deepakmdass88/fs-ubuntu
</code></pre>

<p>Now i&#8217;m going to start the Docker FreeSwitch container in foreground.</p>

<pre><code>$ docker run --rm --privilieged -i -t -p 5060:5060/tcp -p 5060:5060/udp -p 16384:16384/udp -p 16385:16385/udp -p 16386:16386/udp -p 16387:16387/udp -p 16388:16388/udp -p 16389:16389/udp -p 16390:16390/udp -p 16391:16391/udp -p 16392:16392/udp -p 16393:16393/udp -p 5080:5080/tcp -p 5080:5080/udp deepakmdass88/fs-ubuntu /bin/bash
</code></pre>

<p>The <code>privilieged</code> option was enabled because, the FreeSwitch init script sets some custom <strong>ulimit</strong> values, so the container has to be given special privileges. Corresponding SIP and RTP ports are forwarded from the host to the container.</p>

<p>Now before starting the Freeswitch service, we can set up the CollectD agent. By default, the Ubuntu repostiry contains CollectD versio 4.10, but the Graphite plugin is available from version 5.0+ onwards. So we can use somne PPA which has the corresponding version available.</p>

<pre><code>$ apt-get install python-software-properties

$ add-apt-repository ppa:joey-imbasciano/collectd5

$ apt-get update &amp;&amp; apt-get install collectd
</code></pre>

<p>Now in the <code>/etc/collectd.conf</code>, uncomment <em>LoadPlugin write_graphite</em>. Also, in the same file and uncomment the plugin definition and fill in the server details.</p>

<pre><code>&lt;Plugin write_graphite&gt;
  &lt;Carbon&gt;
        Host "dockergraphite.example.com"
        Port "2003"
        Protocol "tcp"
        LogSendErrors true
        Prefix "collectd."
        StoreRates true
        AlwaysAppendDS false
        EscapeCharacter "_"
  &lt;/Carbon&gt;
&lt;/Plugin&gt;
</code></pre>

<p>I&#8217;ve enabled a custom <a href="https://github.com/ReadyTalk/freeswitch-collectd-plugin">freeswitch</a> plugin, which will extract the current ongoing calls count from freeswitch and sends it to the graphite server. Once the config changes are done we can restart the CollectD service. Now we can check our graphite UI to see if the default metrics like memory, load, cpu etc. are reaching the graphite server. Once CollectD-Graphite setup is ready, we can go ahead with our load test. So, once the call has reached the server, we need some <em>Dialplan</em> to continue the calls. So the simplest method is to create an infinite loop of playing some file, or some conference. Below are some dialplans that i&#8217;ve created in the <code>public.xml</code></p>

<pre><code># Infinite Play Loop

 &lt;extension name="111222333"&gt;
       &lt;condition field="destination_number" expression="^111222333$"&gt;
         &lt;action application="answer"/&gt;
         &lt;action application="playback" data="sounds/music/8000/got.wav"/&gt;
         &lt;action application="transfer" data="111222333 XML public"/&gt;
       &lt;/condition&gt;
    &lt;/extension&gt;

# Test conference

  &lt;extension name="docker-fs-test-conf"&gt;
    &lt;condition field="destination_number" expression="^112233"&gt;
      &lt;action application="answer"/&gt;
      &lt;action application="sleep" data="500"/&gt;
      &lt;action application="conference" data="docker-test@public"/&gt;
    &lt;/condition&gt;
  &lt;/extension&gt;


# Default IVR menu

    &lt;extension name="ivr_demo"&gt;
      &lt;condition field="destination_number" expression="^5000$"&gt;
        &lt;action application="answer"/&gt;
        &lt;action application="sleep" data="2000"/&gt;
        &lt;action application="ivr" data="demo_ivr"/&gt;
      &lt;/condition&gt;
    &lt;/extension&gt;
</code></pre>

<p>Now, we have the dialplans ready, next is authentication. By default there are two ways, Digest auth and IP Whitelist. Here i&#8217;m going to use IP whitelist, so we need to whitelist our IP in the acl.conf file.</p>

<pre><code> &lt;list name="domains" default="deny"&gt;
      &lt;!-- domain= is special it scans the domain from the directory to build the ACL --&gt;
      &lt;node type="allow" domain="$${domain}"/&gt;
      &lt;node type="allow" cidr="xxx.xxx.xxx.xxx/32"/&gt;                 # IP of FS from which we are going to send the calls
      &lt;!-- use cidr= if you wish to allow ip ranges to this domains acl. --&gt;
      &lt;!-- &lt;node type="allow" cidr="192.168.0.0/24"/&gt; --&gt;
 &lt;/list&gt;
</code></pre>

<p>Now we can start the Freeswitch service.</p>

<pre><code>$ /etc/init.d/freeswitch start
</code></pre>

<p>We can check the freeswich service using the fs_cli command.</p>

<pre><code>$ /usr/local/freeswitch/bin/fs_cli -x "show status"

UP 0 years, 0 days, 6 hours, 34 minutes, 59 seconds, 648 milliseconds, 56 microseconds
FreeSWITCH (Version 1.5.13b git 39200cd 2014-07-02 21:55:21Z 64bit) is ready
1068 session(s) since startup
0 session(s) - peak 299, last 5min 0
0 session(s) per Sec out of max 30, peak 29, last 5min 0
1000 session(s) max
min idle cpu 0.00/100.00
Current Stack Size/Max 240K/8192K
</code></pre>

<p>Now freeswitch is ready to accept the connection. We can start sending the calls from our Load test freeswitch. Below is the script that was used to originate the calls from the load test Freeswitch machine. This will create simultaneous calls towards the Docker FS.</p>

<pre><code>IP_URI="sip:111222333@&lt;docker-fs-ip&gt;:5060"
MAX_CALLS=$1

while [ 1 ]; do

set -i req
req=$(/usr/local/freeswitch/bin/fs_cli -q -b -x "show channels count" | awk '{print $1}')
if [ $req -lt $MAX_CALLS ]; then
    /usr/local/freeswitch/bin/fs_cli -q -b -x "bgapi originate sofia/external/$SIP_URI loadtest"
else
    echo "sleep a bit ..."
    sleep 10s

fi
</code></pre>

<p>While bulk calls are being made from the Load test freeswitch machines, to test the Quality in real time, it&#8217;s better to dial to the extension directly from a Sip Phone/Client and ensure that voice quality is good. Below is my Graphite dashboard for the load test.</p>

<p>Default Graphite UI</p>

<p><img src="/images/Docker-FS-Loadtest.png"></p>

<p><a href="https://github.com/urbanairship/tessera">Tessera</a> UI</p>

<p><img src="/images/tessera-graphite.png"></p>

<p>The FS was stable till 500 simultaneous calls, after that there was a sudden drop in calls and also the voice quality started dropping and in a minute the Freeswitch crashed due to Segmentation fault. I&#8217;m  going to analyze the core dump file to understand more about the crash. The other smaller drops that we see in the graph was caused by the Load test Freeswitch machine, as the load was getting high when the number of calls was increased. But 500 simultaneous calls are pretty decent and the there was no issue in voice quality till the number of calls crossed 500. Though it&#8217;s very difficult to make a final confirmation, i decided to go ahead with phase 2 load test.</p>

<p>In the phase 2 test, i&#8217;m planning to use multiple FS load test machines to generate large simultaneous calls + running 2 separate FS containers on the same host and split the incoming calls to both these containers. Once the phase 2 test is completed, ill share the test results in an another blog post. Docker is still under heavy development, and i&#8217;m sure Docker will be entering Telephony soon.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/06/27/managing-docker-clusters-using-mesos-and-marathon/">Managing Docker Clusters Using Mesos and Marathon</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-06-27T07:31:00+00:00" pubdate data-updated="true">Jun 27<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/containerization/'>containerization</a>, <a class='category' href='/blog/categories/docker/'>docker</a>, <a class='category' href='/blog/categories/docker-cluster/'>docker-cluster</a>, <a class='category' href='/blog/categories/marathon/'>marathon</a>, <a class='category' href='/blog/categories/mesos/'>mesos</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p><code>Docker</code> has became one of my favourite tool. It&#8217;s super cool and super easy tool to manage linux containers. LXC&#8217;s are around in IT world for some time, but by the entry of Docker last year, the wave started rising. Thanks to <a href="https://github.com/dotcloud/docker">Docker</a> team and <a href="https://twitter.com/solomonstre">Solomon Hykes</a> for open sourcing such a wonderfull project. I&#8217;ve already mentioned a lot of stuffs about Docker in my previos blogs, so today im going explain how Docker can be used as a Cluster. There are some interesting tools like <a href="https://coreos.com/">CoreOS</a>, <a href="https://github.com/spotify/helios">Helios</a> etc for managing Docker as a cluster. But today i&#8217;m going to explain on how to set up a Docker cluster using Apache <a href="http://mesos.apache.org/">Mesos</a>. CoreOS is a custom linux os which comes with SystemD. But the restriction is, we have to use that custom images of coreos.. Indeed CoreOS team open sourced some exciting tools like etcd fleet which works with CoreOS for managing Docker clusters. But Mesos is quite simple, we can install it via package, or even using tar balls available in thier Github repo onto most of the Linux Distro&#8217;s and it&#8217;s quite easy to configure also. Mesos is heavily used by Twitter to manage their data center&#8217;s. And now <a href="http://mesosphere.io/">Mesosphere</a> has opensourced a new tool called <a href="https://github.com/mesosphere/marathon">Mararthon</a> which now provides a UI and a Rest API for maaging and scheduling Mesos Frameworks aka jobs, in this case containers as a service.</p>

<p>A few weeks ago, Mesos 0.19 was released which comes with an official support for Docker coantiners by intergrating <a href="https://github.com/mesosphere/deimos">Deimos</a> into it. And a few days ago Marathon has released their new version 0.6.0 supports launching any task in a Docker container via Mesos 0.19+</p>

<h2>Setting up Mesos Cluster</h2>

<p>In this test setup, i&#8217;m going to setup both Mesos master/slave and Zookeeper on the same Ubuntu 14.04 vagrant node. First we can install the dependencies,</p>

<pre><code>$ apt-get install curl python-setuptools python-pip python-dev python-protobuf
</code></pre>

<p>Now we can install Zookeeper</p>

<pre><code>$ apt-get install zookeeperd
</code></pre>

<p>After the installation, ZooKeeper has 1 configuration. Each Zookeeper needs to know its position in the quorum.</p>

<pre><code>$ echo 1 | sudo dd of=/var/lib/zookeeper/myid
</code></pre>

<p>Now we can setup Docker</p>

<pre><code>$ echo "deb http://get.docker.io/ubuntu docker main" &gt; /etc/apt/sources.list.d/docker.list

$ apt-get update &amp;&amp; apt-get install lxc-docker

$ docker version

   Client version: 1.0.0
   Client API version: 1.12
   Go version (client): go1.2.1
   Git commit (client): 63fe64c
   Server version: 1.0.0
   Server API version: 1.12
   Go version (server): go1.2.1
   Git commit (server): 63fe64c
</code></pre>

<p>Let&#8217;s pull some basic ubuntu images from Docker Hub so that we can use the same for testing.</p>

<pre><code>$ docker pull libmesos/ubuntu
</code></pre>

<p>Now we can configure Mesos</p>

<pre><code>$ curl -fL http://downloads.mesosphere.io/master/ubuntu/14.04/mesos_0.19.0~ubuntu14.04%2B1_amd64.deb -o /tmp/mesos.deb

$ dpkg -i /tmp/mesos.deb

$ mkdir -p /etc/mesos-master

$ echo in_memory | sudo dd of=/etc/mesos-master/registry

## Mesos Python egg for use in authoring frameworks

$ curl -fL http://downloads.mesosphere.io/master/ubuntu/14.04/mesos-0.19.0_rc2-py2.7-linux-x86_64.egg -o /tmp/mesos.egg

$ easy_install /tmp/mesos.egg
</code></pre>

<p>We can download the latest Marathon 0.6 from <a href="http://downloads.mesosphere.io/marathon/marathon-0.6.0/marathon-0.6.0.tgz">here</a></p>

<pre><code>$ tar xvzf marathon-0.6.0.tgz
</code></pre>

<p>Mesos uses Deimos for managing dockers, Deimos can installed via pip</p>

<pre><code>$ pip install deimos
</code></pre>

<p>Also, we need to configure mesos to use Deimos,</p>

<pre><code>$ mkdir -p /etc/mesos-slave

$ echo /usr/local/bin/deimos | sudo dd of=/etc/mesos-slave/containerizer_path

$ echo external | sudo dd of=/etc/mesos-slave/isolation
</code></pre>

<p>Now we can start all the services.</p>

<pre><code>$ initctl reload-configuration

$ service docker start

$ service zookeeper start

$ service mesos-master start

$ service mesos-slave start

##### Starting Marathon #####

$ cd marathon-0.6.0

$ ./bin/start --master zk://localhost:2181/mesos --zk_hosts localhost:2181
</code></pre>

<p>Marathon will now start listening to port <em>8080</em>, We can access the UI from the browser via this port, also via rest API using the same port.</p>

<pre><code>curl localhost:8080/help   # gives us some details about the API's
</code></pre>

<p>I just went through the Deimos code, so under the hood they are using <code>docker run</code> with some default parameters like <code>--sig-proxy</code>,  <code>--rm</code>,  <code>--cidfile</code>,  <code>-v</code>, <code>-w</code> and extra parameters that we are passing while creating the task via Marathon.</p>

<p>As of now, we still can&#8217;t pass details like Container image, Docker options via Marathon GUI. So we can use the Rest API for the time being. Below is a sample curl request for launcing a single container,</p>

<pre><code>curl -X POST -H "Accept: application/json" -H "Content-Type: application/json" \
    localhost:8080/v2/apps -d '{
        "container": {"image": "docker:///libmesos/ubuntu", "options": ["--privileged"]},
        "cpus": 0.5,
        "cmd": "sleep 500",
        "id": "docker-tester",
        "instances": 1,
        "mem": 300
    }'
</code></pre>

<p>We can pass custom options to the docker run command via &#8220;options&#8221;. After making the curl request, we can check the syslog, as mesos will be logging into syslog by default. We can even see the Docker run command on the same.</p>

<pre><code>Jun 27 07:24:58 vagrant-ubuntu-trusty-64 deimos[19227]: deimos.containerizer.docker.launch() exit 0 // docker run --sig-proxy --rm --cidfile /tmp/deimos/mesos/00d459fb-22ca-4af7-9a97-ef8a510905f2/cid -w /tmp/mesos-sandbox -v /tmp/deimos/mesos/00d459fb-22ca-4af7-9a97-ef8a510905f2/fs:/tmp/mesos-sandbox --privileged -p 31498:31498 -c 512 -m 300m -e PORT=31498 -e PORT0=31498 -e PORTS=31498 libmesos/ubuntu sh -c 'sleep 500'
</code></pre>

<p>We can also use the Marathon Rest API to check the status of the job which we started.</p>

<pre><code>curl -X GET -H "Content-Type: application/json" localhost:8080/v2/apps
</code></pre>

<p>Below is the screenshort for the same from the Marathon UI.</p>

<p><img src="/images/marathon1.png"></p>

<p>We can also check if the container is launched via <code>docker ps</code> command.</p>

<p><img src="/images/docker1.png"></p>

<p>A more detailed report about the Docker job which we have launched can be viewed via the default Mesos GUI listening on port <em>5050</em> on the Mesos master. Now we can test the scalability of the Job. Currently we have only one container running. So now we can try scaling say adding one more node. We can do it in two ways, like via PUT request using curl or using GUI</p>

<pre><code>curl -X PUT -H "Content-Type: application/json" localhost:8080/v2/apps/docker-tester \
    "container": {"image": "docker:///libmesos/ubuntu", "options": ["--privileged"]},
            "cpus": 0.5,
            "cmd": "sleep 500",
            "id": "docker-tester",
            "instances": 2,     # increasing the instance count to 2
            "mem": 300
            }'
</code></pre>

<p>Now we can use the <code>docker ps</code> command to see if the new container is launched or not. Also we can see that status in UI also.</p>

<p><img src="/images/docker2.png"></p>

<p><img src="/images/marathon2.png"></p>

<p>Similarly, we can scale down also. I&#8217;ve tested the same and all seems to be good. Marathon ensures that the docker process will be running. So incase if the process crashes Marathon will restart the same and ensures that the instances are up and running as per our configuration. There are a few other Open Sourced Mesos Scheduler&#8217;s  like Apache Aurora, Airbnb&#8217;s Chronos. But for my requirement marathon is pretty straight and simple and also provides a very good Rest API layer for managing containers. Mesos, Marathon and Docker are still young, but provides a killer combination for managing clusters built over Docker containers.</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/06/10/dockerizing-freeswitch-docker-enters-telephony/">Dockerizing Freeswitch - Docker Enters Telephony</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-06-10T11:39:00+00:00" pubdate data-updated="true">Jun 10<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/docker/'>Docker</a>, <a class='category' href='/blog/categories/freeswitch/'>FreeSwitch</a>, <a class='category' href='/blog/categories/telephony/'>Telephony</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p><code>Docker</code> has became one of the hottest topics in IT now a days. <code>Docker</code> is an open-source project that automates the deployment of applications inside software containers. Docker extends a common container format called <em>Linux Containers</em> (LXC), with a high-level API providing lightweight virtualization that runs processes in isolation.Docker uses <em>LXC, cgroups,</em> and the <em>Linux kernel</em> itself. Though i coudn&#8217;t make out to the DockerCon 2014 in SF, a lot new developments were announced on the DockerCon. Especially three new Opensource Projects <a href="https://github.com/docker/libcontainer">libcontainer</a>, <a href="https://github.com/docker/libchan">libchan</a> and <a href="https://github.com/docker/libswarm">libswarn</a>. Docker is indeed creating a revolution in the container space, creating a next generation of scalable platform management. There are a lot PAAS services like <a href="http://deis.io/">Deis</a>, <a href="http://signup.resin.io/">resin.io</a>, <a href="https://github.com/progrium/dokku">Dokku</a> which are already using Docker in production. Another important and exciting project is <a href="https://coreos.com/">CoreOS</a>. CoreOS uses tools like <a href="http://www.freedesktop.org/wiki/Software/systemd/">SystemD</a>, <a href="https://github.com/coreos/fleet">Fleet</a>, <a href="https://github.com/coreos/etcd">EtcD</a> to build a fully scalabale docker based cluster management system. I definitely need a separate blog to write about CoreOS, it&#8217;s really a super exciting project to play with.</p>

<p>Last week Docker Team released Version 1.0 of Docker. So i&#8217;ll be using the same in this new set up. It&#8217;s been almost 6 Month&#8217;s since i&#8217;ve been working @ <a href="http://plivo.com">Plivo</a> as a DevOps Engineer. Telephony was really a very new platform for me. And my first companion was offcourse <a href="http://freeswitch.org/">FreeSwitch</a>,a scalable open source cross-platform telephony platform designed to route and interconnect popular communication protocols using audio, video, text or any other form of media. I was heavily using Vagrant for all my experiments in my mac. But after started using Docker, it really made me crazy. I&#8217;ve played for some time wiht LXC&#8217;s long back. So this was like a leap back to the container world.</p>

<p>There are a lot of concerns on using Virtual Machines in Telephony world. Especially for the server&#8217;s that handles the Real Time voice packets, as voice quality is pretty important in Telephony. Docker&#8217;s again more light weight isolated environment, and i decide to see how Docker can perform with such issues. If Docker handle Freeswitch smoothly, then i&#8217;m sure that we can use Docker for other telephony app&#8217;s like OpenSIPS/Kamailio etc, as they handle only sessions not the Media traffic. I know there are a lot of concerns like CPU load, Network etc, but this is like an initial move to test Docker into Telephony.</p>

<h2>Setting Up Docker </h2>

<p>Docker 1.0 is available from the Official Docker repo.</p>

<pre><code>$ echo "deb http://get.docker.io/ubuntu docker main" &gt; /etc/apt/sources.list.d/docker.list

$ apt-get update &amp;&amp; apt-get install lxc-docker
</code></pre>

<p>Now we can check the Docker version using the docker binary itself.</p>

<pre><code>$ docker version

Client version: 1.0.0
Client API version: 1.12
Go version (client): go1.2.1
Git commit (client): 63fe64c
Server version: 1.0.0
Server API version: 1.12
Go version (server): go1.2.1
Git commit (server): 63fe64c
</code></pre>

<p>Now Docker is installed, but we need some OS images to use with docker. We can build custom images using debootstrap etc. But there are official minimal images available in Docker <a href="https://registry.hub.docker.com/">HUB</a>. We can search for the repositories and can pull those images via docker binary itself.</p>

<p>For example to pull the entire Ubuntu images, we can just do,</p>

<pre><code>$ docker pull ubuntu
</code></pre>

<p>But this will download all the ubuntu images available in the repo. We can also do selective download by using the tag.</p>

<pre><code>$ docker pull ubuntu:14:04
</code></pre>

<p>Once the images are downloaded, we can use images option in docker binary to see all the downloaded images.</p>

<pre><code>$ docker images

REPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
ubuntu                          14.04               ad892dd21d60        10 days ago         275.5 MB
</code></pre>

<p>Here i&#8217;m not going to daemonize the container, i&#8217;ll be using the interactive option. But first, let&#8217;s start a new container.</p>

<pre><code>$ docker run -t -i ubuntu:14.04 /bin/bash
</code></pre>

<p>This command will start a conatiner and will open up a bash session for us and we will be inside the bash session. Now to use an application we need to open up corresponding ports to outside world. We can use the &#8220;-p&#8221; option while starting a docker container to enable port forwarding. Under the hood, docker is using IPtables for the same. In the case of Freeswitch, we need to open 5060,5080 for the default Sofia profiles (Internal and External). Also we need to open the RTP ports. In this test i&#8217;ll be opening a predefined set of ports ie from &#8220;16384&#8221; to &#8220;16394&#8221;. (As my Docker host resides on Azure, creating an Endpoint for each port forward is really a pain, so i decided to open only a few). And also i&#8217;ll be opening port 22, so that we can have an ssh server inside the container.</p>

<pre><code>$ docker run -t -i -p 2223:22 -p 5060:5060/tcp -p 5060:5060/udp -p 16384:16384/udp -p 16385:16385/udp -p 16386:16386/udp -p 16387:16387/udp -p 16388:16388/udp -p 16389:16389/udp -p 16390:16390/udp -p 16391:16391/udp -p 16392:16392/udp -p 16393:16393/udp -p 5080:5080/tcp -p 5080:5080/udp ubuntu:14.04 /bin/bash
</code></pre>

<p>This will start a new container and Docker by default will setup the IPtables for port forwarding. So now my IPtables looks like this.</p>

<pre><code>Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
   43 16850 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:5080
    0     0 ACCEPT     tcp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           tcp dpt:5080
  988  198K ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16392
    0     0 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16389
    0     0 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16385
    0     0 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16393
 2026  405K ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16388
 8817 1763K ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16384
12144 8684K ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:5060
 4359  257K ACCEPT     tcp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           tcp dpt:5060
 9917 1983K ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16390
    0     0 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16387
    0     0 ACCEPT     tcp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           tcp dpt:22
   38  4848 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16391
    1   152 ACCEPT     udp  --  !docker0 docker0  0.0.0.0/0            172.17.0.6           udp dpt:16386
    0     0 ACCEPT     all  --  *      lxcbr0  0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  --  lxcbr0 *       0.0.0.0/0            0.0.0.0/0
 431K  630M ACCEPT     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
 128K   19M ACCEPT     all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0
   16  2460 ACCEPT     all  --  docker0 docker0  0.0.0.0/0            0.0.0.0/0
</code></pre>

<p>Now we can go ahead with Freeswitch compilation. In my previous <a href="http://beingasysadmin.com/blog/2014/02/14/sip-trunking-using-plivo-and-freeswitch/">blog</a>, i&#8217;ve mentioned how to compile and set up freeswitch. Once freeswitch is ready, we need to make a few changes. By default, Freeswitch uses STUN to route through NAT, but this doesn&#8217;t work with Docker. So we have to set the external IP manually. In the Freeswitch installed folder, edit <code>conf/autoload_configs/switch.conf.xml</code>. In this file we can set the External IP manually. Add the below lines to switch_conf.xml.</p>

<pre><code>&lt;X-PRE-PROCESS cmd="set" data="external_sip_ip=&lt;YOUR_EXTERNAL_IP&gt;"/&gt;
&lt;X-PRE-PROCESS cmd="set" data="external_rtp_ip=&lt;YOUR_EXTERNAL_IP&gt;"/&gt;
</code></pre>

<p>Also we need to modify the Default Sofia Profiles and need to set the <em>ext-rtp-ip</em> and <em>ext-sip-ip</em> to use our external IP added in the <code>switch_conf.xml</code> file while establishing connections. Add the below lines to the <code>conf/sip_profiles/internal.xml</code> and <code>conf/sip_profiles/external.xml</code></p>

<pre><code>&lt;param name="ext-rtp-ip" value="$${external_rtp_ip}"/&gt;
&lt;param name="ext-sip-ip" value="$${external_sip_ip}"/&gt;
</code></pre>

<p>Now we need to set teh RTP ip range to the range which we have forwarded while creting the container. So we need to edit <code>conf/autoload_configs/switch.conf.xml</code></p>

<pre><code>&lt;param name="rtp-start-port" value="16384"/&gt;
&lt;param name="rtp-end-port" value="16394"/&gt;
</code></pre>

<p>Once the changes are made, we can start the FreeSwitch service. Now to make sure that the External IP is working properly, we can check the sofia profile status using fs_cli. below is a sample output of the sofia profile status.</p>

<pre><code>freeswitch@internal&gt; sofia status profile internal
=================================================================================================
Name                internal
Domain Name         N/A
Auto-NAT            false
DBName              sofia_reg_internal
Pres Hosts          172.17.0.6,172.17.0.6
Dialplan            XML
Context             public
Challenge Realm     auto_from
RTP-IP              172.17.0.6
Ext-RTP-IP          &lt;my_external_ip&gt;
SIP-IP              172.17.0.6
Ext-SIP-IP          &lt;my_external_ip&gt;
URL                 sip:mod_sofia@&lt;my_external_ip&gt;:5060
BIND-URL            sip:mod_sofia@&lt;my_external_ip&gt;:5060;maddr=172.17.0.6;transport=udp,tcp
HOLD-MUSIC          local_stream://moh
OUTBOUND-PROXY      N/A
CODECS IN           OPUS,G722,PCMU,PCMA,GSM
CODECS OUT          OPUS,G722,PCMU,PCMA,GSM
TEL-EVENT           101
DTMF-MODE           rfc2833
CNG                 13
SESSION-TO          0
MAX-DIALOG          0
NOMEDIA             false
LATE-NEG            true
PROXY-MEDIA         false
ZRTP-PASSTHRU       true
AGGRESSIVENAT       false
CALLS-IN            0
FAILED-CALLS-IN     0
CALLS-OUT           0
FAILED-CALLS-OUT    0
REGISTRATIONS       1
</code></pre>

<p>Now freeswitch ahs started successfully. We can test some basic calls using softphones like Xlite, Telephone etc. By default, there are some default extensions and user&#8217;s available, so we can use the same for testing the calls. But i really wanted to try trunkning also and wanted to see the quality of the voice. So i created <a href="http://beingasysadmin.com/blog/2014/02/14/sip-trunking-using-plivo-and-freeswitch/">SIP trunking in Freeswitch using Plivo</a>. And i tested a couple of calls to US and India DID&#8217;s and no issues were detected in the quality. But again i need to test the laod of the server&#8217;s when it startes handling concurrent calls and also the voice quality. But i decied to d oit as a Phase II. But as of now, Docker FreeSwitch is working perfectly like a physical machine with out issues.</p>

<p>So now we have a working FreeSwitch container, now here comes the main advantage of the Docker. We can create a new image with all these changes, so that nex time i dont need to work from scratch. I can use this saved image and a readymade Docker Freeswitch container can be launched in seconds. Since we are in interactive mode, we should not quit the session before it&#8217;s saved or else all the things will be lost,becoz dokcer will destroy the same. So open up a new shell on the docker host and use the commit option. But to use the commit command, we need to know the container id, so here docker ps command comes handy.</p>

<pre><code>$ docker ps

CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS           NAMES

e7f3c02346d4        a4196763d248        /bin/bash           32 hours ago        Up 32 hours         0.0.0.0:2223-&gt;22/tcp, 0.0.0.0:5060-&gt;5060/tcp, 0.0.0.0:5060-&gt;5060/udp, 0.0.0.0:5080-&gt;5080/tcp, 0.0.0.0:5080-&gt;5080/udp, 0.0.0.0:16384-&gt;16384/udp, 0.0.0.0:16385-&gt;16385/udp, 0.0.0.0:16386-&gt;16386/udp, 0.0.0.0:16387-&gt;16387/udp, 0.0.0.0:16388-&gt;16388/udp, 0.0.0.0:16389-&gt;16389/udp, 0.0.0.0:16390-&gt;16390/udp, 0.0.0.0:16391-&gt;16391/udp, 0.0.0.0:16392-&gt;16392/udp, 0.0.0.0:16393-&gt;16393/udp   silly_turing
</code></pre>

<p>In my case &#8220;e7f3c02346d4&#8221; is the container ID. So i can use the same for commit. I won&#8217;t be commiting to the base Ubuntu image, as i can use the same for other purposes, so here i&#8217;ll commiting to a new image say &#8220;ubntu-fs-docker&#8221;</p>

<pre><code>$ docker commit -m "&lt;commit message&gt;" e7f3c02346d4 ubntu-fs-docker
</code></pre>

<p>Now we can use this &#8220;ubntu-fs-docker&#8221; image to launch a ready made FreeSwitch server&#8217;s.</p>

<p>Docker is a very juvenile project about more than a year old. But the use cases are expanding heavily in the Modern IT world. Docker is fueling up a new generation of scalable servers. Wishing all the best for Docker and kudos to <a href="https://twitter.com/solomonstre">Solomon Hykes</a> and the DotCloud team for opensourcing such a powerfull project</p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/05/19/uchiwa-an-awesome-dashboard-for-sensu/">UCHIWA - an Awesome Dashboard for Sensu</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-05-19T06:42:00+00:00" pubdate data-updated="true">May 19<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/devops/'>DevOps</a>, <a class='category' href='/blog/categories/monitoring/'>Monitoring</a>, <a class='category' href='/blog/categories/sensu/'>Sensu</a>, <a class='category' href='/blog/categories/sensu-dashboard/'>sensu-dashboard</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>It&#8217;s been more than a year since i&#8217;ve started playing with <code>Sensu</code>. It was one of coolest monitoring projects that i&#8217;ve ever worked with. Perfect for <em>Cloud</em> infrastructure and backed by a cool community. Though Sensu is still in <em>Juvenile</em> (v 0.12) state, its mature enough to tackle majority of the monitoring issues. Recently i started migrating all our monitoring from traditional Nagios to Sensu and the pretty cool thing with sensu is, we can directly use the Nagios plugins with Sensu. That&#8217;s an easy task for migration. We don&#8217;t have to rebuild all the check&#8217;s to make with Sensu. But most of the people outside was pointing to the sensu&#8217;s default dashboard. Though the dashboard doesn&#8217;t looks pretty fancy, it can do all the functions. But having a good dashboard which can display the current status is always a time saver.</p>

<p>So i was being searching for a good dashboard and the first choice on the google search was <a href="https://github.com/sensu/sensu-admin.git">Sensu-Admin</a>. A Rails project, which needs a backend DB. But still i was not satisfied with it, and i started looking out for something different. The second choice was <a href="https://github.com/cloudant/sabisu">sabisu</a>. Sabisu uses Cloudant&#8217;s hosted Couchdb with Lucene. We just need to store all the events in a Redis List and a custom script which reads the data from the Redis List and pushes it to the Cloudant&#8217;s CouchDB. So we basically need a Cloudant account and the Webapp makes Lucene queries to the Cloudant DB and displays the results on the Sabisu dashboard. Though i tried to rebuild the same setup locally, like running a CouchDB+Lucene locally and sending the same data to the local couchdb. With some codehack&#8217;s i was able to make the webapp talk to my local CouchDB and display the results on the dashboard.</p>

<p>But then, I found a super cool dashboard project called <a href="https://github.com/palourde/uchiwa">UCHIWA</a> which was started in Github a few days back by <a href="https://github.com/palourde">Simon Plourde</a>. <a href="https://github.com/palourde/uchiwa">Uchiwa</a> is simple dashboard built with <em>NodeJS</em> and uses <em>SocketIO</em> for real time updates. The screenshot&#8217;s looks super cool and i decided to give it a try. It has only one dependency, NodeJS, no backend DB&#8217;s required as it talks to the Sensu&#8217;s API in realtime.</p>

<h2>Setting Up NodeJS</h2>

<p>For Ubuntu, we can use the chris-lea&#8217;s PPA.</p>

<pre><code>apt-get install python-software-properties        # required for "apt-add-repository" binary

apt-add-repository ppa:chris-lea/node.js

apt-get update

apt-get install nodejs
</code></pre>

<p>now we have the latest NodeJS on our system, we can start setting up Uchiwa.</p>

<h2>Setting Up Uchiwa Dashboard</h2>

<p><code>Uchiwa</code>&#8217;s source is available in Github.</p>

<pre><code>git clone https://github.com/palourde/uchiwa.git
</code></pre>

<p>Once  cloned, the repository contains the &#8220;package.json&#8221; file which contains the list of necessary dependencies. We can use &#8220;npm&#8221; (node package manager) to install all these.</p>

<pre><code>cd uchiwa

npm install
</code></pre>

<p>Now we need to create a config file for the app. There is a sample config file available in the repo. So we need to mention the Sensu&#8217;s API IP and Port number and also the auth credentials if any. Plus auth credentials for accessing Uchiwa Dashboard page.</p>

<p>once all these are set, we are done. We just need to start the service.</p>

<pre><code>node app.js 
</code></pre>

<p>We can access the page via <em>http://localhost:3000/</em>,  or we can proxy pass from the webserver. Instructions for <em>Nginx</em> is available on the Readme of the project. Now we need to keep this app running all the time. So it&#8217;s better to create a init/upstart process for the same, so that the process will start automatically when the system reboots. There is a cool Node project called <a href="https://github.com/nodejitsu/forever">forever</a> which is a  simple CLI tool for ensuring that a given script runs continuously.</p>

<p>I&#8217;ve created an upstart script for Uchiwa, bu putting a conf file &#8220;uchiwa.conf&#8221; in the &#8220;/etc/init&#8221; directory. Below is the content for the conf file. Once the file is in place, we have to do a reload of the upstart configuration. <code>initctl reload-configuration</code> will do the trick.</p>

<pre><code>description "uchiwa - dashboard for sensu"
env APP_PATH = "/usr/local/uchiwa/"

start on startup
stop on shutdown

script
  cd $APP_PATH
  exec forever start app.js
end script
</code></pre>

<p>Uchiwa looks pretty cool and neat and it has the stash support also. There are couple of addons required like &#8220;Downtime&#8221;, but Uchiwa is a pretty new project and i&#8217;m sure that this project is gonna grow soon. It has already received 99 stars on the Github. Kudos to <a href="https://github.com/palourde">Simon Plourde</a> for Open Sourcing this awesome project.</p>

<p><img src="/images/uchiwa1.png"></p>

<p><img src="/images/uchiwa2.png"></p>
</div>
  
  


</div>

      </article>
    
    
      <article class="span4">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/05/05/extending-elk-stack-to-voip-infrastructure/">Extending ELK Stack to VOIP Infrastructure</a></h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <a href="https://plus.google.com/deepakmdass88?rel=author"><span class="fn">Deepak M Das</span></a></span>
  

 - 
        








  


<time datetime="2014-05-05T05:12:00+00:00" pubdate data-updated="true">May 5<span>th</span>, 2014</time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/collectd/'>Collectd</a>, <a class='category' href='/blog/categories/elasticsearch/'>Elasticsearch</a>, <a class='category' href='/blog/categories/freeswitch/'>Freeswitch</a>, <a class='category' href='/blog/categories/kibana/'>Kibana</a>, <a class='category' href='/blog/categories/logstash/'>Logstash</a>, <a class='category' href='/blog/categories/monitoring/'>Monitoring</a>, <a class='category' href='/blog/categories/voip/'>VOIP</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>Being a <code>DevOps</code> guy, i always love metrics. Visualized metrics gives a good picture of what&#8217;s happening in our live battle stations. There are now a quite lot of Open Source tools for monitoring and visualizing. It&#8217;s more than a year since i&#8217;ve started using <em>Logstash</em>. It never turned me down. <code>ElasticSearch-Logstash-Kibana</code> (ELK) is a killer combination. Though i started Elasticsearch + Logstash as a log analyzer, later <a href="https://github.com/etsy/statsd/">StatsD</a> and <a href="http://graphite.wikidot.com/">Graphite</a> took it to the next level. When we have a simple infrastructure it&#8217;s easy to monitor. But when the infra starts scaling, it becomes quite difficult to keep track of all the events happening inside each nodes. Though service checks can help, but there is still limitation for it. I faced a lot of scenarios where things breaks but service checks will be fine. Under such scenarios logs are the only hope. They have all these events captured.</p>

<p>At Plivo, we manage a variety of servers from SIP, Media, Proxy, WebServers, DB&#8217;s etc. Being a fully Cloud based system, i really wanted to have a system which can keep track of all the live events/status of what&#8217;s really happening inside our infra. So my plan was to collect two important stats, 1) Server&#8217;s events 2) Application events.</p>

<h4>Collectd and Logstash</h4>

<p><code>Collectd</code> is a daemon which collects system performance statistics periodically. Since we have a lot Server&#8217;s which handle Realtime Media, it&#8217;s a very critical component for us. We need to ensure that the server&#8217;s are not getting overloaded and there is no latency in network. I&#8217;ve been using Logstash heavily for stashing all my logs. And there is a stable input plugin for collectd to send the all the system metrics to logstash.</p>

<p>First we need to enable the Network Plugin, and then we need to mention our Logstash server IP and port so that collectd can start injecting metrics. Below is a sample colectd configuration.</p>

<pre><code>Hostname    "test.plivo.com"
Interval 10
Timeout 4
Include "/etc/collectd/filters.conf"
Include "/etc/collectd/thresholds.conf"
ReportStats true
    LogLevel info
LoadPlugin interface
LoadPlugin load
LoadPlugin memory
LoadPlugin network
&lt;Plugin interface&gt;
    Interface "eth0"
    IgnoreSelected false
&lt;/Plugin&gt;
&lt;Plugin network&gt;
    Server "{logstash_server_ip}" "logstash_server_port"    # if no port number is mentioned, it will take the default port number (25826)
&lt;/Plugin&gt;
</code></pre>

<p>Now on the Logstash server, we need to add the CollectD plugin on to the input filter in the logstash&#8217;s config file.</p>

<pre><code>input {
      collectd {
      port =&gt; "5555"    # default port is 25826
      }
}
</code></pre>

<p>Now we are set. Based the plugins enabled in the collectd config file, collctd will start sending the metrics to Logstash on the Interval mentioned in the config, default is 10s. So in my case, i wanted the Load, CPU usage, Memory usage, Bandiwdth (TX and RX) etc. There are default plugins for all these metrics, which we can just enable it in the config file. We also had some custom plugins to collect some custom metrics. BTW writing custom plugin is pretty easy in Collectd.</p>

<p>Now using the Logstash&#8217;s Elasticsearch output plugin, we can keep these metrics in Elasticsearch. Now this where Kibana comes in. We can start visualizing these metrics via Kibana. We need to create a custom Lucene Query. Once we have the query, we can create a custom histogram&#8217;s for each of these queries. Below aresome sample Lucene queries that we can use with Kibana.</p>

<pre><code>For Load -&gt; collectd_type:"load" AND host:"test.plivo.com"
For Network usage -&gt; collectd_type:"if_octets" AND host:"test.plivo.com"
</code></pre>

<p>Below is the screenshot of  histogram for Load and Network (TX and RX)</p>

<p><img src="/images/collectd.png"></p>

<h5>Log Events</h5>

<p>Now next is to collect the events from the application logs. We use SIP protocol for all our VOIP sessions. So all our SIP server&#8217;s are very critical for us. SIP is pretty similar to HTTP. The response codes are very similar to HTTP responses, ie 1xx, 2xx, 3xx, 4xx, 5xx, 6xx. So i wrote some custom grok patterns so keep track of all of these responses and stores the same on the Elasticsearch.</p>

<p>The second stats which i was interested was our SIP registrar server. We provide SIP endpoints to our customers so that they can use the same with SIP/Soft phones. So i was more interested on stats like Number of registrations/sec, Auth error rates. Plus using ElasticSearch&#8217;s MAP facet&#8217;s i can create BetterMap. In my previous blog post&#8217;s i&#8217;ve mentioned on how to create these bettermaps using Kibana and Elasticsearch. Below bettermap screenshot shows us the SIP endpoint registrations from various locations in the last 2 hours.</p>

<p><img src="/images/bettermap.png"></p>

<p>Now using the Kibana we can start visualizing all these data&#8217;s. Below is a sample of Dashboard that i&#8217;ve created using Kibana.</p>

<p><img src="/images/event_logs.png"></p>

<p>ELK stack proved to be an amazing combination. We are currently injecting 3 million events every day and ElasticSearch was blazingly fast in indexing all theses.</p>
</div>
  
  


</div>

      </article>
    
    </div>
      <div class="row">
        <ul class="pager">
          
            <li class="previous">
              <a href="/blog/page/2/">&larr; Older</a>
            </li>
          
          
        </ul>
      </div>
  </div>
</div>


  <div id="footer-widgets">
  <div class="container">
    <div class="row">
  <div class="span3">
    <h2>recent posts</h2>
    <ul class="recent_posts">
      
        <li>
          <a href="/blog/2014/10/14/monitoring-redis-using-collectd-and-elk/">Monitoring Redis using CollectD and ELK</a>
        </li>
      
        <li>
          <a href="/blog/2014/08/31/mesos-with-native-docker-support/">Mesos with Native Docker Support</a>
        </li>
      
        <li>
          <a href="/blog/2014/08/31/upgrading-marthon-for-mesos-native-docker-support/">Upgrading Marthon for Mesos Native Docker Support</a>
        </li>
      
        <li>
          <a href="/blog/2014/08/25/marathon-event-bus/">Marathon Event Bus</a>
        </li>
      
        <li>
          <a href="/blog/2014/08/15/managing-docker-cluster-using-multiple-mesos-masters/">Managing Docker Cluster using Multiple Mesos Masters</a>
        </li>
      
    </ul>
    <h2><a href="/blog/archives">archives</a></h2>
  </div>
  <div class="span3">
    <h2>instagram</h2>
    <div class="instagram"></div>
    <button id="instabutton" class="btn">more</button>
  </div>
  <div class="span4">
    <h2>twitter</h2>
    <a href="https://twitter.com/deepakmdass88" class="twitter-follow-button" data-show-count="true" data-lang="en">Follow @deepakmdass88</a>
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
    <div class="tweet">
    </div>
  </div>
  <div class="span2">
    <h2>found on</h2>
    <a href="https://github.com/deepakmdass88/" rel="tooltip" title="Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a>
    <a href="http://www.linkedin.com/pub/deepak-dass/44/54/602" rel="tooltip" title="Linkedin"><img class="social_icon" title="Linkedin" alt="Linkedin icon" src="/images/glyphicons_377_linked_in.png"></a>
    <a href="http://twitter.com/deepakmdass88" rel="tooltip" title="Twitter"><img class="social_icon" title="Twitter" alt="Twitter icon" src="/images/glyphicons_391_twitter_t.png"></a>
    <a href="https://plus.google.com/105770729176086017609/posts" rel="tooltip" title="Google Plus"><img class="social_icon" title="Google Plus" alt="Google Plus icon" src="/images/glyphicons_386_google_plus.png"></a>
    <a href="http://ttp://www.quora.com/Deepak-M-Dass" rel="tooltip" title="Quora"><img class="social_icon" title="Quora" alt="Quora icon" src="/images/glyphicons_385_quora.png"></a>
    <h2>contact at</h2>
    <a href="mailto:deepakmdass88@gmail.com">deepakmdass88@gmail.com</a>
  </div>
</div>

  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-left">
  <a href="/">Welcome to My Nerd World</a>
  - Copyright &copy; 2014 - Deepak M Das
</p>
<p class="pull-right">
  Powered by <a href="http://octopress.org/">Octopress</a>. Designed by <a href="http://www.AdrianArtiles.com">Adrian Artiles</a>.
</p>

  </div>
</footer>

  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript"></script>
<script>window.jQuery || document.write('<script src="/javascripts/libs/jquery-1.7.2.min.js" type="text/javascript"><\/script>')</script>
<script src="/javascripts/libs/bootstrap.min.js" type="text/javascript"></script>
<script src="/javascripts/jquery.tweet.js" type="text/javascript"></script>
<script src="/javascripts/jquery.instagram.js" type="text/javascript"></script>
<script src="/javascripts/custom.js" type="text/javascript"></script>





</body>
</html>
